Fold: 1/5 
Epoch [0/10] Batch 0/7568 Train_loss 6.6989099979400635 
Epoch [0/10] Batch 100/7568 Train_loss 2.4906456190170627 
Epoch [0/10] Batch 200/7568 Train_loss 2.384514068044833 
Epoch [0/10] Batch 300/7568 Train_loss 2.3208281036429232 
Epoch [0/10] Batch 400/7568 Train_loss 2.278797197111527 
Epoch [0/10] Batch 500/7568 Train_loss 2.2607251106503004 
Epoch [0/10] Batch 600/7568 Train_loss 2.25238221878815 
Epoch [0/10] Batch 700/7568 Train_loss 2.2446217673821387 
Epoch [0/10] Batch 800/7568 Train_loss 2.2405630639392635 
Epoch [0/10] Batch 900/7568 Train_loss 2.2364061292711557 
Epoch [0/10] Batch 1000/7568 Train_loss 2.2282651385346375 
Epoch [0/10] Batch 1100/7568 Train_loss 2.2224840384440245 
Epoch [0/10] Batch 1200/7568 Train_loss 2.2176475642399427 
Epoch [0/10] Batch 1300/7568 Train_loss 2.213221580580415 
Epoch [0/10] Batch 1400/7568 Train_loss 2.2127872189077458 
Epoch [0/10] Batch 1500/7568 Train_loss 2.207810856615361 
Epoch [0/10] Batch 1600/7568 Train_loss 2.199985121169141 
Epoch [0/10] Batch 1700/7568 Train_loss 2.19876271882245 
Epoch [0/10] Batch 1800/7568 Train_loss 2.1962041260135763 
Epoch [0/10] Batch 1900/7568 Train_loss 2.188683135718154 
Epoch [0/10] Batch 2000/7568 Train_loss 2.186429558199891 
Epoch [0/10] Batch 2100/7568 Train_loss 2.1801236309270697 
Epoch [0/10] Batch 2200/7568 Train_loss 2.174908903794281 
Epoch [0/10] Batch 2300/7568 Train_loss 2.172611929599304 
Epoch [0/10] Batch 2400/7568 Train_loss 2.1693852627149526 
Epoch [0/10] Batch 2500/7568 Train_loss 2.1671089241173163 
Epoch [0/10] Batch 2600/7568 Train_loss 2.168100414577813 
Epoch [0/10] Batch 2700/7568 Train_loss 2.1620866134542087 
Epoch [0/10] Batch 2800/7568 Train_loss 2.1599734669650337 
Epoch [0/10] Batch 2900/7568 Train_loss 2.1564573253338315 
Epoch [0/10] Batch 3000/7568 Train_loss 2.1542926571992624 
Epoch [0/10] Batch 3100/7568 Train_loss 2.157612699656112 
Epoch [0/10] Batch 3200/7568 Train_loss 2.160124402878453 
Epoch [0/10] Batch 3300/7568 Train_loss 2.16086642282302 
Epoch [0/10] Batch 3400/7568 Train_loss 2.1607477223279896 
Epoch [0/10] Batch 3500/7568 Train_loss 2.158904967234241 
Epoch [0/10] Batch 3600/7568 Train_loss 2.156908954277746 
Epoch [0/10] Batch 3700/7568 Train_loss 2.155309841590406 
Epoch [0/10] Batch 3800/7568 Train_loss 2.15673058847751 
Epoch [0/10] Batch 3900/7568 Train_loss 2.1537262533157033 
Epoch [0/10] Batch 4000/7568 Train_loss 2.1510980240916853 
Epoch [0/10] Batch 4100/7568 Train_loss 2.150955592154715 
Epoch [0/10] Batch 4200/7568 Train_loss 2.1489388732005943 
Epoch [0/10] Batch 4300/7568 Train_loss 2.1486063872093073 
Epoch [0/10] Batch 4400/7568 Train_loss 2.1467510918678006 
Epoch [0/10] Batch 4500/7568 Train_loss 2.145512131223001 
Epoch [0/10] Batch 4600/7568 Train_loss 2.1432666342492546 
Epoch [0/10] Batch 4700/7568 Train_loss 2.1436847197286144 
Epoch [0/10] Batch 4800/7568 Train_loss 2.143448418903676 
Epoch [0/10] Batch 4900/7568 Train_loss 2.1424616778174776 
Epoch [0/10] Batch 5000/7568 Train_loss 2.1411805555158865 
Epoch [0/10] Batch 5100/7568 Train_loss 2.1403491160149692 
Epoch [0/10] Batch 5200/7568 Train_loss 2.1401087999833797 
Epoch [0/10] Batch 5300/7568 Train_loss 2.1403150919841 
Epoch [0/10] Batch 5400/7568 Train_loss 2.1388252879517253 
Epoch [0/10] Batch 5500/7568 Train_loss 2.1378940546408045 
Epoch [0/10] Batch 5600/7568 Train_loss 2.13712978767558 
Epoch [0/10] Batch 5700/7568 Train_loss 2.136095061099722 
Epoch [0/10] Batch 5800/7568 Train_loss 2.1359140266980763 
Epoch [0/10] Batch 5900/7568 Train_loss 2.1358236684824545 
Epoch [0/10] Batch 6000/7568 Train_loss 2.1358573298241432 
Epoch [0/10] Batch 6100/7568 Train_loss 2.135833533561058 
Epoch [0/10] Batch 6200/7568 Train_loss 2.134029543854223 
Epoch [0/10] Batch 6300/7568 Train_loss 2.1339482446747287 
Epoch [0/10] Batch 6400/7568 Train_loss 2.1329340090373683 
Epoch [0/10] Batch 6500/7568 Train_loss 2.1319892771864115 
Epoch [0/10] Batch 6600/7568 Train_loss 2.131253703441056 
Epoch [0/10] Batch 6700/7568 Train_loss 2.1312549786825374 
Epoch [0/10] Batch 6800/7568 Train_loss 2.1305202605599094 
Epoch [0/10] Batch 6900/7568 Train_loss 2.130093013938182 
Epoch [0/10] Batch 7000/7568 Train_loss 2.128039195015122 
Epoch [0/10] Batch 7100/7568 Train_loss 2.1275696622326064 
Epoch [0/10] Batch 7200/7568 Train_loss 2.1271545719678677 
Epoch [0/10] Batch 7300/7568 Train_loss 2.126855369626153 
Epoch [0/10] Batch 7400/7568 Train_loss 2.1258979184366034 
Epoch [0/10] Batch 7500/7568 Train_loss 2.1257240751974504 
Epoch: 0/10 	Training Loss: 2.125136 	Validation Loss: 2.040556 Duration seconds: 1059.7704660892487 
Validation loss decreased (inf --> 2.040556).  Saving model ... 
best_valid_loss_fold [2.040555554349566] Best_Epoch [0]Epoch [1/10] Batch 0/7568 Train_loss 1.4565277099609375 
Epoch [1/10] Batch 100/7568 Train_loss 2.015281480432737 
Epoch [1/10] Batch 200/7568 Train_loss 2.0346129997007885 
Epoch [1/10] Batch 300/7568 Train_loss 2.0368178565042756 
Epoch [1/10] Batch 400/7568 Train_loss 2.067935699759576 
Epoch [1/10] Batch 500/7568 Train_loss 2.0866279852782896 
Epoch [1/10] Batch 600/7568 Train_loss 2.0855975576278176 
Epoch [1/10] Batch 700/7568 Train_loss 2.072906970616414 
Epoch [1/10] Batch 800/7568 Train_loss 2.074509838900614 
Epoch [1/10] Batch 900/7568 Train_loss 2.081448493916107 
Epoch [1/10] Batch 1000/7568 Train_loss 2.074380226276971 
Epoch [1/10] Batch 1100/7568 Train_loss 2.079056458891142 
Epoch [1/10] Batch 1200/7568 Train_loss 2.0743610298603796 
Epoch [1/10] Batch 1300/7568 Train_loss 2.070786126765446 
Epoch [1/10] Batch 1400/7568 Train_loss 2.075984830157745 
Epoch [1/10] Batch 1500/7568 Train_loss 2.076896623342852 
Epoch [1/10] Batch 1600/7568 Train_loss 2.0762630544142153 
Epoch [1/10] Batch 1700/7568 Train_loss 2.0645412330519517 
Epoch [1/10] Batch 1800/7568 Train_loss 2.069986041951749 
Epoch [1/10] Batch 1900/7568 Train_loss 2.0736208469114574 
Epoch [1/10] Batch 2000/7568 Train_loss 2.071230028277037 
Epoch [1/10] Batch 2100/7568 Train_loss 2.0731672401345382 
Epoch [1/10] Batch 2200/7568 Train_loss 2.0707766118305804 
Epoch [1/10] Batch 2300/7568 Train_loss 2.0705413885709256 
Epoch [1/10] Batch 2400/7568 Train_loss 2.071498013421527 
Epoch [1/10] Batch 2500/7568 Train_loss 2.068655892956357 
Epoch [1/10] Batch 2600/7568 Train_loss 2.0692238015413102 
Epoch [1/10] Batch 2700/7568 Train_loss 2.069546323282019 
Epoch [1/10] Batch 2800/7568 Train_loss 2.0686700428353597 
Epoch [1/10] Batch 2900/7568 Train_loss 2.0714887283210794 
Epoch [1/10] Batch 3000/7568 Train_loss 2.0699468775416805 
Epoch [1/10] Batch 3100/7568 Train_loss 2.072744933766382 
Epoch [1/10] Batch 3200/7568 Train_loss 2.0751674862689655 
Epoch [1/10] Batch 3300/7568 Train_loss 2.079176450161203 
Epoch [1/10] Batch 3400/7568 Train_loss 2.0796983702652105 
Epoch [1/10] Batch 3500/7568 Train_loss 2.0820124543962804 
Epoch [1/10] Batch 3600/7568 Train_loss 2.08354948958037 
Epoch [1/10] Batch 3700/7568 Train_loss 2.083935470206291 
Epoch [1/10] Batch 3800/7568 Train_loss 2.0831794425221815 
Epoch [1/10] Batch 3900/7568 Train_loss 2.083000987882768 
Epoch [1/10] Batch 4000/7568 Train_loss 2.0822918047475327 
Epoch [1/10] Batch 4100/7568 Train_loss 2.07931408805328 
Epoch [1/10] Batch 4200/7568 Train_loss 2.080163458516075 
Epoch [1/10] Batch 4300/7568 Train_loss 2.0810668580831693 
Epoch [1/10] Batch 4400/7568 Train_loss 2.0825641635628456 
Epoch [1/10] Batch 4500/7568 Train_loss 2.082079588353859 
Epoch [1/10] Batch 4600/7568 Train_loss 2.082656530942302 
Epoch [1/10] Batch 4700/7568 Train_loss 2.0826369700400633 
Epoch [1/10] Batch 4800/7568 Train_loss 2.083489703442424 
Epoch [1/10] Batch 4900/7568 Train_loss 2.0832359633459605 
Epoch [1/10] Batch 5000/7568 Train_loss 2.0845845727247374 
Epoch [1/10] Batch 5100/7568 Train_loss 2.0861502428033543 
Epoch [1/10] Batch 5200/7568 Train_loss 2.0845823208830994 
Epoch [1/10] Batch 5300/7568 Train_loss 2.08449276409786 
Epoch [1/10] Batch 5400/7568 Train_loss 2.084018950952554 
Epoch [1/10] Batch 5500/7568 Train_loss 2.0829624433048073 
Epoch [1/10] Batch 5600/7568 Train_loss 2.08310616380071 
Epoch [1/10] Batch 5700/7568 Train_loss 2.0828206060100407 
Epoch [1/10] Batch 5800/7568 Train_loss 2.0831722226857603 
Epoch [1/10] Batch 5900/7568 Train_loss 2.0849360701613984 
Epoch [1/10] Batch 6000/7568 Train_loss 2.0843937066972624 
Epoch [1/10] Batch 6100/7568 Train_loss 2.0833946062031194 
Epoch [1/10] Batch 6200/7568 Train_loss 2.083328912259013 
Epoch [1/10] Batch 6300/7568 Train_loss 2.0829355025803205 
Epoch [1/10] Batch 6400/7568 Train_loss 2.08250376869181 
Epoch [1/10] Batch 6500/7568 Train_loss 2.082607696980921 
Epoch [1/10] Batch 6600/7568 Train_loss 2.0814525712450207 
Epoch [1/10] Batch 6700/7568 Train_loss 2.081443483932501 
Epoch [1/10] Batch 6800/7568 Train_loss 2.080932198243796 
Epoch [1/10] Batch 6900/7568 Train_loss 2.079797235002139 
Epoch [1/10] Batch 7000/7568 Train_loss 2.078597274599731 
Epoch [1/10] Batch 7100/7568 Train_loss 2.0787812842124618 
Epoch [1/10] Batch 7200/7568 Train_loss 2.0783558229641224 
Epoch [1/10] Batch 7300/7568 Train_loss 2.0783264299252804 
Epoch [1/10] Batch 7400/7568 Train_loss 2.0758703484811134 
Epoch [1/10] Batch 7500/7568 Train_loss 2.0757276570100878 
Epoch: 1/10 	Training Loss: 2.077210 	Validation Loss: 2.019015 Duration seconds: 1073.5873019695282 
Validation loss decreased (2.040556 --> 2.019015).  Saving model ... 
best_valid_loss_fold [2.0190145950803795] Best_Epoch [1]Epoch [2/10] Batch 0/7568 Train_loss 2.2542232275009155 
Epoch [2/10] Batch 100/7568 Train_loss 2.0543080937154223 
Epoch [2/10] Batch 200/7568 Train_loss 2.03764882439108 
Epoch [2/10] Batch 300/7568 Train_loss 2.065769984971645 
Epoch [2/10] Batch 400/7568 Train_loss 2.0586691837655637 
Epoch [2/10] Batch 500/7568 Train_loss 2.060052061717429 
Epoch [2/10] Batch 600/7568 Train_loss 2.0672056729751893 
Epoch [2/10] Batch 700/7568 Train_loss 2.076119147187973 
Epoch [2/10] Batch 800/7568 Train_loss 2.07092309033454 
Epoch [2/10] Batch 900/7568 Train_loss 2.0751822590993325 
Epoch [2/10] Batch 1000/7568 Train_loss 2.08177664896825 
Epoch [2/10] Batch 1100/7568 Train_loss 2.0844299563432584 
Epoch [2/10] Batch 1200/7568 Train_loss 2.0830946021731154 
Epoch [2/10] Batch 1300/7568 Train_loss 2.0806087390287025 
Epoch [2/10] Batch 1400/7568 Train_loss 2.0792607752490606 
Epoch [2/10] Batch 1500/7568 Train_loss 2.0747252557672398 
Epoch [2/10] Batch 1600/7568 Train_loss 2.076214329990501 
Epoch [2/10] Batch 1700/7568 Train_loss 2.079289461070478 
Epoch [2/10] Batch 1800/7568 Train_loss 2.0781791822437707 
Epoch [2/10] Batch 1900/7568 Train_loss 2.075883488923797 
Epoch [2/10] Batch 2000/7568 Train_loss 2.0753790814986175 
Epoch [2/10] Batch 2100/7568 Train_loss 2.0720412413004974 
Epoch [2/10] Batch 2200/7568 Train_loss 2.0697453257876166 
Epoch [2/10] Batch 2300/7568 Train_loss 2.0685616522859047 
Epoch [2/10] Batch 2400/7568 Train_loss 2.06898030506502 
Epoch [2/10] Batch 2500/7568 Train_loss 2.071786567729647 
Epoch [2/10] Batch 2600/7568 Train_loss 2.0692381571000102 
Epoch [2/10] Batch 2700/7568 Train_loss 2.0665232991949356 
Epoch [2/10] Batch 2800/7568 Train_loss 2.0639814297053185 
Epoch [2/10] Batch 2900/7568 Train_loss 2.066271428721233 
Epoch [2/10] Batch 3000/7568 Train_loss 2.0684342505643065 
Epoch [2/10] Batch 3100/7568 Train_loss 2.0687590876065003 
Epoch [2/10] Batch 3200/7568 Train_loss 2.067051482308622 
Epoch [2/10] Batch 3300/7568 Train_loss 2.0674843786657306 
Epoch [2/10] Batch 3400/7568 Train_loss 2.067866858398343 
Epoch [2/10] Batch 3500/7568 Train_loss 2.069124024839137 
Epoch [2/10] Batch 3600/7568 Train_loss 2.0689146051437315 
Epoch [2/10] Batch 3700/7568 Train_loss 2.067251340504982 
Epoch [2/10] Batch 3800/7568 Train_loss 2.063365501656215 
Epoch [2/10] Batch 3900/7568 Train_loss 2.062583299106862 
Epoch [2/10] Batch 4000/7568 Train_loss 2.062285760437599 
Epoch [2/10] Batch 4100/7568 Train_loss 2.0627162037964597 
Epoch [2/10] Batch 4200/7568 Train_loss 2.0635915890077103 
Epoch [2/10] Batch 4300/7568 Train_loss 2.0647629705716555 
Epoch [2/10] Batch 4400/7568 Train_loss 2.0637234619153517 
Epoch [2/10] Batch 4500/7568 Train_loss 2.0612952766174 
Epoch [2/10] Batch 4600/7568 Train_loss 2.060461252931019 
Epoch [2/10] Batch 4700/7568 Train_loss 2.060938099867941 
Epoch [2/10] Batch 4800/7568 Train_loss 2.0620621587181804 
Epoch [2/10] Batch 4900/7568 Train_loss 2.0625782815613714 
Epoch [2/10] Batch 5000/7568 Train_loss 2.0626979802005696 
Epoch [2/10] Batch 5100/7568 Train_loss 2.0632756283220837 
Epoch [2/10] Batch 5200/7568 Train_loss 2.062223096867177 
Epoch [2/10] Batch 5300/7568 Train_loss 2.0636448002557577 
Epoch [2/10] Batch 5400/7568 Train_loss 2.0632954817103184 
Epoch [2/10] Batch 5500/7568 Train_loss 2.063294637218928 
Epoch [2/10] Batch 5600/7568 Train_loss 2.0613405597042855 
Epoch [2/10] Batch 5700/7568 Train_loss 2.0619464277635644 
Epoch [2/10] Batch 5800/7568 Train_loss 2.0606926943187855 
Epoch [2/10] Batch 5900/7568 Train_loss 2.061534538660549 
Epoch [2/10] Batch 6000/7568 Train_loss 2.062638607797991 
Epoch [2/10] Batch 6100/7568 Train_loss 2.0618419130231254 
Epoch [2/10] Batch 6200/7568 Train_loss 2.0616947037450655 
Epoch [2/10] Batch 6300/7568 Train_loss 2.0605254406929774 
Epoch [2/10] Batch 6400/7568 Train_loss 2.061199357634484 
Epoch [2/10] Batch 6500/7568 Train_loss 2.0607305362928354 
Epoch [2/10] Batch 6600/7568 Train_loss 2.0613051554869024 
Epoch [2/10] Batch 6700/7568 Train_loss 2.0611688808783044 
Epoch [2/10] Batch 6800/7568 Train_loss 2.0610321234156324 
Epoch [2/10] Batch 6900/7568 Train_loss 2.0617288125414657 
Epoch [2/10] Batch 7000/7568 Train_loss 2.0616411756569275 
Epoch [2/10] Batch 7100/7568 Train_loss 2.0627430624973435 
Epoch [2/10] Batch 7200/7568 Train_loss 2.0617255519047624 
Epoch [2/10] Batch 7300/7568 Train_loss 2.0617078136514433 
Epoch [2/10] Batch 7400/7568 Train_loss 2.060866370825345 
Epoch [2/10] Batch 7500/7568 Train_loss 2.059537282442271 
Epoch: 2/10 	Training Loss: 2.059062 	Validation Loss: 1.994067 Duration seconds: 1060.7653851509094 
Validation loss decreased (2.019015 --> 1.994067).  Saving model ... 
best_valid_loss_fold [1.9940668808820068] Best_Epoch [2]Epoch [3/10] Batch 0/7568 Train_loss 1.9805490970611572 
Epoch [3/10] Batch 100/7568 Train_loss 2.0785536469504384 
Epoch [3/10] Batch 200/7568 Train_loss 2.067203769339851 
Epoch [3/10] Batch 300/7568 Train_loss 2.041675468962454 
Epoch [3/10] Batch 400/7568 Train_loss 2.04798349344225 
Epoch [3/10] Batch 500/7568 Train_loss 2.0346022867216558 
Epoch [3/10] Batch 600/7568 Train_loss 2.037842114287089 
Epoch [3/10] Batch 700/7568 Train_loss 2.0263520062777522 
Epoch [3/10] Batch 800/7568 Train_loss 2.0232415859693296 
Epoch [3/10] Batch 900/7568 Train_loss 2.0300322341072174 
Epoch [3/10] Batch 1000/7568 Train_loss 2.034130764457134 
Epoch [3/10] Batch 1100/7568 Train_loss 2.0341547730156337 
Epoch [3/10] Batch 1200/7568 Train_loss 2.0302513374525937 
Epoch [3/10] Batch 1300/7568 Train_loss 2.029811299930711 
Epoch [3/10] Batch 1400/7568 Train_loss 2.029720970737262 
Epoch [3/10] Batch 1500/7568 Train_loss 2.03651350321649 
Epoch [3/10] Batch 1600/7568 Train_loss 2.032281177061413 
Epoch [3/10] Batch 1700/7568 Train_loss 2.0387108415925876 
Epoch [3/10] Batch 1800/7568 Train_loss 2.0376813382340564 
Epoch [3/10] Batch 1900/7568 Train_loss 2.0384793752105783 
Epoch [3/10] Batch 2000/7568 Train_loss 2.040160556902354 
Epoch [3/10] Batch 2100/7568 Train_loss 2.0412246582197158 
Epoch [3/10] Batch 2200/7568 Train_loss 2.0432746583686425 
Epoch [3/10] Batch 2300/7568 Train_loss 2.0430584722358422 
Epoch [3/10] Batch 2400/7568 Train_loss 2.0432526747642186 
Epoch [3/10] Batch 2500/7568 Train_loss 2.048575302116016 
Epoch [3/10] Batch 2600/7568 Train_loss 2.047832576942554 
Epoch [3/10] Batch 2700/7568 Train_loss 2.0481684256409856 
Epoch [3/10] Batch 2800/7568 Train_loss 2.0478560701039212 
Epoch [3/10] Batch 2900/7568 Train_loss 2.049527463523407 
Epoch [3/10] Batch 3000/7568 Train_loss 2.0481535341244306 
Epoch [3/10] Batch 3100/7568 Train_loss 2.0497354308142657 
Epoch [3/10] Batch 3200/7568 Train_loss 2.0507713140071986 
Epoch [3/10] Batch 3300/7568 Train_loss 2.0530027230330865 
Epoch [3/10] Batch 3400/7568 Train_loss 2.0526775367441896 
Epoch [3/10] Batch 3500/7568 Train_loss 2.050985883994805 
Epoch [3/10] Batch 3600/7568 Train_loss 2.051641880613471 
Epoch [3/10] Batch 3700/7568 Train_loss 2.0542710233390156 
Epoch [3/10] Batch 3800/7568 Train_loss 2.052738728710803 
Epoch [3/10] Batch 3900/7568 Train_loss 2.0522045604626786 
Epoch [3/10] Batch 4000/7568 Train_loss 2.0500570358938948 
Epoch [3/10] Batch 4100/7568 Train_loss 2.0505771945396214 
Epoch [3/10] Batch 4200/7568 Train_loss 2.0500490461781857 
Epoch [3/10] Batch 4300/7568 Train_loss 2.049475320920407 
Epoch [3/10] Batch 4400/7568 Train_loss 2.0491079143101425 
Epoch [3/10] Batch 4500/7568 Train_loss 2.0500525418429794 
Epoch [3/10] Batch 4600/7568 Train_loss 2.0498062934264154 
Epoch [3/10] Batch 4700/7568 Train_loss 2.049770062803981 
Epoch [3/10] Batch 4800/7568 Train_loss 2.0493345977915025 
Epoch [3/10] Batch 4900/7568 Train_loss 2.0511169370489566 
Epoch [3/10] Batch 5000/7568 Train_loss 2.0499170662370045 
Epoch [3/10] Batch 5100/7568 Train_loss 2.0496496751355298 
Epoch [3/10] Batch 5200/7568 Train_loss 2.0503937391074682 
Epoch [3/10] Batch 5300/7568 Train_loss 2.048845598753489 
Epoch [3/10] Batch 5400/7568 Train_loss 2.0490588119139366 
Epoch [3/10] Batch 5500/7568 Train_loss 2.0476572166866247 
Epoch [3/10] Batch 5600/7568 Train_loss 2.0491111773975597 
Epoch [3/10] Batch 5700/7568 Train_loss 2.048527198975817 
Epoch [3/10] Batch 5800/7568 Train_loss 2.0483233448496216 
Epoch [3/10] Batch 5900/7568 Train_loss 2.047563668531728 
Epoch [3/10] Batch 6000/7568 Train_loss 2.0456207893606306 
Epoch [3/10] Batch 6100/7568 Train_loss 2.048081063189637 
Epoch [3/10] Batch 6200/7568 Train_loss 2.0475493055849263 
Epoch [3/10] Batch 6300/7568 Train_loss 2.0471795133973205 
Epoch [3/10] Batch 6400/7568 Train_loss 2.0457359634769317 
Epoch [3/10] Batch 6500/7568 Train_loss 2.0465859255159367 
Epoch [3/10] Batch 6600/7568 Train_loss 2.0459235694562463 
Epoch [3/10] Batch 6700/7568 Train_loss 2.046294608792028 
Epoch [3/10] Batch 6800/7568 Train_loss 2.046478691099628 
Epoch [3/10] Batch 6900/7568 Train_loss 2.046575684388113 
Epoch [3/10] Batch 7000/7568 Train_loss 2.047010747698005 
Epoch [3/10] Batch 7100/7568 Train_loss 2.0466563041212944 
Epoch [3/10] Batch 7200/7568 Train_loss 2.046087103370883 
Epoch [3/10] Batch 7300/7568 Train_loss 2.0459791569381345 
Epoch [3/10] Batch 7400/7568 Train_loss 2.046005294142688 
Epoch [3/10] Batch 7500/7568 Train_loss 2.0471720966372535 
Epoch: 3/10 	Training Loss: 2.048223 	Validation Loss: 1.991046 Duration seconds: 1083.8440222740173 
Validation loss decreased (1.994067 --> 1.991046).  Saving model ... 
best_valid_loss_fold [1.9910459688512203] Best_Epoch [3]Epoch [4/10] Batch 0/7568 Train_loss 2.1680614352226257 
Epoch [4/10] Batch 100/7568 Train_loss 2.02018462932936 
Epoch [4/10] Batch 200/7568 Train_loss 2.045582734159569 
Epoch [4/10] Batch 300/7568 Train_loss 2.038566447868696 
Epoch [4/10] Batch 400/7568 Train_loss 2.0281515892901623 
Epoch [4/10] Batch 500/7568 Train_loss 2.0136833861618935 
Epoch [4/10] Batch 600/7568 Train_loss 2.0131676464330734 
Epoch [4/10] Batch 700/7568 Train_loss 2.0300019208272753 
Epoch [4/10] Batch 800/7568 Train_loss 2.035010072603803 
Epoch [4/10] Batch 900/7568 Train_loss 2.03612022697264 
Epoch [4/10] Batch 1000/7568 Train_loss 2.0452871050064143 
Epoch [4/10] Batch 1100/7568 Train_loss 2.0393822337420393 
Epoch [4/10] Batch 1200/7568 Train_loss 2.030444892183827 
Epoch [4/10] Batch 1300/7568 Train_loss 2.040859352078555 
Epoch [4/10] Batch 1400/7568 Train_loss 2.036867758669996 
Epoch [4/10] Batch 1500/7568 Train_loss 2.029743661469892 
Epoch [4/10] Batch 1600/7568 Train_loss 2.029835596717871 
Epoch [4/10] Batch 1700/7568 Train_loss 2.034067390185465 
Epoch [4/10] Batch 1800/7568 Train_loss 2.0334221870286275 
Epoch [4/10] Batch 1900/7568 Train_loss 2.0389779579915355 
Epoch [4/10] Batch 2000/7568 Train_loss 2.0402457719591487 
Epoch [4/10] Batch 2100/7568 Train_loss 2.0400004727905108 
Epoch [4/10] Batch 2200/7568 Train_loss 2.0386876521301183 
Epoch [4/10] Batch 2300/7568 Train_loss 2.040838065165792 
Epoch [4/10] Batch 2400/7568 Train_loss 2.0399592537238966 
Epoch [4/10] Batch 2500/7568 Train_loss 2.038312233129486 
Epoch [4/10] Batch 2600/7568 Train_loss 2.0337299383841585 
Epoch [4/10] Batch 2700/7568 Train_loss 2.0351950377391383 
Epoch [4/10] Batch 2800/7568 Train_loss 2.0393480499624483 
Epoch [4/10] Batch 2900/7568 Train_loss 2.0392554952425614 
Epoch [4/10] Batch 3000/7568 Train_loss 2.0386466750876977 
Epoch [4/10] Batch 3100/7568 Train_loss 2.0405599785087496 
Epoch [4/10] Batch 3200/7568 Train_loss 2.0369144747896666 
Epoch [4/10] Batch 3300/7568 Train_loss 2.036491935156902 
Epoch [4/10] Batch 3400/7568 Train_loss 2.035832350103689 
Epoch [4/10] Batch 3500/7568 Train_loss 2.0367599243148673 
Epoch [4/10] Batch 3600/7568 Train_loss 2.03664404657426 
Epoch [4/10] Batch 3700/7568 Train_loss 2.036871557053248 
Epoch [4/10] Batch 3800/7568 Train_loss 2.036094240465938 
Epoch [4/10] Batch 3900/7568 Train_loss 2.0338247455341394 
Epoch [4/10] Batch 4000/7568 Train_loss 2.0348012759086043 
Epoch [4/10] Batch 4100/7568 Train_loss 2.036759981009414 
Epoch [4/10] Batch 4200/7568 Train_loss 2.0365007358969636 
Epoch [4/10] Batch 4300/7568 Train_loss 2.036515112565274 
Epoch [4/10] Batch 4400/7568 Train_loss 2.0368011694310932 
Epoch [4/10] Batch 4500/7568 Train_loss 2.0347667673022927 
Epoch [4/10] Batch 4600/7568 Train_loss 2.0347747007185313 
Epoch [4/10] Batch 4700/7568 Train_loss 2.0331510607453516 
Epoch [4/10] Batch 4800/7568 Train_loss 2.0340750276595445 
Epoch [4/10] Batch 4900/7568 Train_loss 2.035805141970202 
Epoch [4/10] Batch 5000/7568 Train_loss 2.0364293531462376 
Epoch [4/10] Batch 5100/7568 Train_loss 2.035605558894401 
Epoch [4/10] Batch 5200/7568 Train_loss 2.037304280265962 
Epoch [4/10] Batch 5300/7568 Train_loss 2.034190791119519 
Epoch [4/10] Batch 5400/7568 Train_loss 2.0357707962517915 
Epoch [4/10] Batch 5500/7568 Train_loss 2.0374688256238724 
Epoch [4/10] Batch 5600/7568 Train_loss 2.036939433333044 
Epoch [4/10] Batch 5700/7568 Train_loss 2.0365522295586835 
Epoch [4/10] Batch 5800/7568 Train_loss 2.035370670661004 
Epoch [4/10] Batch 5900/7568 Train_loss 2.035589627549115 
Epoch [4/10] Batch 6000/7568 Train_loss 2.035735385494617 
Epoch [4/10] Batch 6100/7568 Train_loss 2.0368034020860044 
Epoch [4/10] Batch 6200/7568 Train_loss 2.038195091336509 
Epoch [4/10] Batch 6300/7568 Train_loss 2.0390249911853573 
Epoch [4/10] Batch 6400/7568 Train_loss 2.039535663925359 
Epoch [4/10] Batch 6500/7568 Train_loss 2.040039852188195 
Epoch [4/10] Batch 6600/7568 Train_loss 2.0405442276911705 
Epoch [4/10] Batch 6700/7568 Train_loss 2.039850231700914 
Epoch [4/10] Batch 6800/7568 Train_loss 2.0396865293363877 
Epoch [4/10] Batch 6900/7568 Train_loss 2.0386543044589116 
Epoch [4/10] Batch 7000/7568 Train_loss 2.0388092118711953 
Epoch [4/10] Batch 7100/7568 Train_loss 2.03820969023355 
Epoch [4/10] Batch 7200/7568 Train_loss 2.0374993496848517 
Epoch [4/10] Batch 7300/7568 Train_loss 2.037857968802567 
Epoch [4/10] Batch 7400/7568 Train_loss 2.0389916785761018 
Epoch [4/10] Batch 7500/7568 Train_loss 2.039321163561706 
Epoch: 4/10 	Training Loss: 2.038517 	Validation Loss: 1.988351 Duration seconds: 1118.6477239131927 
Validation loss decreased (1.991046 --> 1.988351).  Saving model ... 
best_valid_loss_fold [1.9883514490986665] Best_Epoch [4]Epoch [5/10] Batch 0/7568 Train_loss 2.743416428565979 
Epoch [5/10] Batch 100/7568 Train_loss 2.0000088111598893 
Epoch [5/10] Batch 200/7568 Train_loss 2.014899480328038 
Epoch [5/10] Batch 300/7568 Train_loss 2.0214700927815565 
Epoch [5/10] Batch 400/7568 Train_loss 2.017282612175864 
Epoch [5/10] Batch 500/7568 Train_loss 2.0151275923538825 
Epoch [5/10] Batch 600/7568 Train_loss 2.0259982578691944 
Epoch [5/10] Batch 700/7568 Train_loss 2.0316771959378443 
Epoch [5/10] Batch 800/7568 Train_loss 2.029112374337946 
Epoch [5/10] Batch 900/7568 Train_loss 2.0289951927141265 
Epoch [5/10] Batch 1000/7568 Train_loss 2.022943252085389 
Epoch [5/10] Batch 1100/7568 Train_loss 2.0218801101522486 
Epoch [5/10] Batch 1200/7568 Train_loss 2.0252383144558417 
Epoch [5/10] Batch 1300/7568 Train_loss 2.0250024290026656 
Epoch [5/10] Batch 1400/7568 Train_loss 2.030123620135158 
Epoch [5/10] Batch 1500/7568 Train_loss 2.0285487311738954 
Epoch [5/10] Batch 1600/7568 Train_loss 2.030579473411456 
Epoch [5/10] Batch 1700/7568 Train_loss 2.03101480503439 
Epoch [5/10] Batch 1800/7568 Train_loss 2.0265327485641262 
Epoch [5/10] Batch 1900/7568 Train_loss 2.02443216819079 
Epoch [5/10] Batch 2000/7568 Train_loss 2.026255827886739 
Epoch [5/10] Batch 2100/7568 Train_loss 2.02449005409217 
Epoch [5/10] Batch 2200/7568 Train_loss 2.0244447445358973 
Epoch [5/10] Batch 2300/7568 Train_loss 2.0264958914228495 
Epoch [5/10] Batch 2400/7568 Train_loss 2.0266728417041002 
Epoch [5/10] Batch 2500/7568 Train_loss 2.025627817307673 
Epoch [5/10] Batch 2600/7568 Train_loss 2.0216653572938985 
Epoch [5/10] Batch 2700/7568 Train_loss 2.0240390622524234 
Epoch [5/10] Batch 2800/7568 Train_loss 2.0263509102423716 
Epoch [5/10] Batch 2900/7568 Train_loss 2.0287829143261957 
Epoch [5/10] Batch 3000/7568 Train_loss 2.0314263320915584 
Epoch [5/10] Batch 3100/7568 Train_loss 2.0325515537814955 
Epoch [5/10] Batch 3200/7568 Train_loss 2.0311676082281314 
Epoch [5/10] Batch 3300/7568 Train_loss 2.036768400280431 
Epoch [5/10] Batch 3400/7568 Train_loss 2.0363095260869577 
Epoch [5/10] Batch 3500/7568 Train_loss 2.0356328899167973 
Epoch [5/10] Batch 3600/7568 Train_loss 2.0370210844467858 
Epoch [5/10] Batch 3700/7568 Train_loss 2.038139286115101 
Epoch [5/10] Batch 3800/7568 Train_loss 2.0379431940805315 
Epoch [5/10] Batch 3900/7568 Train_loss 2.0372654104467327 
Epoch [5/10] Batch 4000/7568 Train_loss 2.036383120490667 
Epoch [5/10] Batch 4100/7568 Train_loss 2.0375080401306325 
Epoch [5/10] Batch 4200/7568 Train_loss 2.0387125541197775 
Epoch [5/10] Batch 4300/7568 Train_loss 2.0389476562881574 
Epoch [5/10] Batch 4400/7568 Train_loss 2.0371751590015887 
Epoch [5/10] Batch 4500/7568 Train_loss 2.0354722022275293 
Epoch [5/10] Batch 4600/7568 Train_loss 2.0358429889469347 
Epoch [5/10] Batch 4700/7568 Train_loss 2.035275117811739 
Epoch [5/10] Batch 4800/7568 Train_loss 2.037090742182692 
Epoch [5/10] Batch 4900/7568 Train_loss 2.0389128693468206 
Epoch [5/10] Batch 5000/7568 Train_loss 2.038199742557621 
Epoch [5/10] Batch 5100/7568 Train_loss 2.0382983629533156 
Epoch [5/10] Batch 5200/7568 Train_loss 2.037768052248766 
Epoch [5/10] Batch 5300/7568 Train_loss 2.0363253197890816 
Epoch [5/10] Batch 5400/7568 Train_loss 2.03593056717254 
Epoch [5/10] Batch 5500/7568 Train_loss 2.034646509612415 
Epoch [5/10] Batch 5600/7568 Train_loss 2.033389622790859 
Epoch [5/10] Batch 5700/7568 Train_loss 2.0324948467438397 
Epoch [5/10] Batch 5800/7568 Train_loss 2.032203153918846 
Epoch [5/10] Batch 5900/7568 Train_loss 2.03150933938266 
Epoch [5/10] Batch 6000/7568 Train_loss 2.0322035143915125 
Epoch [5/10] Batch 6100/7568 Train_loss 2.0330829802003576 
Epoch [5/10] Batch 6200/7568 Train_loss 2.032675001289052 
Epoch [5/10] Batch 6300/7568 Train_loss 2.03217883103472 
Epoch [5/10] Batch 6400/7568 Train_loss 2.031570524640743 
Epoch [5/10] Batch 6500/7568 Train_loss 2.0323347784504304 
Epoch [5/10] Batch 6600/7568 Train_loss 2.032989646323503 
Epoch [5/10] Batch 6700/7568 Train_loss 2.033117567469088 
Epoch [5/10] Batch 6800/7568 Train_loss 2.033165444262132 
Epoch [5/10] Batch 6900/7568 Train_loss 2.032596363444861 
Epoch [5/10] Batch 7000/7568 Train_loss 2.0322204755406057 
Epoch [5/10] Batch 7100/7568 Train_loss 2.0313220963232483 
Epoch [5/10] Batch 7200/7568 Train_loss 2.0305716499340667 
Epoch [5/10] Batch 7300/7568 Train_loss 2.0300770603623426 
Epoch [5/10] Batch 7400/7568 Train_loss 2.0307495094480394 
Epoch [5/10] Batch 7500/7568 Train_loss 2.0308582621198576 
Epoch: 5/10 	Training Loss: 2.031181 	Validation Loss: 1.973914 Duration seconds: 1081.039725780487 
Validation loss decreased (1.988351 --> 1.973914).  Saving model ... 
best_valid_loss_fold [1.973913587692802] Best_Epoch [5]Epoch [6/10] Batch 0/7568 Train_loss 1.3264177739620209 
Epoch [6/10] Batch 100/7568 Train_loss 2.1205614175241774 
Epoch [6/10] Batch 200/7568 Train_loss 2.0707175002169254 
Epoch [6/10] Batch 300/7568 Train_loss 2.0667753739907497 
Epoch [6/10] Batch 400/7568 Train_loss 2.0541900647177065 
Epoch [6/10] Batch 500/7568 Train_loss 2.04068648437421 
Epoch [6/10] Batch 600/7568 Train_loss 2.0437814287853717 
Epoch [6/10] Batch 700/7568 Train_loss 2.0436445794755143 
Epoch [6/10] Batch 800/7568 Train_loss 2.044904763332467 
Epoch [6/10] Batch 900/7568 Train_loss 2.0417118172567505 
Epoch [6/10] Batch 1000/7568 Train_loss 2.0426118034463783 
Epoch [6/10] Batch 1100/7568 Train_loss 2.0362451542625637 
Epoch [6/10] Batch 1200/7568 Train_loss 2.0390923857862604 
Epoch [6/10] Batch 1300/7568 Train_loss 2.030013248576374 
Epoch [6/10] Batch 1400/7568 Train_loss 2.0262728402288706 
Epoch [6/10] Batch 1500/7568 Train_loss 2.0247616254036465 
Epoch [6/10] Batch 1600/7568 Train_loss 2.0196554210876987 
Epoch [6/10] Batch 1700/7568 Train_loss 2.0158590143792843 
Epoch [6/10] Batch 1800/7568 Train_loss 2.0220986340010847 
Epoch [6/10] Batch 1900/7568 Train_loss 2.0245001056691585 
Epoch [6/10] Batch 2000/7568 Train_loss 2.0244313887451244 
Epoch [6/10] Batch 2100/7568 Train_loss 2.025928202801974 
Epoch [6/10] Batch 2200/7568 Train_loss 2.0277248407590935 
Epoch [6/10] Batch 2300/7568 Train_loss 2.0269068634248413 
Epoch [6/10] Batch 2400/7568 Train_loss 2.028493926127669 
Epoch [6/10] Batch 2500/7568 Train_loss 2.0297168513409187 
Epoch [6/10] Batch 2600/7568 Train_loss 2.0306845408352645 
Epoch [6/10] Batch 2700/7568 Train_loss 2.0320408559593615 
Epoch [6/10] Batch 2800/7568 Train_loss 2.02995881182898 
Epoch [6/10] Batch 2900/7568 Train_loss 2.0312463156994522 
Epoch [6/10] Batch 3000/7568 Train_loss 2.0280828480123083 
Epoch [6/10] Batch 3100/7568 Train_loss 2.029287624156348 
Epoch [6/10] Batch 3200/7568 Train_loss 2.030203987177146 
Epoch [6/10] Batch 3300/7568 Train_loss 2.02882099101053 
Epoch [6/10] Batch 3400/7568 Train_loss 2.0279483627489614 
Epoch [6/10] Batch 3500/7568 Train_loss 2.026124912495274 
Epoch [6/10] Batch 3600/7568 Train_loss 2.027532022753645 
Epoch [6/10] Batch 3700/7568 Train_loss 2.026776388746537 
Epoch [6/10] Batch 3800/7568 Train_loss 2.026814949458381 
Epoch [6/10] Batch 3900/7568 Train_loss 2.027409763763697 
Epoch [6/10] Batch 4000/7568 Train_loss 2.0273615085789634 
Epoch [6/10] Batch 4100/7568 Train_loss 2.0294367674610085 
Epoch [6/10] Batch 4200/7568 Train_loss 2.0265237615665237 
Epoch [6/10] Batch 4300/7568 Train_loss 2.0266012489532375 
Epoch [6/10] Batch 4400/7568 Train_loss 2.024965252413232 
Epoch [6/10] Batch 4500/7568 Train_loss 2.026577977502089 
Epoch [6/10] Batch 4600/7568 Train_loss 2.0278889150839476 
Epoch [6/10] Batch 4700/7568 Train_loss 2.0266366433023366 
Epoch [6/10] Batch 4800/7568 Train_loss 2.0252122507787353 
Epoch [6/10] Batch 4900/7568 Train_loss 2.0269235524990634 
Epoch [6/10] Batch 5000/7568 Train_loss 2.027424236037354 
Epoch [6/10] Batch 5100/7568 Train_loss 2.0280496486756783 
Epoch [6/10] Batch 5200/7568 Train_loss 2.0277226469025567 
Epoch [6/10] Batch 5300/7568 Train_loss 2.0259984568607536 
Epoch [6/10] Batch 5400/7568 Train_loss 2.025002660607435 
Epoch [6/10] Batch 5500/7568 Train_loss 2.0253883880813475 
Epoch [6/10] Batch 5600/7568 Train_loss 2.026098721597987 
Epoch [6/10] Batch 5700/7568 Train_loss 2.0282768921159144 
Epoch [6/10] Batch 5800/7568 Train_loss 2.0275141535221843 
Epoch [6/10] Batch 5900/7568 Train_loss 2.027514720115595 
Epoch [6/10] Batch 6000/7568 Train_loss 2.0278974154048752 
Epoch [6/10] Batch 6100/7568 Train_loss 2.0278289628613093 
Epoch [6/10] Batch 6200/7568 Train_loss 2.029287837982812 
Epoch [6/10] Batch 6300/7568 Train_loss 2.0291458897516095 
Epoch [6/10] Batch 6400/7568 Train_loss 2.0281038535975644 
Epoch [6/10] Batch 6500/7568 Train_loss 2.0282419175115884 
Epoch [6/10] Batch 6600/7568 Train_loss 2.0276191063115787 
Epoch [6/10] Batch 6700/7568 Train_loss 2.028957268645952 
Epoch [6/10] Batch 6800/7568 Train_loss 2.0290030058215356 
Epoch [6/10] Batch 6900/7568 Train_loss 2.0285543345736516 
Epoch [6/10] Batch 7000/7568 Train_loss 2.029467279924493 
Epoch [6/10] Batch 7100/7568 Train_loss 2.0303382329908226 
Epoch [6/10] Batch 7200/7568 Train_loss 2.029360119807457 
Epoch [6/10] Batch 7300/7568 Train_loss 2.02884826238205 
Epoch [6/10] Batch 7400/7568 Train_loss 2.0287393313366855 
Epoch [6/10] Batch 7500/7568 Train_loss 2.0296553249892164 
Epoch: 6/10 	Training Loss: 2.029483 	Validation Loss: 2.000855 Duration seconds: 1094.823168039322 
best_valid_loss_fold [1.973913587692802] Best_Epoch [6]Epoch [7/10] Batch 0/7568 Train_loss 1.610836684703827 
Epoch [7/10] Batch 100/7568 Train_loss 2.0466643652998573 
Epoch [7/10] Batch 200/7568 Train_loss 2.0336075479266658 
Epoch [7/10] Batch 300/7568 Train_loss 2.0067636473135297 
Epoch [7/10] Batch 400/7568 Train_loss 2.0141179911662217 
Epoch [7/10] Batch 500/7568 Train_loss 2.022799923569856 
Epoch [7/10] Batch 600/7568 Train_loss 2.0359435308098597 
Epoch [7/10] Batch 700/7568 Train_loss 2.02663057817202 
Epoch [7/10] Batch 800/7568 Train_loss 2.030578971132357 
Epoch [7/10] Batch 900/7568 Train_loss 2.0225258782654834 
Epoch [7/10] Batch 1000/7568 Train_loss 2.0262383248333213 
Epoch [7/10] Batch 1100/7568 Train_loss 2.021801237027067 
Epoch [7/10] Batch 1200/7568 Train_loss 2.0191062909305146 
Epoch [7/10] Batch 1300/7568 Train_loss 2.0114121202388056 
Epoch [7/10] Batch 1400/7568 Train_loss 2.0147134441826533 
Epoch [7/10] Batch 1500/7568 Train_loss 2.017425501956136 
Epoch [7/10] Batch 1600/7568 Train_loss 2.0187245407546035 
Epoch [7/10] Batch 1700/7568 Train_loss 2.018279090944211 
Epoch [7/10] Batch 1800/7568 Train_loss 2.014255876891286 
Epoch [7/10] Batch 1900/7568 Train_loss 2.009190209778217 
Epoch [7/10] Batch 2000/7568 Train_loss 2.010073238450369 
Epoch [7/10] Batch 2100/7568 Train_loss 2.0115866658163886 
Epoch [7/10] Batch 2200/7568 Train_loss 2.011289633759744 
Epoch [7/10] Batch 2300/7568 Train_loss 2.012424011435886 
Epoch [7/10] Batch 2400/7568 Train_loss 2.0158750285510467 
Epoch [7/10] Batch 2500/7568 Train_loss 2.0184283313907083 
Epoch [7/10] Batch 2600/7568 Train_loss 2.0193444013091426 
Epoch [7/10] Batch 2700/7568 Train_loss 2.017689507450496 
Epoch [7/10] Batch 2800/7568 Train_loss 2.013561196016099 
Epoch [7/10] Batch 2900/7568 Train_loss 2.014315125604039 
Epoch [7/10] Batch 3000/7568 Train_loss 2.014560910517158 
Epoch [7/10] Batch 3100/7568 Train_loss 2.0165689545931333 
Epoch [7/10] Batch 3200/7568 Train_loss 2.018327157931639 
Epoch [7/10] Batch 3300/7568 Train_loss 2.0166427904152573 
Epoch [7/10] Batch 3400/7568 Train_loss 2.0169995905668796 
Epoch [7/10] Batch 3500/7568 Train_loss 2.0179172744455762 
Epoch [7/10] Batch 3600/7568 Train_loss 2.0164876422743703 
Epoch [7/10] Batch 3700/7568 Train_loss 2.0155925421264165 
Epoch [7/10] Batch 3800/7568 Train_loss 2.016958905223701 
Epoch [7/10] Batch 3900/7568 Train_loss 2.0187770187190117 
Epoch [7/10] Batch 4000/7568 Train_loss 2.0193316402382564 
Epoch [7/10] Batch 4100/7568 Train_loss 2.0169066468249643 
Epoch [7/10] Batch 4200/7568 Train_loss 2.017594145926173 
Epoch [7/10] Batch 4300/7568 Train_loss 2.0195256468446496 
Epoch [7/10] Batch 4400/7568 Train_loss 2.020695492831774 
Epoch [7/10] Batch 4500/7568 Train_loss 2.019842139477731 
Epoch [7/10] Batch 4600/7568 Train_loss 2.019843842977546 
Epoch [7/10] Batch 4700/7568 Train_loss 2.018846183986746 
Epoch [7/10] Batch 4800/7568 Train_loss 2.0188611833205248 
Epoch [7/10] Batch 4900/7568 Train_loss 2.0183478996622832 
Epoch [7/10] Batch 5000/7568 Train_loss 2.0188724233064383 
Epoch [7/10] Batch 5100/7568 Train_loss 2.01829074716374 
Epoch [7/10] Batch 5200/7568 Train_loss 2.01747586956748 
Epoch [7/10] Batch 5300/7568 Train_loss 2.016741335687154 
Epoch [7/10] Batch 5400/7568 Train_loss 2.018854253323757 
Epoch [7/10] Batch 5500/7568 Train_loss 2.020479594102514 
Epoch [7/10] Batch 5600/7568 Train_loss 2.019734696841436 
Epoch [7/10] Batch 5700/7568 Train_loss 2.0192580893335164 
Epoch [7/10] Batch 5800/7568 Train_loss 2.0193064245550287 
Epoch [7/10] Batch 5900/7568 Train_loss 2.0179713980674014 
Epoch [7/10] Batch 6000/7568 Train_loss 2.0198515384962232 
Epoch [7/10] Batch 6100/7568 Train_loss 2.0202614313030884 
Epoch [7/10] Batch 6200/7568 Train_loss 2.018839348251157 
Epoch [7/10] Batch 6300/7568 Train_loss 2.018847660843086 
Epoch [7/10] Batch 6400/7568 Train_loss 2.018824165963117 
Epoch [7/10] Batch 6500/7568 Train_loss 2.0185788619877685 
Epoch [7/10] Batch 6600/7568 Train_loss 2.018911280883585 
Epoch [7/10] Batch 6700/7568 Train_loss 2.0202577549954737 
Epoch [7/10] Batch 6800/7568 Train_loss 2.020476271590491 
Epoch [7/10] Batch 6900/7568 Train_loss 2.0199309491563615 
Epoch [7/10] Batch 7000/7568 Train_loss 2.0223530856452148 
Epoch [7/10] Batch 7100/7568 Train_loss 2.0221048964213995 
Epoch [7/10] Batch 7200/7568 Train_loss 2.0219152580510573 
Epoch [7/10] Batch 7300/7568 Train_loss 2.0238705309547473 
Epoch [7/10] Batch 7400/7568 Train_loss 2.0237461719327223 
Epoch [7/10] Batch 7500/7568 Train_loss 2.022746739862617 
Epoch: 7/10 	Training Loss: 2.023246 	Validation Loss: 1.981202 Duration seconds: 1087.4729709625244 
best_valid_loss_fold [1.973913587692802] Best_Epoch [7]Epoch [8/10] Batch 0/7568 Train_loss 2.2277918457984924 
Epoch [8/10] Batch 100/7568 Train_loss 1.9729322947487973 
Epoch [8/10] Batch 200/7568 Train_loss 1.978949982654396 
Epoch [8/10] Batch 300/7568 Train_loss 1.9767415882444064 
Epoch [8/10] Batch 400/7568 Train_loss 1.9759189359787992 
Epoch [8/10] Batch 500/7568 Train_loss 1.9791497211315912 
Epoch [8/10] Batch 600/7568 Train_loss 1.99245858670967 
Epoch [8/10] Batch 700/7568 Train_loss 2.006265497773078 
Epoch [8/10] Batch 800/7568 Train_loss 2.0074178524008404 
Epoch [8/10] Batch 900/7568 Train_loss 1.9987450311834885 
Epoch [8/10] Batch 1000/7568 Train_loss 1.9966506707918394 
Epoch [8/10] Batch 1100/7568 Train_loss 1.999303304558987 
Epoch [8/10] Batch 1200/7568 Train_loss 2.008391974818002 
Epoch [8/10] Batch 1300/7568 Train_loss 2.004387708270577 
Epoch [8/10] Batch 1400/7568 Train_loss 2.0067629137799026 
Epoch [8/10] Batch 1500/7568 Train_loss 2.010813991946828 
Epoch [8/10] Batch 1600/7568 Train_loss 2.0137178692629307 
Epoch [8/10] Batch 1700/7568 Train_loss 2.0085645226136437 
Epoch [8/10] Batch 1800/7568 Train_loss 2.0057377524504325 
Epoch [8/10] Batch 1900/7568 Train_loss 2.002886552118678 
Epoch [8/10] Batch 2000/7568 Train_loss 2.0043997049540176 
Epoch [8/10] Batch 2100/7568 Train_loss 2.0026053475488204 
Epoch [8/10] Batch 2200/7568 Train_loss 2.001435776858479 
Epoch [8/10] Batch 2300/7568 Train_loss 2.003396403324547 
Epoch [8/10] Batch 2400/7568 Train_loss 2.0057946999325744 
Epoch [8/10] Batch 2500/7568 Train_loss 2.0073594449639844 
Epoch [8/10] Batch 2600/7568 Train_loss 2.011543111342139 
Epoch [8/10] Batch 2700/7568 Train_loss 2.0111572103052833 
Epoch [8/10] Batch 2800/7568 Train_loss 2.010998378392417 
Epoch [8/10] Batch 2900/7568 Train_loss 2.0134461834713413 
Epoch [8/10] Batch 3000/7568 Train_loss 2.012734152364238 
Epoch [8/10] Batch 3100/7568 Train_loss 2.013340053208333 
Epoch [8/10] Batch 3200/7568 Train_loss 2.0121821880070576 
Epoch [8/10] Batch 3300/7568 Train_loss 2.0107841779023943 
Epoch [8/10] Batch 3400/7568 Train_loss 2.009600078299865 
Epoch [8/10] Batch 3500/7568 Train_loss 2.008628708291006 
Epoch [8/10] Batch 3600/7568 Train_loss 2.0082361304496796 
Epoch [8/10] Batch 3700/7568 Train_loss 2.00907274557268 
Epoch [8/10] Batch 3800/7568 Train_loss 2.010384287231378 
Epoch [8/10] Batch 3900/7568 Train_loss 2.0124748060410527 
Epoch [8/10] Batch 4000/7568 Train_loss 2.0099836270404263 
Epoch [8/10] Batch 4100/7568 Train_loss 2.0098197501596955 
Epoch [8/10] Batch 4200/7568 Train_loss 2.010200078736825 
Epoch [8/10] Batch 4300/7568 Train_loss 2.0108972973142825 
Epoch [8/10] Batch 4400/7568 Train_loss 2.013175045325252 
Epoch [8/10] Batch 4500/7568 Train_loss 2.0136981973765136 
Epoch [8/10] Batch 4600/7568 Train_loss 2.0136350696111758 
Epoch [8/10] Batch 4700/7568 Train_loss 2.0146412704788963 
Epoch [8/10] Batch 4800/7568 Train_loss 2.0144281277486966 
Epoch [8/10] Batch 4900/7568 Train_loss 2.0155054140798754 
Epoch [8/10] Batch 5000/7568 Train_loss 2.0151644014902197 
Epoch [8/10] Batch 5100/7568 Train_loss 2.013861618189689 
Epoch [8/10] Batch 5200/7568 Train_loss 2.0155911983904575 
Epoch [8/10] Batch 5300/7568 Train_loss 2.0150558693502068 
Epoch [8/10] Batch 5400/7568 Train_loss 2.0156651593633015 
Epoch [8/10] Batch 5500/7568 Train_loss 2.0173428013143355 
Epoch [8/10] Batch 5600/7568 Train_loss 2.018455110411839 
Epoch [8/10] Batch 5700/7568 Train_loss 2.018908217217374 
Epoch [8/10] Batch 5800/7568 Train_loss 2.018396082781981 
Epoch [8/10] Batch 5900/7568 Train_loss 2.0186765459766227 
Epoch [8/10] Batch 6000/7568 Train_loss 2.0193506943158477 
Epoch [8/10] Batch 6100/7568 Train_loss 2.0187420487511334 
Epoch [8/10] Batch 6200/7568 Train_loss 2.018832872037329 
Epoch [8/10] Batch 6300/7568 Train_loss 2.0201068795440578 
Epoch [8/10] Batch 6400/7568 Train_loss 2.020016125257994 
Epoch [8/10] Batch 6500/7568 Train_loss 2.019602792448419 
Epoch [8/10] Batch 6600/7568 Train_loss 2.0191519652134793 
Epoch [8/10] Batch 6700/7568 Train_loss 2.0190627568182777 
Epoch [8/10] Batch 6800/7568 Train_loss 2.0182933443531432 
Epoch [8/10] Batch 6900/7568 Train_loss 2.018037496455561 
Epoch [8/10] Batch 7000/7568 Train_loss 2.01810446217041 
Epoch [8/10] Batch 7100/7568 Train_loss 2.017674267464096 
Epoch [8/10] Batch 7200/7568 Train_loss 2.018026776850811 
Epoch [8/10] Batch 7300/7568 Train_loss 2.017678544211022 
Epoch [8/10] Batch 7400/7568 Train_loss 2.017130864701824 
Epoch [8/10] Batch 7500/7568 Train_loss 2.0169576431588956 
Epoch: 8/10 	Training Loss: 2.016315 	Validation Loss: 1.974318 Duration seconds: 1088.307415008545 
best_valid_loss_fold [1.973913587692802] Best_Epoch [8]Epoch [9/10] Batch 0/7568 Train_loss 3.8777172565460205 
Epoch [9/10] Batch 100/7568 Train_loss 2.0561992037709396 
Epoch [9/10] Batch 200/7568 Train_loss 1.992589165248088 
Epoch [9/10] Batch 300/7568 Train_loss 2.005355890803163 
Epoch [9/10] Batch 400/7568 Train_loss 2.0135411774354086 
Epoch [9/10] Batch 500/7568 Train_loss 2.032623736860747 
Epoch [9/10] Batch 600/7568 Train_loss 2.0278883582839553 
Epoch [9/10] Batch 700/7568 Train_loss 2.029693471679674 
Epoch [9/10] Batch 800/7568 Train_loss 2.0212136893608745 
Epoch [9/10] Batch 900/7568 Train_loss 2.015160319047022 
Epoch [9/10] Batch 1000/7568 Train_loss 2.013808921813131 
Epoch [9/10] Batch 1100/7568 Train_loss 2.0115323541356043 
Epoch [9/10] Batch 1200/7568 Train_loss 2.005116034184368 
Epoch [9/10] Batch 1300/7568 Train_loss 2.0041166148008887 
Epoch [9/10] Batch 1400/7568 Train_loss 1.9960489402193413 
Epoch [9/10] Batch 1500/7568 Train_loss 2.0047182756551183 
Epoch [9/10] Batch 1600/7568 Train_loss 2.0070901788598965 
Epoch [9/10] Batch 1700/7568 Train_loss 2.006343832739937 
Epoch [9/10] Batch 1800/7568 Train_loss 2.007924065010406 
Epoch [9/10] Batch 1900/7568 Train_loss 2.0137677984855977 
Epoch [9/10] Batch 2000/7568 Train_loss 2.0114640207930483 
Epoch [9/10] Batch 2100/7568 Train_loss 2.011304941171978 
Epoch [9/10] Batch 2200/7568 Train_loss 2.0091563489953264 
Epoch [9/10] Batch 2300/7568 Train_loss 2.0075340327790174 
Epoch [9/10] Batch 2400/7568 Train_loss 2.004703991937419 
Epoch [9/10] Batch 2500/7568 Train_loss 2.0064352590088745 
Epoch [9/10] Batch 2600/7568 Train_loss 2.004610842257818 
Epoch [9/10] Batch 2700/7568 Train_loss 2.0034896565538474 
Epoch [9/10] Batch 2800/7568 Train_loss 2.001905351536736 
Epoch [9/10] Batch 2900/7568 Train_loss 2.0041745251846574 
Epoch [9/10] Batch 3000/7568 Train_loss 2.0051914824421746 
Epoch [9/10] Batch 3100/7568 Train_loss 2.008660160021911 
Epoch [9/10] Batch 3200/7568 Train_loss 2.0104444688444993 
Epoch [9/10] Batch 3300/7568 Train_loss 2.0089115861264983 
Epoch [9/10] Batch 3400/7568 Train_loss 2.012154895793652 
Epoch [9/10] Batch 3500/7568 Train_loss 2.015430135995584 
Epoch [9/10] Batch 3600/7568 Train_loss 2.016810483297385 
Epoch [9/10] Batch 3700/7568 Train_loss 2.0176318450510036 
Epoch [9/10] Batch 3800/7568 Train_loss 2.017273309826067 
Epoch [9/10] Batch 3900/7568 Train_loss 2.018640961090255 
Epoch [9/10] Batch 4000/7568 Train_loss 2.021943019482828 
Epoch [9/10] Batch 4100/7568 Train_loss 2.02253831289318 
Epoch [9/10] Batch 4200/7568 Train_loss 2.0230246233290305 
Epoch [9/10] Batch 4300/7568 Train_loss 2.0219768153820112 
Epoch [9/10] Batch 4400/7568 Train_loss 2.0223932562091624 
Epoch [9/10] Batch 4500/7568 Train_loss 2.0207124514974137 
Epoch [9/10] Batch 4600/7568 Train_loss 2.019646643695171 
Epoch [9/10] Batch 4700/7568 Train_loss 2.0181726771126502 
Epoch [9/10] Batch 4800/7568 Train_loss 2.0173748301555903 
Epoch [9/10] Batch 4900/7568 Train_loss 2.0188268833052403 
Epoch [9/10] Batch 5000/7568 Train_loss 2.019860499833279 
Epoch [9/10] Batch 5100/7568 Train_loss 2.0194523306401004 
Epoch [9/10] Batch 5200/7568 Train_loss 2.018608697599641 
Epoch [9/10] Batch 5300/7568 Train_loss 2.0196739429883115 
Epoch [9/10] Batch 5400/7568 Train_loss 2.01890103244446 
Epoch [9/10] Batch 5500/7568 Train_loss 2.0204374612249953 
Epoch [9/10] Batch 5600/7568 Train_loss 2.019551598643686 
Epoch [9/10] Batch 5700/7568 Train_loss 2.021130370549561 
Epoch [9/10] Batch 5800/7568 Train_loss 2.0192976512106053 
Epoch [9/10] Batch 5900/7568 Train_loss 2.020411747499071 
Epoch [9/10] Batch 6000/7568 Train_loss 2.0213884098706414 
Epoch [9/10] Batch 6100/7568 Train_loss 2.021227630019833 
Epoch [9/10] Batch 6200/7568 Train_loss 2.0205765678803393 
Epoch [9/10] Batch 6300/7568 Train_loss 2.020755898721198 
Epoch [9/10] Batch 6400/7568 Train_loss 2.0214692167873736 
Epoch [9/10] Batch 6500/7568 Train_loss 2.022127870185488 
Epoch [9/10] Batch 6600/7568 Train_loss 2.0216470831516133 
Epoch [9/10] Batch 6700/7568 Train_loss 2.0213288860416396 
Epoch [9/10] Batch 6800/7568 Train_loss 2.0208603515070758 
Epoch [9/10] Batch 6900/7568 Train_loss 2.020684555259291 
Epoch [9/10] Batch 7000/7568 Train_loss 2.020606533312283 
Epoch [9/10] Batch 7100/7568 Train_loss 2.021038072211795 
Epoch [9/10] Batch 7200/7568 Train_loss 2.020677555703716 
Epoch [9/10] Batch 7300/7568 Train_loss 2.020794054348654 
Epoch [9/10] Batch 7400/7568 Train_loss 2.020042475855056 
Epoch [9/10] Batch 7500/7568 Train_loss 2.0182083556054833 
Epoch: 9/10 	Training Loss: 2.017895 	Validation Loss: 1.974006 Duration seconds: 1093.1637732982635 
best_valid_loss_fold [1.973913587692802] Best_Epoch [9]Fold: 2/5 
Epoch [0/10] Batch 0/7568 Train_loss 1.5295392274856567 
Epoch [0/10] Batch 100/7568 Train_loss 2.111806263870532 
Epoch [0/10] Batch 200/7568 Train_loss 2.0639815996980193 
Epoch [0/10] Batch 300/7568 Train_loss 2.0579505474167408 
Epoch [0/10] Batch 400/7568 Train_loss 2.052383828638795 
Epoch [0/10] Batch 500/7568 Train_loss 2.047427036537382 
Epoch [0/10] Batch 600/7568 Train_loss 2.054598018923734 
Epoch [0/10] Batch 700/7568 Train_loss 2.0310285895765254 
Epoch [0/10] Batch 800/7568 Train_loss 2.0178209179163575 
Epoch [0/10] Batch 900/7568 Train_loss 2.0313222225048433 
Epoch [0/10] Batch 1000/7568 Train_loss 2.0305838216315735 
Epoch [0/10] Batch 1100/7568 Train_loss 2.0279066610428336 
Epoch [0/10] Batch 1200/7568 Train_loss 2.0147172829367337 
Epoch [0/10] Batch 1300/7568 Train_loss 2.0127191915707257 
Epoch [0/10] Batch 1400/7568 Train_loss 2.0141449849321362 
Epoch [0/10] Batch 1500/7568 Train_loss 2.0091515081592752 
Epoch [0/10] Batch 1600/7568 Train_loss 2.0103675999598978 
Epoch [0/10] Batch 1700/7568 Train_loss 2.0154189138833667 
Epoch [0/10] Batch 1800/7568 Train_loss 2.016338267751961 
Epoch [0/10] Batch 1900/7568 Train_loss 2.0157817407446994 
Epoch [0/10] Batch 2000/7568 Train_loss 2.0141354972484526 
Epoch [0/10] Batch 2100/7568 Train_loss 2.0132001894377685 
Epoch [0/10] Batch 2200/7568 Train_loss 2.0153630576327086 
Epoch [0/10] Batch 2300/7568 Train_loss 2.01609094215574 
Epoch [0/10] Batch 2400/7568 Train_loss 2.0135781175454226 
Epoch [0/10] Batch 2500/7568 Train_loss 2.0111589909779553 
Epoch [0/10] Batch 2600/7568 Train_loss 2.0134282243315966 
Epoch [0/10] Batch 2700/7568 Train_loss 2.0164451710893596 
Epoch [0/10] Batch 2800/7568 Train_loss 2.016449279468726 
Epoch [0/10] Batch 2900/7568 Train_loss 2.0175389389474407 
Epoch [0/10] Batch 3000/7568 Train_loss 2.0199885790356396 
Epoch [0/10] Batch 3100/7568 Train_loss 2.020328757850173 
Epoch [0/10] Batch 3200/7568 Train_loss 2.021831505836192 
Epoch [0/10] Batch 3300/7568 Train_loss 2.020875029841433 
Epoch [0/10] Batch 3400/7568 Train_loss 2.019139075587687 
Epoch [0/10] Batch 3500/7568 Train_loss 2.01771732020705 
Epoch [0/10] Batch 3600/7568 Train_loss 2.0179784085660537 
Epoch [0/10] Batch 3700/7568 Train_loss 2.0173694978079837 
Epoch [0/10] Batch 3800/7568 Train_loss 2.01928062915755 
Epoch [0/10] Batch 3900/7568 Train_loss 2.019278448943113 
Epoch [0/10] Batch 4000/7568 Train_loss 2.0210115026270143 
Epoch [0/10] Batch 4100/7568 Train_loss 2.0198223532387116 
Epoch [0/10] Batch 4200/7568 Train_loss 2.020318268172459 
Epoch [0/10] Batch 4300/7568 Train_loss 2.0214243830497587 
Epoch [0/10] Batch 4400/7568 Train_loss 2.0201039024779157 
Epoch [0/10] Batch 4500/7568 Train_loss 2.019256391202283 
Epoch [0/10] Batch 4600/7568 Train_loss 2.0189581936603007 
Epoch [0/10] Batch 4700/7568 Train_loss 2.018742619949664 
Epoch [0/10] Batch 4800/7568 Train_loss 2.019440660009134 
Epoch [0/10] Batch 4900/7568 Train_loss 2.01753897814757 
Epoch [0/10] Batch 5000/7568 Train_loss 2.016409924345073 
Epoch [0/10] Batch 5100/7568 Train_loss 2.0151352396731843 
Epoch [0/10] Batch 5200/7568 Train_loss 2.0159512370326294 
Epoch [0/10] Batch 5300/7568 Train_loss 2.013744395567075 
Epoch [0/10] Batch 5400/7568 Train_loss 2.013090844894643 
Epoch [0/10] Batch 5500/7568 Train_loss 2.0140662897155406 
Epoch [0/10] Batch 5600/7568 Train_loss 2.013569652323871 
Epoch [0/10] Batch 5700/7568 Train_loss 2.013812113348757 
Epoch [0/10] Batch 5800/7568 Train_loss 2.013401483304232 
Epoch [0/10] Batch 5900/7568 Train_loss 2.0145798601314913 
Epoch [0/10] Batch 6000/7568 Train_loss 2.0150859586036596 
Epoch [0/10] Batch 6100/7568 Train_loss 2.0141172295851777 
Epoch [0/10] Batch 6200/7568 Train_loss 2.012492650878024 
Epoch [0/10] Batch 6300/7568 Train_loss 2.0131298164761535 
Epoch [0/10] Batch 6400/7568 Train_loss 2.012655961466495 
Epoch [0/10] Batch 6500/7568 Train_loss 2.012536595132127 
Epoch [0/10] Batch 6600/7568 Train_loss 2.0117835623421394 
Epoch [0/10] Batch 6700/7568 Train_loss 2.0127961168296657 
Epoch [0/10] Batch 6800/7568 Train_loss 2.0132265787052943 
Epoch [0/10] Batch 6900/7568 Train_loss 2.0135924020568075 
Epoch [0/10] Batch 7000/7568 Train_loss 2.0134414525359654 
Epoch [0/10] Batch 7100/7568 Train_loss 2.0132844608894556 
Epoch [0/10] Batch 7200/7568 Train_loss 2.0135055884973423 
Epoch [0/10] Batch 7300/7568 Train_loss 2.0130808292745876 
Epoch [0/10] Batch 7400/7568 Train_loss 2.013133053275608 
Epoch [0/10] Batch 7500/7568 Train_loss 2.01266075228313 
Epoch: 0/10 	Training Loss: 2.012038 	Validation Loss: 1.985497 Duration seconds: 982.6526665687561 
Validation loss decreased (inf --> 1.985497).  Saving model ... 
best_valid_loss_fold [1.9854968970653308] Best_Epoch [0]Epoch [1/10] Batch 0/7568 Train_loss 2.263508588075638 
Epoch [1/10] Batch 100/7568 Train_loss 2.014229016581384 
Epoch [1/10] Batch 200/7568 Train_loss 2.0391488505240103 
Epoch [1/10] Batch 300/7568 Train_loss 2.0452467312547458 
Epoch [1/10] Batch 400/7568 Train_loss 2.024366789848133 
Epoch [1/10] Batch 500/7568 Train_loss 2.028667188719837 
Epoch [1/10] Batch 600/7568 Train_loss 2.0314081181801495 
Epoch [1/10] Batch 700/7568 Train_loss 2.027624146999544 
Epoch [1/10] Batch 800/7568 Train_loss 2.0265585065036826 
Epoch [1/10] Batch 900/7568 Train_loss 2.0202612499814983 
Epoch [1/10] Batch 1000/7568 Train_loss 2.012041883615704 
Epoch [1/10] Batch 1100/7568 Train_loss 2.006287880708801 
Epoch [1/10] Batch 1200/7568 Train_loss 2.0041956661345064 
Epoch [1/10] Batch 1300/7568 Train_loss 2.0081995804432626 
Epoch [1/10] Batch 1400/7568 Train_loss 2.0123716525674973 
Epoch [1/10] Batch 1500/7568 Train_loss 2.0067304977371565 
Epoch [1/10] Batch 1600/7568 Train_loss 2.007773611589866 
Epoch [1/10] Batch 1700/7568 Train_loss 2.0108908863364774 
Epoch [1/10] Batch 1800/7568 Train_loss 2.012866289333659 
Epoch [1/10] Batch 1900/7568 Train_loss 2.0171073887659836 
Epoch [1/10] Batch 2000/7568 Train_loss 2.017236379036184 
Epoch [1/10] Batch 2100/7568 Train_loss 2.0143064959723853 
Epoch [1/10] Batch 2200/7568 Train_loss 2.016915219655258 
Epoch [1/10] Batch 2300/7568 Train_loss 2.0149385749723847 
Epoch [1/10] Batch 2400/7568 Train_loss 2.0131123082394997 
Epoch [1/10] Batch 2500/7568 Train_loss 2.0137946168240046 
Epoch [1/10] Batch 2600/7568 Train_loss 2.012094887849323 
Epoch [1/10] Batch 2700/7568 Train_loss 2.010708435796138 
Epoch [1/10] Batch 2800/7568 Train_loss 2.007632673302747 
Epoch [1/10] Batch 2900/7568 Train_loss 2.007952536623546 
Epoch [1/10] Batch 3000/7568 Train_loss 2.008282044224046 
Epoch [1/10] Batch 3100/7568 Train_loss 2.011274399679732 
Epoch [1/10] Batch 3200/7568 Train_loss 2.0114915230029524 
Epoch [1/10] Batch 3300/7568 Train_loss 2.013925060006894 
Epoch [1/10] Batch 3400/7568 Train_loss 2.014432775907676 
Epoch [1/10] Batch 3500/7568 Train_loss 2.0126362137072973 
Epoch [1/10] Batch 3600/7568 Train_loss 2.011866112863213 
Epoch [1/10] Batch 3700/7568 Train_loss 2.009824079493354 
Epoch [1/10] Batch 3800/7568 Train_loss 2.010184112907836 
Epoch [1/10] Batch 3900/7568 Train_loss 2.0085189231188902 
Epoch [1/10] Batch 4000/7568 Train_loss 2.0074290834726676 
Epoch [1/10] Batch 4100/7568 Train_loss 2.0067809450026113 
Epoch [1/10] Batch 4200/7568 Train_loss 2.0059042005588883 
Epoch [1/10] Batch 4300/7568 Train_loss 2.00630535305558 
Epoch [1/10] Batch 4400/7568 Train_loss 2.0080632976789796 
Epoch [1/10] Batch 4500/7568 Train_loss 2.007327237971569 
Epoch [1/10] Batch 4600/7568 Train_loss 2.007198698739336 
Epoch [1/10] Batch 4700/7568 Train_loss 2.0066970871581953 
Epoch [1/10] Batch 4800/7568 Train_loss 2.0083351011165504 
Epoch [1/10] Batch 4900/7568 Train_loss 2.0080892792303353 
Epoch [1/10] Batch 5000/7568 Train_loss 2.0082699279234757 
Epoch [1/10] Batch 5100/7568 Train_loss 2.0077570141314105 
Epoch [1/10] Batch 5200/7568 Train_loss 2.0062704367709605 
Epoch [1/10] Batch 5300/7568 Train_loss 2.0068430405133313 
Epoch [1/10] Batch 5400/7568 Train_loss 2.0071892465011785 
Epoch [1/10] Batch 5500/7568 Train_loss 2.0065162104951146 
Epoch [1/10] Batch 5600/7568 Train_loss 2.00769404316189 
Epoch [1/10] Batch 5700/7568 Train_loss 2.0077401782400544 
Epoch [1/10] Batch 5800/7568 Train_loss 2.0094604421598996 
Epoch [1/10] Batch 5900/7568 Train_loss 2.0094353646933882 
Epoch [1/10] Batch 6000/7568 Train_loss 2.010148362273813 
Epoch [1/10] Batch 6100/7568 Train_loss 2.0102444786924627 
Epoch [1/10] Batch 6200/7568 Train_loss 2.009588016247849 
Epoch [1/10] Batch 6300/7568 Train_loss 2.0101096329850217 
Epoch [1/10] Batch 6400/7568 Train_loss 2.0104560840085157 
Epoch [1/10] Batch 6500/7568 Train_loss 2.011278793376201 
Epoch [1/10] Batch 6600/7568 Train_loss 2.0111855870506976 
Epoch [1/10] Batch 6700/7568 Train_loss 2.011725172531674 
Epoch [1/10] Batch 6800/7568 Train_loss 2.0118612416582655 
Epoch [1/10] Batch 6900/7568 Train_loss 2.011011695439894 
Epoch [1/10] Batch 7000/7568 Train_loss 2.011034007864907 
Epoch [1/10] Batch 7100/7568 Train_loss 2.011205771608246 
Epoch [1/10] Batch 7200/7568 Train_loss 2.011797705285252 
Epoch [1/10] Batch 7300/7568 Train_loss 2.012301974662884 
Epoch [1/10] Batch 7400/7568 Train_loss 2.0116730683370307 
Epoch [1/10] Batch 7500/7568 Train_loss 2.011959421592559 
Epoch: 1/10 	Training Loss: 2.011324 	Validation Loss: 1.990884 Duration seconds: 1058.993804216385 
best_valid_loss_fold [1.9854968970653308] Best_Epoch [1]Epoch [2/10] Batch 0/7568 Train_loss 2.1270211040973663 
Epoch [2/10] Batch 100/7568 Train_loss 2.01827867786483 
Epoch [2/10] Batch 200/7568 Train_loss 2.0221764816721874 
Epoch [2/10] Batch 300/7568 Train_loss 1.9896731644472807 
Epoch [2/10] Batch 400/7568 Train_loss 2.000601975586349 
Epoch [2/10] Batch 500/7568 Train_loss 2.004340169375052 
Epoch [2/10] Batch 600/7568 Train_loss 2.006357493694134 
Epoch [2/10] Batch 700/7568 Train_loss 2.0106438100975694 
Epoch [2/10] Batch 800/7568 Train_loss 2.0212579419959646 
Epoch [2/10] Batch 900/7568 Train_loss 2.0169916287577245 
Epoch [2/10] Batch 1000/7568 Train_loss 2.0191887366426338 
Epoch [2/10] Batch 1100/7568 Train_loss 2.022683056076476 
Epoch [2/10] Batch 1200/7568 Train_loss 2.0260278343980658 
Epoch [2/10] Batch 1300/7568 Train_loss 2.0263518405066008 
Epoch [2/10] Batch 1400/7568 Train_loss 2.0212054562666517 
Epoch [2/10] Batch 1500/7568 Train_loss 2.0230856733886817 
Epoch [2/10] Batch 1600/7568 Train_loss 2.0215444633042643 
Epoch [2/10] Batch 1700/7568 Train_loss 2.018743004958676 
Epoch [2/10] Batch 1800/7568 Train_loss 2.0126716160049445 
Epoch [2/10] Batch 1900/7568 Train_loss 2.0137545733327054 
Epoch [2/10] Batch 2000/7568 Train_loss 2.0144553629980275 
Epoch [2/10] Batch 2100/7568 Train_loss 2.017166210584048 
Epoch [2/10] Batch 2200/7568 Train_loss 2.0161521999755267 
Epoch [2/10] Batch 2300/7568 Train_loss 2.0174332874228464 
Epoch [2/10] Batch 2400/7568 Train_loss 2.02021552545361 
Epoch [2/10] Batch 2500/7568 Train_loss 2.017147962312277 
Epoch [2/10] Batch 2600/7568 Train_loss 2.0180037163086277 
Epoch [2/10] Batch 2700/7568 Train_loss 2.015412446499003 
Epoch [2/10] Batch 2800/7568 Train_loss 2.0166834877914974 
Epoch [2/10] Batch 2900/7568 Train_loss 2.0194607445317194 
Epoch [2/10] Batch 3000/7568 Train_loss 2.019210628065337 
Epoch [2/10] Batch 3100/7568 Train_loss 2.0174796341340873 
Epoch [2/10] Batch 3200/7568 Train_loss 2.01512629091181 
Epoch [2/10] Batch 3300/7568 Train_loss 2.0129325438795287 
Epoch [2/10] Batch 3400/7568 Train_loss 2.012272152487792 
Epoch [2/10] Batch 3500/7568 Train_loss 2.0105713219838086 
Epoch [2/10] Batch 3600/7568 Train_loss 2.0106805723405685 
Epoch [2/10] Batch 3700/7568 Train_loss 2.007718204232011 
Epoch [2/10] Batch 3800/7568 Train_loss 2.0096891786206306 
Epoch [2/10] Batch 3900/7568 Train_loss 2.010106870480086 
Epoch [2/10] Batch 4000/7568 Train_loss 2.0101795817421126 
Epoch [2/10] Batch 4100/7568 Train_loss 2.009343382110104 
Epoch [2/10] Batch 4200/7568 Train_loss 2.009934780357055 
Epoch [2/10] Batch 4300/7568 Train_loss 2.009743420579106 
Epoch [2/10] Batch 4400/7568 Train_loss 2.0086165819079245 
Epoch [2/10] Batch 4500/7568 Train_loss 2.0073235945201295 
Epoch [2/10] Batch 4600/7568 Train_loss 2.007402414502566 
Epoch [2/10] Batch 4700/7568 Train_loss 2.0080237440056456 
Epoch [2/10] Batch 4800/7568 Train_loss 2.008151717342776 
Epoch [2/10] Batch 4900/7568 Train_loss 2.007014472535425 
Epoch [2/10] Batch 5000/7568 Train_loss 2.0082343943749303 
Epoch [2/10] Batch 5100/7568 Train_loss 2.0061582375089224 
Epoch [2/10] Batch 5200/7568 Train_loss 2.005591718818426 
Epoch [2/10] Batch 5300/7568 Train_loss 2.0060370780683603 
Epoch [2/10] Batch 5400/7568 Train_loss 2.0063301345371047 
Epoch [2/10] Batch 5500/7568 Train_loss 2.007041132665725 
Epoch [2/10] Batch 5600/7568 Train_loss 2.006820825737856 
Epoch [2/10] Batch 5700/7568 Train_loss 2.008193423012603 
Epoch [2/10] Batch 5800/7568 Train_loss 2.00672033350286 
Epoch [2/10] Batch 5900/7568 Train_loss 2.0057654324612804 
Epoch [2/10] Batch 6000/7568 Train_loss 2.004645175187712 
Epoch [2/10] Batch 6100/7568 Train_loss 2.005400557466031 
Epoch [2/10] Batch 6200/7568 Train_loss 2.005422349981369 
Epoch [2/10] Batch 6300/7568 Train_loss 2.006027122642895 
Epoch [2/10] Batch 6400/7568 Train_loss 2.008533495626884 
Epoch [2/10] Batch 6500/7568 Train_loss 2.006846470473235 
Epoch [2/10] Batch 6600/7568 Train_loss 2.0057621829402463 
Epoch [2/10] Batch 6700/7568 Train_loss 2.0051313861770925 
Epoch [2/10] Batch 6800/7568 Train_loss 2.0050556587000274 
Epoch [2/10] Batch 6900/7568 Train_loss 2.005298911279275 
Epoch [2/10] Batch 7000/7568 Train_loss 2.007253052566481 
Epoch [2/10] Batch 7100/7568 Train_loss 2.0079228647001828 
Epoch [2/10] Batch 7200/7568 Train_loss 2.0080839924192513 
Epoch [2/10] Batch 7300/7568 Train_loss 2.0085529733126237 
Epoch [2/10] Batch 7400/7568 Train_loss 2.0086794238954826 
Epoch [2/10] Batch 7500/7568 Train_loss 2.0095313636621084 
Epoch: 2/10 	Training Loss: 2.009472 	Validation Loss: 1.992567 Duration seconds: 1087.5686538219452 
best_valid_loss_fold [1.9854968970653308] Best_Epoch [2]Epoch [3/10] Batch 0/7568 Train_loss 1.7151598632335663 
Epoch [3/10] Batch 100/7568 Train_loss 2.035546932037514 
Epoch [3/10] Batch 200/7568 Train_loss 2.0076860546769195 
Epoch [3/10] Batch 300/7568 Train_loss 2.0107419140315135 
Epoch [3/10] Batch 400/7568 Train_loss 2.0091797857361837 
Epoch [3/10] Batch 500/7568 Train_loss 2.0075019767243942 
Epoch [3/10] Batch 600/7568 Train_loss 2.0163303585149683 
Epoch [3/10] Batch 700/7568 Train_loss 2.0068086697567207 
Epoch [3/10] Batch 800/7568 Train_loss 2.0159861353341113 
Epoch [3/10] Batch 900/7568 Train_loss 2.01823709435455 
Epoch [3/10] Batch 1000/7568 Train_loss 2.024634356845866 
Epoch [3/10] Batch 1100/7568 Train_loss 2.022306880187057 
Epoch [3/10] Batch 1200/7568 Train_loss 2.022833111792877 
Epoch [3/10] Batch 1300/7568 Train_loss 2.0213694671751625 
Epoch [3/10] Batch 1400/7568 Train_loss 2.022917175254679 
Epoch [3/10] Batch 1500/7568 Train_loss 2.015177695742057 
Epoch [3/10] Batch 1600/7568 Train_loss 2.0191991390622013 
Epoch [3/10] Batch 1700/7568 Train_loss 2.0218089225098077 
Epoch [3/10] Batch 1800/7568 Train_loss 2.021895535559141 
Epoch [3/10] Batch 1900/7568 Train_loss 2.0207537533122575 
Epoch [3/10] Batch 2000/7568 Train_loss 2.0221165334177518 
Epoch [3/10] Batch 2100/7568 Train_loss 2.0186240778174755 
Epoch [3/10] Batch 2200/7568 Train_loss 2.014676193143497 
Epoch [3/10] Batch 2300/7568 Train_loss 2.014317922925545 
Epoch [3/10] Batch 2400/7568 Train_loss 2.0102844050294504 
Epoch [3/10] Batch 2500/7568 Train_loss 2.008543691245235 
Epoch [3/10] Batch 2600/7568 Train_loss 2.0075408996916058 
Epoch [3/10] Batch 2700/7568 Train_loss 2.0044488504788296 
Epoch [3/10] Batch 2800/7568 Train_loss 2.004510659733784 
Epoch [3/10] Batch 2900/7568 Train_loss 2.0036546196384046 
Epoch [3/10] Batch 3000/7568 Train_loss 2.00577808845067 
Epoch [3/10] Batch 3100/7568 Train_loss 2.004772308266036 
Epoch [3/10] Batch 3200/7568 Train_loss 2.001958760045648 
Epoch [3/10] Batch 3300/7568 Train_loss 1.9997937157998407 
Epoch [3/10] Batch 3400/7568 Train_loss 2.0012708639649914 
Epoch [3/10] Batch 3500/7568 Train_loss 2.0013417645339726 
Epoch [3/10] Batch 3600/7568 Train_loss 2.000357812537944 
Epoch [3/10] Batch 3700/7568 Train_loss 2.001793072316106 
Epoch [3/10] Batch 3800/7568 Train_loss 2.0019183985710582 
Epoch [3/10] Batch 3900/7568 Train_loss 2.0045605792705294 
Epoch [3/10] Batch 4000/7568 Train_loss 2.004203399928681 
Epoch [3/10] Batch 4100/7568 Train_loss 2.006365776323627 
Epoch [3/10] Batch 4200/7568 Train_loss 2.008881263596278 
Epoch [3/10] Batch 4300/7568 Train_loss 2.0098677973599246 
Epoch [3/10] Batch 4400/7568 Train_loss 2.008850473239324 
Epoch [3/10] Batch 4500/7568 Train_loss 2.0079782469600764 
Epoch [3/10] Batch 4600/7568 Train_loss 2.0104337394904688 
Epoch [3/10] Batch 4700/7568 Train_loss 2.010170742615794 
Epoch [3/10] Batch 4800/7568 Train_loss 2.0088624996589983 
Epoch [3/10] Batch 4900/7568 Train_loss 2.0111174880850196 
Epoch [3/10] Batch 5000/7568 Train_loss 2.0120825621485112 
Epoch [3/10] Batch 5100/7568 Train_loss 2.0121947523017325 
Epoch [3/10] Batch 5200/7568 Train_loss 2.0111307881985967 
Epoch [3/10] Batch 5300/7568 Train_loss 2.011101718157828 
Epoch [3/10] Batch 5400/7568 Train_loss 2.0112053465620274 
Epoch [3/10] Batch 5500/7568 Train_loss 2.011722952094951 
Epoch [3/10] Batch 5600/7568 Train_loss 2.0112100591953896 
Epoch [3/10] Batch 5700/7568 Train_loss 2.0122088942009615 
Epoch [3/10] Batch 5800/7568 Train_loss 2.0122889325554425 
Epoch [3/10] Batch 5900/7568 Train_loss 2.011557679444708 
Epoch [3/10] Batch 6000/7568 Train_loss 2.0116431200887694 
Epoch [3/10] Batch 6100/7568 Train_loss 2.0114650964307073 
Epoch [3/10] Batch 6200/7568 Train_loss 2.0117425750487468 
Epoch [3/10] Batch 6300/7568 Train_loss 2.0110495886198585 
Epoch [3/10] Batch 6400/7568 Train_loss 2.0126077889991056 
Epoch [3/10] Batch 6500/7568 Train_loss 2.011978113214395 
Epoch [3/10] Batch 6600/7568 Train_loss 2.012457198556075 
Epoch [3/10] Batch 6700/7568 Train_loss 2.01323792876512 
Epoch [3/10] Batch 6800/7568 Train_loss 2.0119220463493575 
Epoch [3/10] Batch 6900/7568 Train_loss 2.0119195588342245 
Epoch [3/10] Batch 7000/7568 Train_loss 2.011439513445428 
Epoch [3/10] Batch 7100/7568 Train_loss 2.0114115020634005 
Epoch [3/10] Batch 7200/7568 Train_loss 2.010638499972622 
Epoch [3/10] Batch 7300/7568 Train_loss 2.010109039765158 
Epoch [3/10] Batch 7400/7568 Train_loss 2.0097545699370323 
Epoch [3/10] Batch 7500/7568 Train_loss 2.0096532054470946 
Epoch: 3/10 	Training Loss: 2.009191 	Validation Loss: 1.983963 Duration seconds: 1100.1741654872894 
Validation loss decreased (1.985497 --> 1.983963).  Saving model ... 
best_valid_loss_fold [1.9839628934387676] Best_Epoch [3]Epoch [4/10] Batch 0/7568 Train_loss 2.268981486558914 
Epoch [4/10] Batch 100/7568 Train_loss 2.0542488807793893 
Epoch [4/10] Batch 200/7568 Train_loss 2.0149004188787876 
Epoch [4/10] Batch 300/7568 Train_loss 2.0109772803478463 
Epoch [4/10] Batch 400/7568 Train_loss 2.0374615263760534 
Epoch [4/10] Batch 500/7568 Train_loss 2.0365571447772655 
Epoch [4/10] Batch 600/7568 Train_loss 2.039563948173888 
Epoch [4/10] Batch 700/7568 Train_loss 2.037795748503164 
Epoch [4/10] Batch 800/7568 Train_loss 2.0390170718660663 
Epoch [4/10] Batch 900/7568 Train_loss 2.0425473820918407 
Epoch [4/10] Batch 1000/7568 Train_loss 2.040178590453231 
Epoch [4/10] Batch 1100/7568 Train_loss 2.035556420291369 
Epoch [4/10] Batch 1200/7568 Train_loss 2.028620046602995 
Epoch [4/10] Batch 1300/7568 Train_loss 2.0342329129015457 
Epoch [4/10] Batch 1400/7568 Train_loss 2.033984478640182 
Epoch [4/10] Batch 1500/7568 Train_loss 2.033599272648308 
Epoch [4/10] Batch 1600/7568 Train_loss 2.0349678038378047 
Epoch [4/10] Batch 1700/7568 Train_loss 2.03630612881691 
Epoch [4/10] Batch 1800/7568 Train_loss 2.03436693814746 
Epoch [4/10] Batch 1900/7568 Train_loss 2.034075054869659 
Epoch [4/10] Batch 2000/7568 Train_loss 2.0306004390217316 
Epoch [4/10] Batch 2100/7568 Train_loss 2.029608362015232 
Epoch [4/10] Batch 2200/7568 Train_loss 2.0316268326964284 
Epoch [4/10] Batch 2300/7568 Train_loss 2.0305362699965404 
Epoch [4/10] Batch 2400/7568 Train_loss 2.0300651157351646 
Epoch [4/10] Batch 2500/7568 Train_loss 2.0321275260461804 
Epoch [4/10] Batch 2600/7568 Train_loss 2.0308089767600976 
Epoch [4/10] Batch 2700/7568 Train_loss 2.027287182985346 
Epoch [4/10] Batch 2800/7568 Train_loss 2.024608976481983 
Epoch [4/10] Batch 2900/7568 Train_loss 2.022892252016544 
Epoch [4/10] Batch 3000/7568 Train_loss 2.0240015492851198 
Epoch [4/10] Batch 3100/7568 Train_loss 2.025798187284499 
Epoch [4/10] Batch 3200/7568 Train_loss 2.0247428297475145 
Epoch [4/10] Batch 3300/7568 Train_loss 2.0235070650756666 
Epoch [4/10] Batch 3400/7568 Train_loss 2.0250951419881567 
Epoch [4/10] Batch 3500/7568 Train_loss 2.0248979250896455 
Epoch [4/10] Batch 3600/7568 Train_loss 2.023341468194232 
Epoch [4/10] Batch 3700/7568 Train_loss 2.020899483667809 
Epoch [4/10] Batch 3800/7568 Train_loss 2.022756315146393 
Epoch [4/10] Batch 3900/7568 Train_loss 2.020369176126174 
Epoch [4/10] Batch 4000/7568 Train_loss 2.018170876950361 
Epoch [4/10] Batch 4100/7568 Train_loss 2.017652101431408 
Epoch [4/10] Batch 4200/7568 Train_loss 2.01667226714425 
Epoch [4/10] Batch 4300/7568 Train_loss 2.015995843930595 
Epoch [4/10] Batch 4400/7568 Train_loss 2.015212783063035 
Epoch [4/10] Batch 4500/7568 Train_loss 2.0156968351212696 
Epoch [4/10] Batch 4600/7568 Train_loss 2.0127741808354713 
Epoch [4/10] Batch 4700/7568 Train_loss 2.012442294662746 
Epoch [4/10] Batch 4800/7568 Train_loss 2.0126416057757948 
Epoch [4/10] Batch 4900/7568 Train_loss 2.0132924973283632 
Epoch [4/10] Batch 5000/7568 Train_loss 2.011978220573737 
Epoch [4/10] Batch 5100/7568 Train_loss 2.0121300355011846 
Epoch [4/10] Batch 5200/7568 Train_loss 2.0115696277682127 
Epoch [4/10] Batch 5300/7568 Train_loss 2.012122637274085 
Epoch [4/10] Batch 5400/7568 Train_loss 2.010487169675685 
Epoch [4/10] Batch 5500/7568 Train_loss 2.0098048248246156 
Epoch [4/10] Batch 5600/7568 Train_loss 2.0088007773853502 
Epoch [4/10] Batch 5700/7568 Train_loss 2.0078935407767315 
Epoch [4/10] Batch 5800/7568 Train_loss 2.007738352091615 
Epoch [4/10] Batch 5900/7568 Train_loss 2.007118592591452 
Epoch [4/10] Batch 6000/7568 Train_loss 2.0058949933963266 
Epoch [4/10] Batch 6100/7568 Train_loss 2.0069771321449803 
Epoch [4/10] Batch 6200/7568 Train_loss 2.006870321875387 
Epoch [4/10] Batch 6300/7568 Train_loss 2.0072785025786795 
Epoch [4/10] Batch 6400/7568 Train_loss 2.0075811343632757 
Epoch [4/10] Batch 6500/7568 Train_loss 2.007113005281173 
Epoch [4/10] Batch 6600/7568 Train_loss 2.0064757780028444 
Epoch [4/10] Batch 6700/7568 Train_loss 2.0055928970540466 
Epoch [4/10] Batch 6800/7568 Train_loss 2.0047647412770497 
Epoch [4/10] Batch 6900/7568 Train_loss 2.0044370376922105 
Epoch [4/10] Batch 7000/7568 Train_loss 2.0061126529932056 
Epoch [4/10] Batch 7100/7568 Train_loss 2.0072201796273683 
Epoch [4/10] Batch 7200/7568 Train_loss 2.0062676005281683 
Epoch [4/10] Batch 7300/7568 Train_loss 2.0071382747131117 
Epoch [4/10] Batch 7400/7568 Train_loss 2.0068613509361266 
Epoch [4/10] Batch 7500/7568 Train_loss 2.0077595479539068 
Epoch: 4/10 	Training Loss: 2.007845 	Validation Loss: 1.990867 Duration seconds: 1109.632752418518 
best_valid_loss_fold [1.9839628934387676] Best_Epoch [4]Epoch [5/10] Batch 0/7568 Train_loss 1.8947356939315796 
Epoch [5/10] Batch 100/7568 Train_loss 2.091610123320381 
Epoch [5/10] Batch 200/7568 Train_loss 2.0589333209232312 
Epoch [5/10] Batch 300/7568 Train_loss 2.057770696589321 
Epoch [5/10] Batch 400/7568 Train_loss 2.060407131836004 
Epoch [5/10] Batch 500/7568 Train_loss 2.0507504862404153 
Epoch [5/10] Batch 600/7568 Train_loss 2.0463809145369662 
Epoch [5/10] Batch 700/7568 Train_loss 2.0458636470085882 
Epoch [5/10] Batch 800/7568 Train_loss 2.0404693322458516 
Epoch [5/10] Batch 900/7568 Train_loss 2.027037297101582 
Epoch [5/10] Batch 1000/7568 Train_loss 2.0305906120058776 
Epoch [5/10] Batch 1100/7568 Train_loss 2.040899122822404 
Epoch [5/10] Batch 1200/7568 Train_loss 2.0452584780374234 
Epoch [5/10] Batch 1300/7568 Train_loss 2.0420053282281603 
Epoch [5/10] Batch 1400/7568 Train_loss 2.0365638218336155 
Epoch [5/10] Batch 1500/7568 Train_loss 2.0356603737158587 
Epoch [5/10] Batch 1600/7568 Train_loss 2.03260857558377 
Epoch [5/10] Batch 1700/7568 Train_loss 2.035567452326233 
Epoch [5/10] Batch 1800/7568 Train_loss 2.0359705348004242 
Epoch [5/10] Batch 1900/7568 Train_loss 2.0327759400080656 
Epoch [5/10] Batch 2000/7568 Train_loss 2.030668898091383 
Epoch [5/10] Batch 2100/7568 Train_loss 2.0337027064509074 
Epoch [5/10] Batch 2200/7568 Train_loss 2.036286705342383 
Epoch [5/10] Batch 2300/7568 Train_loss 2.033593349518179 
Epoch [5/10] Batch 2400/7568 Train_loss 2.0345088207736604 
Epoch [5/10] Batch 2500/7568 Train_loss 2.03434374926687 
Epoch [5/10] Batch 2600/7568 Train_loss 2.02984650755034 
Epoch [5/10] Batch 2700/7568 Train_loss 2.0265582020152104 
Epoch [5/10] Batch 2800/7568 Train_loss 2.022654370606605 
Epoch [5/10] Batch 2900/7568 Train_loss 2.0232066986469186 
Epoch [5/10] Batch 3000/7568 Train_loss 2.020730849619867 
Epoch [5/10] Batch 3100/7568 Train_loss 2.0224154030111903 
Epoch [5/10] Batch 3200/7568 Train_loss 2.0199912068192862 
Epoch [5/10] Batch 3300/7568 Train_loss 2.0181867328603063 
Epoch [5/10] Batch 3400/7568 Train_loss 2.0172563850905956 
Epoch [5/10] Batch 3500/7568 Train_loss 2.0159884902093452 
Epoch [5/10] Batch 3600/7568 Train_loss 2.0143578946383522 
Epoch [5/10] Batch 3700/7568 Train_loss 2.0150302445741963 
Epoch [5/10] Batch 3800/7568 Train_loss 2.0159573315931163 
Epoch [5/10] Batch 3900/7568 Train_loss 2.014028477962125 
Epoch [5/10] Batch 4000/7568 Train_loss 2.012263181670789 
Epoch [5/10] Batch 4100/7568 Train_loss 2.0128698506040767 
Epoch [5/10] Batch 4200/7568 Train_loss 2.0126241115842993 
Epoch [5/10] Batch 4300/7568 Train_loss 2.013751459025389 
Epoch [5/10] Batch 4400/7568 Train_loss 2.014390384726323 
Epoch [5/10] Batch 4500/7568 Train_loss 2.013712616310838 
Epoch [5/10] Batch 4600/7568 Train_loss 2.0157261893162906 
Epoch [5/10] Batch 4700/7568 Train_loss 2.0160545005370545 
Epoch [5/10] Batch 4800/7568 Train_loss 2.0169335026569453 
Epoch [5/10] Batch 4900/7568 Train_loss 2.0151833268792694 
Epoch [5/10] Batch 5000/7568 Train_loss 2.0146628817250267 
Epoch [5/10] Batch 5100/7568 Train_loss 2.0133892720182884 
Epoch [5/10] Batch 5200/7568 Train_loss 2.013430246104531 
Epoch [5/10] Batch 5300/7568 Train_loss 2.013441578461705 
Epoch [5/10] Batch 5400/7568 Train_loss 2.01537522377846 
Epoch [5/10] Batch 5500/7568 Train_loss 2.0141767783585167 
Epoch [5/10] Batch 5600/7568 Train_loss 2.012407056411299 
Epoch [5/10] Batch 5700/7568 Train_loss 2.0114970858045638 
Epoch [5/10] Batch 5800/7568 Train_loss 2.011676862221959 
Epoch [5/10] Batch 5900/7568 Train_loss 2.0113800816052323 
Epoch [5/10] Batch 6000/7568 Train_loss 2.0119165061881317 
Fold: 1/5 
Epoch [0/10] Batch 0/7568 Train_loss 7.160665988922119 

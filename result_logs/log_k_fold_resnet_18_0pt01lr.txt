Fold: 1/5 
Epoch [0/10] Batch 0/7168 Train_loss 7.143373489379883 
Epoch [0/10] Batch 100/7168 Train_loss 2.838963329201878 
Epoch [0/10] Batch 200/7168 Train_loss 2.648606692899519 
Epoch [0/10] Batch 300/7168 Train_loss 2.5536604631580784 
Epoch [0/10] Batch 400/7168 Train_loss 2.498411855606961 
Epoch [0/10] Batch 500/7168 Train_loss 2.4854588774804345 
Epoch [0/10] Batch 600/7168 Train_loss 2.450094729066887 
Epoch [0/10] Batch 700/7168 Train_loss 2.4338215119743483 
Epoch [0/10] Batch 800/7168 Train_loss 2.4090279979205755 
Epoch [0/10] Batch 900/7168 Train_loss 2.4012674558308227 
Epoch [0/10] Batch 1000/7168 Train_loss 2.3749272687570913 
Epoch [0/10] Batch 1100/7168 Train_loss 2.3613588124188807 
Epoch [0/10] Batch 1200/7168 Train_loss 2.347410054851035 
Epoch [0/10] Batch 1300/7168 Train_loss 2.3405085941583352 
Epoch [0/10] Batch 1400/7168 Train_loss 2.3260191661025864 
Epoch [0/10] Batch 1500/7168 Train_loss 2.312917219081535 
Epoch [0/10] Batch 1600/7168 Train_loss 2.3074593748210894 
Epoch [0/10] Batch 1700/7168 Train_loss 2.292344200915971 
Epoch [0/10] Batch 1800/7168 Train_loss 2.2900293458011136 
Epoch [0/10] Batch 1900/7168 Train_loss 2.2829195953854633 
Epoch [0/10] Batch 2000/7168 Train_loss 2.2759646281175527 
Epoch [0/10] Batch 2100/7168 Train_loss 2.2659822897023667 
Epoch [0/10] Batch 2200/7168 Train_loss 2.2615000640788114 
Epoch [0/10] Batch 2300/7168 Train_loss 2.2549201688303837 
Epoch [0/10] Batch 2400/7168 Train_loss 2.2527905593609523 
Epoch [0/10] Batch 2500/7168 Train_loss 2.2499729843973304 
Epoch [0/10] Batch 2600/7168 Train_loss 2.248422463626093 
Epoch [0/10] Batch 2700/7168 Train_loss 2.24538754195979 
Epoch [0/10] Batch 2800/7168 Train_loss 2.2441411025553504 
Epoch [0/10] Batch 2900/7168 Train_loss 2.244064725629834 
Epoch [0/10] Batch 3000/7168 Train_loss 2.243351825760905 
Epoch [0/10] Batch 3100/7168 Train_loss 2.2414073267076446 
Epoch [0/10] Batch 3200/7168 Train_loss 2.238903657225697 
Epoch [0/10] Batch 3300/7168 Train_loss 2.236857390004698 
Epoch [0/10] Batch 3400/7168 Train_loss 2.2327999843372663 
Epoch [0/10] Batch 3500/7168 Train_loss 2.2332741229687714 
Epoch [0/10] Batch 3600/7168 Train_loss 2.2333191993153116 
Epoch [0/10] Batch 3700/7168 Train_loss 2.2327458574840486 
Epoch [0/10] Batch 3800/7168 Train_loss 2.233305308253349 
Epoch [0/10] Batch 3900/7168 Train_loss 2.232575668807489 
Epoch [0/10] Batch 4000/7168 Train_loss 2.2302614747211655 
Epoch [0/10] Batch 4100/7168 Train_loss 2.228130668172805 
Epoch [0/10] Batch 4200/7168 Train_loss 2.226523345913157 
Epoch [0/10] Batch 4300/7168 Train_loss 2.22607945086609 
Epoch [0/10] Batch 4400/7168 Train_loss 2.225304694541122 
Epoch [0/10] Batch 4500/7168 Train_loss 2.225858165578984 
Epoch [0/10] Batch 4600/7168 Train_loss 2.2249591377922258 
Epoch [0/10] Batch 4700/7168 Train_loss 2.221702033124105 
Epoch [0/10] Batch 4800/7168 Train_loss 2.2228484138321862 
Epoch [0/10] Batch 4900/7168 Train_loss 2.2226249628336032 
Epoch [0/10] Batch 5000/7168 Train_loss 2.2219965816694267 
Epoch [0/10] Batch 5100/7168 Train_loss 2.220920668533329 
Epoch [0/10] Batch 5200/7168 Train_loss 2.2193125488551346 
Epoch [0/10] Batch 5300/7168 Train_loss 2.2175167627749275 
Epoch [0/10] Batch 5400/7168 Train_loss 2.2184683780491605 
Epoch [0/10] Batch 5500/7168 Train_loss 2.2165440216489847 
Epoch [0/10] Batch 5600/7168 Train_loss 2.2156319020316335 
Epoch [0/10] Batch 5700/7168 Train_loss 2.2138903746007306 
Epoch [0/10] Batch 5800/7168 Train_loss 2.2120792040345463 
Epoch [0/10] Batch 5900/7168 Train_loss 2.2113618459833453 
Epoch [0/10] Batch 6000/7168 Train_loss 2.210804145833469 
Epoch [0/10] Batch 6100/7168 Train_loss 2.2101193326962734 
Epoch [0/10] Batch 6200/7168 Train_loss 2.2110952739585046 
Epoch [0/10] Batch 6300/7168 Train_loss 2.210416326663207 
Epoch [0/10] Batch 6400/7168 Train_loss 2.210502882063659 
Epoch [0/10] Batch 6500/7168 Train_loss 2.210162149034836 
Epoch [0/10] Batch 6600/7168 Train_loss 2.210874310974695 
Epoch [0/10] Batch 6700/7168 Train_loss 2.211213380258451 
Epoch [0/10] Batch 6800/7168 Train_loss 2.2108228697906154 
Epoch [0/10] Batch 6900/7168 Train_loss 2.2110543784724164 
Epoch [0/10] Batch 7000/7168 Train_loss 2.210717697626113 
Epoch [0/10] Batch 7100/7168 Train_loss 2.209878192495551 
Epoch: 0/10 	Training Loss: 2.209679 	Validation Loss: 2.171438 Duration seconds: 958.8313410282135 
Validation loss decreased (inf --> 2.171438).  Saving model ... 
best_valid_loss_fold [2.171438303593147] Best_Epoch [0]Epoch [1/10] Batch 0/7168 Train_loss 2.2052036225795746 
Epoch [1/10] Batch 100/7168 Train_loss 2.1142477261843067 
Epoch [1/10] Batch 200/7168 Train_loss 2.1844362111678763 
Epoch [1/10] Batch 300/7168 Train_loss 2.165303575586639 
Epoch [1/10] Batch 400/7168 Train_loss 2.187023998384464 
Epoch [1/10] Batch 500/7168 Train_loss 2.1854089161830985 
Epoch [1/10] Batch 600/7168 Train_loss 2.205916268134276 
Epoch [1/10] Batch 700/7168 Train_loss 2.192848702512692 
Epoch [1/10] Batch 800/7168 Train_loss 2.185351788513967 
Epoch [1/10] Batch 900/7168 Train_loss 2.1929278275975905 
Epoch [1/10] Batch 1000/7168 Train_loss 2.1805710317222746 
Epoch [1/10] Batch 1100/7168 Train_loss 2.1787643164639903 
Epoch [1/10] Batch 1200/7168 Train_loss 2.179842124647344 
Epoch [1/10] Batch 1300/7168 Train_loss 2.186707305642479 
Epoch [1/10] Batch 1400/7168 Train_loss 2.187369267411695 
Epoch [1/10] Batch 1500/7168 Train_loss 2.1843923692362535 
Epoch [1/10] Batch 1600/7168 Train_loss 2.183063154450288 
Epoch [1/10] Batch 1700/7168 Train_loss 2.185754167996807 
Epoch [1/10] Batch 1800/7168 Train_loss 2.186725800903488 
Epoch [1/10] Batch 1900/7168 Train_loss 2.1859695687929244 
Epoch [1/10] Batch 2000/7168 Train_loss 2.183633589524141 
Epoch [1/10] Batch 2100/7168 Train_loss 2.1833820414792804 
Epoch [1/10] Batch 2200/7168 Train_loss 2.1801566891118322 
Epoch [1/10] Batch 2300/7168 Train_loss 2.177377300922221 
Epoch [1/10] Batch 2400/7168 Train_loss 2.176760514326912 
Epoch [1/10] Batch 2500/7168 Train_loss 2.174210032073558 
Epoch [1/10] Batch 2600/7168 Train_loss 2.1775499926088684 
Epoch [1/10] Batch 2700/7168 Train_loss 2.1805974131643837 
Epoch [1/10] Batch 2800/7168 Train_loss 2.181694163504944 
Epoch [1/10] Batch 2900/7168 Train_loss 2.1842968586661495 
Epoch [1/10] Batch 3000/7168 Train_loss 2.1817958417017436 
Epoch [1/10] Batch 3100/7168 Train_loss 2.1836137770405126 
Epoch [1/10] Batch 3200/7168 Train_loss 2.1872491906468325 
Epoch [1/10] Batch 3300/7168 Train_loss 2.186836987300484 
Epoch [1/10] Batch 3400/7168 Train_loss 2.1871720783037847 
Epoch [1/10] Batch 3500/7168 Train_loss 2.1867956071283365 
Epoch [1/10] Batch 3600/7168 Train_loss 2.1851696803963074 
Epoch [1/10] Batch 3700/7168 Train_loss 2.1833391650661653 
Epoch [1/10] Batch 3800/7168 Train_loss 2.1858298872229738 
Epoch [1/10] Batch 3900/7168 Train_loss 2.185552046142215 
Epoch [1/10] Batch 4000/7168 Train_loss 2.1821659928156536 
Epoch [1/10] Batch 4100/7168 Train_loss 2.1822662099785295 
Epoch [1/10] Batch 4200/7168 Train_loss 2.180571485629141 
Epoch [1/10] Batch 4300/7168 Train_loss 2.179736076254147 
Epoch [1/10] Batch 4400/7168 Train_loss 2.1818671787402706 
Epoch [1/10] Batch 4500/7168 Train_loss 2.1829779583019726 
Epoch [1/10] Batch 4600/7168 Train_loss 2.1825592993873792 
Epoch [1/10] Batch 4700/7168 Train_loss 2.1848480914075736 
Epoch [1/10] Batch 4800/7168 Train_loss 2.1850544948753927 
Epoch [1/10] Batch 4900/7168 Train_loss 2.182716732547128 
Epoch [1/10] Batch 5000/7168 Train_loss 2.1812420030595825 
Epoch [1/10] Batch 5100/7168 Train_loss 2.180314108326234 
Epoch [1/10] Batch 5200/7168 Train_loss 2.179414986922314 
Epoch [1/10] Batch 5300/7168 Train_loss 2.17886023930369 
Epoch [1/10] Batch 5400/7168 Train_loss 2.1800771501026337 
Epoch [1/10] Batch 5500/7168 Train_loss 2.178929012889776 
Epoch [1/10] Batch 5600/7168 Train_loss 2.1785869897263237 
Epoch [1/10] Batch 5700/7168 Train_loss 2.1779023993801516 
Epoch [1/10] Batch 5800/7168 Train_loss 2.177997648297271 
Epoch [1/10] Batch 5900/7168 Train_loss 2.1769159830004416 
Epoch [1/10] Batch 6000/7168 Train_loss 2.1771652811920617 
Epoch [1/10] Batch 6100/7168 Train_loss 2.1780872456344733 
Epoch [1/10] Batch 6200/7168 Train_loss 2.1792977996178933 
Epoch [1/10] Batch 6300/7168 Train_loss 2.177730421746294 
Epoch [1/10] Batch 6400/7168 Train_loss 2.1763955210660737 
Epoch [1/10] Batch 6500/7168 Train_loss 2.1775611803116677 
Epoch [1/10] Batch 6600/7168 Train_loss 2.1777401756285863 
Epoch [1/10] Batch 6700/7168 Train_loss 2.1800905012329235 
Epoch [1/10] Batch 6800/7168 Train_loss 2.179384775566031 
Epoch [1/10] Batch 6900/7168 Train_loss 2.1784766512756812 
Epoch [1/10] Batch 7000/7168 Train_loss 2.1784530347086775 
Epoch [1/10] Batch 7100/7168 Train_loss 2.178850125454795 
Epoch: 1/10 	Training Loss: 2.179560 	Validation Loss: 2.172428 Duration seconds: 946.1934192180634 
best_valid_loss_fold [2.171438303593147] Best_Epoch [1]Epoch [2/10] Batch 0/7168 Train_loss 2.8240957856178284 
Epoch [2/10] Batch 100/7168 Train_loss 2.210312694458678 
Epoch [2/10] Batch 200/7168 Train_loss 2.2462177669527517 
Epoch [2/10] Batch 300/7168 Train_loss 2.245186998004533 
Epoch [2/10] Batch 400/7168 Train_loss 2.2396892963204893 
Epoch [2/10] Batch 500/7168 Train_loss 2.208108978714058 
Epoch [2/10] Batch 600/7168 Train_loss 2.2112480443150746 
Epoch [2/10] Batch 700/7168 Train_loss 2.2067609469747747 
Epoch [2/10] Batch 800/7168 Train_loss 2.196115730109435 
Epoch [2/10] Batch 900/7168 Train_loss 2.2038724586020297 
Epoch [2/10] Batch 1000/7168 Train_loss 2.1992212398485704 
Epoch [2/10] Batch 1100/7168 Train_loss 2.1993992487129357 
Epoch [2/10] Batch 1200/7168 Train_loss 2.1959063186807297 
Epoch [2/10] Batch 1300/7168 Train_loss 2.1951354793247857 
Epoch [2/10] Batch 1400/7168 Train_loss 2.190502693691822 
Epoch [2/10] Batch 1500/7168 Train_loss 2.187314235780892 
Epoch [2/10] Batch 1600/7168 Train_loss 2.1796027050678766 
Epoch [2/10] Batch 1700/7168 Train_loss 2.1765019688131106 
Epoch [2/10] Batch 1800/7168 Train_loss 2.1773880802876815 
Epoch [2/10] Batch 1900/7168 Train_loss 2.17705114943018 
Epoch [2/10] Batch 2000/7168 Train_loss 2.1745056739409288 
Epoch [2/10] Batch 2100/7168 Train_loss 2.174529009762405 
Epoch [2/10] Batch 2200/7168 Train_loss 2.178339896751555 
Epoch [2/10] Batch 2300/7168 Train_loss 2.177315176679538 
Epoch [2/10] Batch 2400/7168 Train_loss 2.174424154622661 
Epoch [2/10] Batch 2500/7168 Train_loss 2.17597514063251 
Epoch [2/10] Batch 2600/7168 Train_loss 2.1751380427634888 
Epoch [2/10] Batch 2700/7168 Train_loss 2.1759400310140324 
Epoch [2/10] Batch 2800/7168 Train_loss 2.174933713218094 
Epoch [2/10] Batch 2900/7168 Train_loss 2.172776340195246 
Epoch [2/10] Batch 3000/7168 Train_loss 2.172268620533611 
Epoch [2/10] Batch 3100/7168 Train_loss 2.1738616336510668 
Epoch [2/10] Batch 3200/7168 Train_loss 2.1733592655911216 
Epoch [2/10] Batch 3300/7168 Train_loss 2.172155172937352 
Epoch [2/10] Batch 3400/7168 Train_loss 2.1716864056550067 
Epoch [2/10] Batch 3500/7168 Train_loss 2.1732343625627357 
Epoch [2/10] Batch 3600/7168 Train_loss 2.1738363230345943 
Epoch [2/10] Batch 3700/7168 Train_loss 2.17429722126063 
Epoch [2/10] Batch 3800/7168 Train_loss 2.1759146029730903 
Epoch [2/10] Batch 3900/7168 Train_loss 2.175782239878676 
Epoch [2/10] Batch 4000/7168 Train_loss 2.1772033969064677 
Epoch [2/10] Batch 4100/7168 Train_loss 2.1782862009710873 
Epoch [2/10] Batch 4200/7168 Train_loss 2.1779823765048696 
Epoch [2/10] Batch 4300/7168 Train_loss 2.178402436428141 
Epoch [2/10] Batch 4400/7168 Train_loss 2.178492482190402 
Epoch [2/10] Batch 4500/7168 Train_loss 2.179197796803134 
Epoch [2/10] Batch 4600/7168 Train_loss 2.1769366898023157 
Epoch [2/10] Batch 4700/7168 Train_loss 2.176585206942238 
Epoch [2/10] Batch 4800/7168 Train_loss 2.1788466710289227 
Epoch [2/10] Batch 4900/7168 Train_loss 2.1794194812964136 
Epoch [2/10] Batch 5000/7168 Train_loss 2.1786493085320293 
Epoch [2/10] Batch 5100/7168 Train_loss 2.1803190998646866 
Epoch [2/10] Batch 5200/7168 Train_loss 2.1802800916131013 
Epoch [2/10] Batch 5300/7168 Train_loss 2.1805705193347964 
Epoch [2/10] Batch 5400/7168 Train_loss 2.1799787614405584 
Epoch [2/10] Batch 5500/7168 Train_loss 2.1787372412025614 
Epoch [2/10] Batch 5600/7168 Train_loss 2.177293169149393 
Epoch [2/10] Batch 5700/7168 Train_loss 2.1794847868823353 
Epoch [2/10] Batch 5800/7168 Train_loss 2.178348154471755 
Epoch [2/10] Batch 5900/7168 Train_loss 2.178617266149283 
Epoch [2/10] Batch 6000/7168 Train_loss 2.1794862684543443 
Epoch [2/10] Batch 6100/7168 Train_loss 2.1792965908799675 
Epoch [2/10] Batch 6200/7168 Train_loss 2.1784592596725423 
Epoch [2/10] Batch 6300/7168 Train_loss 2.177219174351792 
Epoch [2/10] Batch 6400/7168 Train_loss 2.1783887417432646 
Epoch [2/10] Batch 6500/7168 Train_loss 2.1787742514680706 
Epoch [2/10] Batch 6600/7168 Train_loss 2.179331616585299 
Epoch [2/10] Batch 6700/7168 Train_loss 2.1795269847027385 
Epoch [2/10] Batch 6800/7168 Train_loss 2.179982710721177 
Epoch [2/10] Batch 6900/7168 Train_loss 2.1811011927643644 
Epoch [2/10] Batch 7000/7168 Train_loss 2.1816436035316205 
Epoch [2/10] Batch 7100/7168 Train_loss 2.1805418014039524 
Epoch: 2/10 	Training Loss: 2.179907 	Validation Loss: 2.171156 Duration seconds: 945.4084315299988 
Validation loss decreased (2.171438 --> 2.171156).  Saving model ... 
best_valid_loss_fold [2.1711555798288567] Best_Epoch [2]Epoch [3/10] Batch 0/7168 Train_loss 2.7663031220436096 
Epoch [3/10] Batch 100/7168 Train_loss 2.169605226504921 
Epoch [3/10] Batch 200/7168 Train_loss 2.1729996007176773 
Epoch [3/10] Batch 300/7168 Train_loss 2.1683423760721454 
Epoch [3/10] Batch 400/7168 Train_loss 2.1834727974975494 
Epoch [3/10] Batch 500/7168 Train_loss 2.1943559557080508 
Epoch [3/10] Batch 600/7168 Train_loss 2.205866434411479 
Epoch [3/10] Batch 700/7168 Train_loss 2.197472293796111 
Epoch [3/10] Batch 800/7168 Train_loss 2.187402142251327 
Epoch [3/10] Batch 900/7168 Train_loss 2.189841734516237 
Epoch [3/10] Batch 1000/7168 Train_loss 2.1924804506095854 
Epoch [3/10] Batch 1100/7168 Train_loss 2.1955975437440403 
Epoch [3/10] Batch 1200/7168 Train_loss 2.1879249487342087 
Epoch [3/10] Batch 1300/7168 Train_loss 2.1873001722213767 
Epoch [3/10] Batch 1400/7168 Train_loss 2.18936389717649 
Epoch [3/10] Batch 1500/7168 Train_loss 2.193610754055154 
Epoch [3/10] Batch 1600/7168 Train_loss 2.19004201896484 
Epoch [3/10] Batch 1700/7168 Train_loss 2.190289975728167 
Epoch [3/10] Batch 1800/7168 Train_loss 2.1929998586222172 
Epoch [3/10] Batch 1900/7168 Train_loss 2.1924182203554716 
Epoch [3/10] Batch 2000/7168 Train_loss 2.1893407447986872 
Epoch [3/10] Batch 2100/7168 Train_loss 2.190706415304906 
Epoch [3/10] Batch 2200/7168 Train_loss 2.189066793595266 
Epoch [3/10] Batch 2300/7168 Train_loss 2.183364854622851 
Epoch [3/10] Batch 2400/7168 Train_loss 2.182230237795382 
Epoch [3/10] Batch 2500/7168 Train_loss 2.1860376602170564 
Epoch [3/10] Batch 2600/7168 Train_loss 2.187337055286734 
Epoch [3/10] Batch 2700/7168 Train_loss 2.1870507353414035 
Epoch [3/10] Batch 2800/7168 Train_loss 2.1853212692223885 
Epoch [3/10] Batch 2900/7168 Train_loss 2.1828209526351467 
Epoch [3/10] Batch 3000/7168 Train_loss 2.1832182602469663 
Epoch [3/10] Batch 3100/7168 Train_loss 2.184126558264622 
Epoch [3/10] Batch 3200/7168 Train_loss 2.184525679672409 
Epoch [3/10] Batch 3300/7168 Train_loss 2.178703280618898 
Epoch [3/10] Batch 3400/7168 Train_loss 2.1791664509431854 
Epoch [3/10] Batch 3500/7168 Train_loss 2.179971619380675 
Epoch [3/10] Batch 3600/7168 Train_loss 2.178155870234526 
Epoch [3/10] Batch 3700/7168 Train_loss 2.17935830855959 
Epoch [3/10] Batch 3800/7168 Train_loss 2.176315668608696 
Epoch [3/10] Batch 3900/7168 Train_loss 2.176663178861737 
Epoch [3/10] Batch 4000/7168 Train_loss 2.1798720374103784 
Epoch [3/10] Batch 4100/7168 Train_loss 2.179296509383423 
Epoch [3/10] Batch 4200/7168 Train_loss 2.176141217226813 
Epoch [3/10] Batch 4300/7168 Train_loss 2.178291519456369 
Epoch [3/10] Batch 4400/7168 Train_loss 2.177159718947366 
Epoch [3/10] Batch 4500/7168 Train_loss 2.1773795632223796 
Epoch [3/10] Batch 4600/7168 Train_loss 2.1780428386879755 
Epoch [3/10] Batch 4700/7168 Train_loss 2.1785862689017237 
Epoch [3/10] Batch 4800/7168 Train_loss 2.178951737949486 
Epoch [3/10] Batch 4900/7168 Train_loss 2.177647603829645 
Epoch [3/10] Batch 5000/7168 Train_loss 2.17781199398958 
Epoch [3/10] Batch 5100/7168 Train_loss 2.178842289713966 
Epoch [3/10] Batch 5200/7168 Train_loss 2.179546907094322 
Epoch [3/10] Batch 5300/7168 Train_loss 2.1793570688109964 
Epoch [3/10] Batch 5400/7168 Train_loss 2.179758636995811 
Epoch [3/10] Batch 5500/7168 Train_loss 2.1802790588253176 
Epoch [3/10] Batch 5600/7168 Train_loss 2.1792113411572336 
Epoch [3/10] Batch 5700/7168 Train_loss 2.1792786474437844 
Epoch [3/10] Batch 5800/7168 Train_loss 2.1782074943043574 
Epoch [3/10] Batch 5900/7168 Train_loss 2.177003913500151 
Epoch [3/10] Batch 6000/7168 Train_loss 2.17623048121632 
Epoch [3/10] Batch 6100/7168 Train_loss 2.1767848694428054 
Epoch [3/10] Batch 6200/7168 Train_loss 2.1767978668405132 
Epoch [3/10] Batch 6300/7168 Train_loss 2.177388170688819 
Epoch [3/10] Batch 6400/7168 Train_loss 2.178755657498007 
Epoch [3/10] Batch 6500/7168 Train_loss 2.1793054017199496 
Epoch [3/10] Batch 6600/7168 Train_loss 2.179545207224801 
Epoch [3/10] Batch 6700/7168 Train_loss 2.180510945812728 
Epoch [3/10] Batch 6800/7168 Train_loss 2.1808297970162265 
Epoch [3/10] Batch 6900/7168 Train_loss 2.18099186820684 
Epoch [3/10] Batch 7000/7168 Train_loss 2.180319704278216 
Epoch [3/10] Batch 7100/7168 Train_loss 2.1798412140667094 
Epoch: 3/10 	Training Loss: 2.179503 	Validation Loss: 2.171136 Duration seconds: 944.5942463874817 
Validation loss decreased (2.171156 --> 2.171136).  Saving model ... 
best_valid_loss_fold [2.1711360796471126] Best_Epoch [3]Epoch [4/10] Batch 0/7168 Train_loss 1.2230924516916275 
Epoch [4/10] Batch 100/7168 Train_loss 2.256525912792376 
Epoch [4/10] Batch 200/7168 Train_loss 2.263019013390019 
Epoch [4/10] Batch 300/7168 Train_loss 2.2330494618594052 
Epoch [4/10] Batch 400/7168 Train_loss 2.2128552967622097 
Epoch [4/10] Batch 500/7168 Train_loss 2.2054645860801916 
Epoch [4/10] Batch 600/7168 Train_loss 2.1945726473547653 
Epoch [4/10] Batch 700/7168 Train_loss 2.2084003361672715 
Epoch [4/10] Batch 800/7168 Train_loss 2.2067712315273345 
Epoch [4/10] Batch 900/7168 Train_loss 2.199263731958334 
Epoch [4/10] Batch 1000/7168 Train_loss 2.202422586711613 
Epoch [4/10] Batch 1100/7168 Train_loss 2.199933099730463 
Epoch [4/10] Batch 1200/7168 Train_loss 2.1936998919558266 
Epoch [4/10] Batch 1300/7168 Train_loss 2.1994898326231094 
Epoch [4/10] Batch 1400/7168 Train_loss 2.1968653105693914 
Epoch [4/10] Batch 1500/7168 Train_loss 2.1954334526261357 
Epoch [4/10] Batch 1600/7168 Train_loss 2.194296738306185 
Epoch [4/10] Batch 1700/7168 Train_loss 2.1938796633135076 
Epoch [4/10] Batch 1800/7168 Train_loss 2.196471790500246 
Epoch [4/10] Batch 1900/7168 Train_loss 2.196609956662571 
Epoch [4/10] Batch 2000/7168 Train_loss 2.1956573608426795 
Epoch [4/10] Batch 2100/7168 Train_loss 2.1906439677370213 
Epoch [4/10] Batch 2200/7168 Train_loss 2.193052271674677 
Epoch [4/10] Batch 2300/7168 Train_loss 2.1927206802697143 
Epoch [4/10] Batch 2400/7168 Train_loss 2.190356897884535 
Epoch [4/10] Batch 2500/7168 Train_loss 2.187331574159067 
Epoch [4/10] Batch 2600/7168 Train_loss 2.184241322262706 
Epoch [4/10] Batch 2700/7168 Train_loss 2.183381076522864 
Epoch [4/10] Batch 2800/7168 Train_loss 2.1864977989961574 
Epoch [4/10] Batch 2900/7168 Train_loss 2.186869697960563 
Epoch [4/10] Batch 3000/7168 Train_loss 2.1880027116268566 
Epoch [4/10] Batch 3100/7168 Train_loss 2.1867031177102807 
Epoch [4/10] Batch 3200/7168 Train_loss 2.187345441697315 
Epoch [4/10] Batch 3300/7168 Train_loss 2.191027102326668 
Epoch [4/10] Batch 3400/7168 Train_loss 2.191790287760523 
Epoch [4/10] Batch 3500/7168 Train_loss 2.192596824266712 
Epoch [4/10] Batch 3600/7168 Train_loss 2.1932341754477873 
Epoch [4/10] Batch 3700/7168 Train_loss 2.1900751910574274 
Epoch [4/10] Batch 3800/7168 Train_loss 2.190922343752096 
Epoch [4/10] Batch 3900/7168 Train_loss 2.188746404190211 
Epoch [4/10] Batch 4000/7168 Train_loss 2.1891244211492764 
Epoch [4/10] Batch 4100/7168 Train_loss 2.1875104346825593 
Epoch [4/10] Batch 4200/7168 Train_loss 2.1892197901714248 
Epoch [4/10] Batch 4300/7168 Train_loss 2.1873148410673946 
Epoch [4/10] Batch 4400/7168 Train_loss 2.188691596427145 
Epoch [4/10] Batch 4500/7168 Train_loss 2.1884002527927615 
Epoch [4/10] Batch 4600/7168 Train_loss 2.1880822258393575 
Epoch [4/10] Batch 4700/7168 Train_loss 2.1874580829162795 
Epoch [4/10] Batch 4800/7168 Train_loss 2.186458080532128 
Epoch [4/10] Batch 4900/7168 Train_loss 2.186714212439226 
Epoch [4/10] Batch 5000/7168 Train_loss 2.186007025634687 
Epoch [4/10] Batch 5100/7168 Train_loss 2.184159098022523 
Epoch [4/10] Batch 5200/7168 Train_loss 2.1825599976254986 
Epoch [4/10] Batch 5300/7168 Train_loss 2.182643331841711 
Epoch [4/10] Batch 5400/7168 Train_loss 2.184637151624048 
Epoch [4/10] Batch 5500/7168 Train_loss 2.1848874913494365 
Epoch [4/10] Batch 5600/7168 Train_loss 2.1852307741135664 
Epoch [4/10] Batch 5700/7168 Train_loss 2.185061205948911 
Epoch [4/10] Batch 5800/7168 Train_loss 2.186026548585631 
Epoch [4/10] Batch 5900/7168 Train_loss 2.186801392819251 
Epoch [4/10] Batch 6000/7168 Train_loss 2.187107021752307 
Epoch [4/10] Batch 6100/7168 Train_loss 2.187879212143245 
Epoch [4/10] Batch 6200/7168 Train_loss 2.1869317913652333 
Epoch [4/10] Batch 6300/7168 Train_loss 2.186083041038083 
Epoch [4/10] Batch 6400/7168 Train_loss 2.1854099659943205 
Epoch [4/10] Batch 6500/7168 Train_loss 2.1848853993596453 
Epoch [4/10] Batch 6600/7168 Train_loss 2.1833277734016545 
Epoch [4/10] Batch 6700/7168 Train_loss 2.1825582228868936 
Epoch [4/10] Batch 6800/7168 Train_loss 2.1825813998230905 
Epoch [4/10] Batch 6900/7168 Train_loss 2.1823691997454144 
Epoch [4/10] Batch 7000/7168 Train_loss 2.181165651585218 
Epoch [4/10] Batch 7100/7168 Train_loss 2.180027400539922 
Epoch: 4/10 	Training Loss: 2.179458 	Validation Loss: 2.171018 Duration seconds: 959.8309681415558 
Validation loss decreased (2.171136 --> 2.171018).  Saving model ... 
best_valid_loss_fold [2.17101803352125] Best_Epoch [4]Epoch [5/10] Batch 0/7168 Train_loss 1.1710955947637558 
Epoch [5/10] Batch 100/7168 Train_loss 2.171781907724862 
Epoch [5/10] Batch 200/7168 Train_loss 2.1943231164222925 
Epoch [5/10] Batch 300/7168 Train_loss 2.185455481475374 
Epoch [5/10] Batch 400/7168 Train_loss 2.1793720832192096 
Epoch [5/10] Batch 500/7168 Train_loss 2.1959937424835805 
Epoch [5/10] Batch 600/7168 Train_loss 2.1994403628214028 
Epoch [5/10] Batch 700/7168 Train_loss 2.202947489736594 
Epoch [5/10] Batch 800/7168 Train_loss 2.19809493275543 
Epoch [5/10] Batch 900/7168 Train_loss 2.2005373757171314 
Epoch [5/10] Batch 1000/7168 Train_loss 2.2018367879755134 
Epoch [5/10] Batch 1100/7168 Train_loss 2.2022706032015864 
Epoch [5/10] Batch 1200/7168 Train_loss 2.2023493537274526 
Epoch [5/10] Batch 1300/7168 Train_loss 2.194132864944849 
Epoch [5/10] Batch 1400/7168 Train_loss 2.1960178775969443 
Epoch [5/10] Batch 1500/7168 Train_loss 2.197174386758553 
Epoch [5/10] Batch 1600/7168 Train_loss 2.200199113431608 
Epoch [5/10] Batch 1700/7168 Train_loss 2.202913090192452 
Epoch [5/10] Batch 1800/7168 Train_loss 2.20094353941035 
Epoch [5/10] Batch 1900/7168 Train_loss 2.1979458122144306 
Epoch [5/10] Batch 2000/7168 Train_loss 2.1945787635238574 
Epoch [5/10] Batch 2100/7168 Train_loss 2.1922746533683117 
Epoch [5/10] Batch 2200/7168 Train_loss 2.1917235026003175 
Epoch [5/10] Batch 2300/7168 Train_loss 2.190992610997762 
Epoch [5/10] Batch 2400/7168 Train_loss 2.187826946650828 
Epoch [5/10] Batch 2500/7168 Train_loss 2.1861007425342738 
Epoch [5/10] Batch 2600/7168 Train_loss 2.1903865616260147 
Epoch [5/10] Batch 2700/7168 Train_loss 2.188339795846932 
Epoch [5/10] Batch 2800/7168 Train_loss 2.189160829296243 
Epoch [5/10] Batch 2900/7168 Train_loss 2.1876251451499047 
Epoch [5/10] Batch 3000/7168 Train_loss 2.186906741623559 
Epoch [5/10] Batch 3100/7168 Train_loss 2.1851884427377386 
Epoch [5/10] Batch 3200/7168 Train_loss 2.1835739752327195 
Epoch [5/10] Batch 3300/7168 Train_loss 2.1855869401958414 
Epoch [5/10] Batch 3400/7168 Train_loss 2.1843835553529507 
Epoch [5/10] Batch 3500/7168 Train_loss 2.183296509178595 
Epoch [5/10] Batch 3600/7168 Train_loss 2.18461038492531 
Epoch [5/10] Batch 3700/7168 Train_loss 2.182094956698111 
Epoch [5/10] Batch 3800/7168 Train_loss 2.179426715812913 
Epoch [5/10] Batch 3900/7168 Train_loss 2.181635941222398 
Epoch [5/10] Batch 4000/7168 Train_loss 2.1822796607614903 
Epoch [5/10] Batch 4100/7168 Train_loss 2.182143547006044 
Epoch [5/10] Batch 4200/7168 Train_loss 2.1813889838519254 
Epoch [5/10] Batch 4300/7168 Train_loss 2.1805761319015002 
Epoch [5/10] Batch 4400/7168 Train_loss 2.180176255202407 
Epoch [5/10] Batch 4500/7168 Train_loss 2.1780492754359693 
Epoch [5/10] Batch 4600/7168 Train_loss 2.180093146700285 
Epoch [5/10] Batch 4700/7168 Train_loss 2.178900527164191 
Epoch [5/10] Batch 4800/7168 Train_loss 2.180424012341516 
Epoch [5/10] Batch 4900/7168 Train_loss 2.182724829721368 
Epoch [5/10] Batch 5000/7168 Train_loss 2.182541411936915 
Epoch [5/10] Batch 5100/7168 Train_loss 2.180563874441931 
Epoch [5/10] Batch 5200/7168 Train_loss 2.1785999835397023 
Epoch [5/10] Batch 5300/7168 Train_loss 2.1807677975113133 
Epoch [5/10] Batch 5400/7168 Train_loss 2.180332127786791 
Epoch [5/10] Batch 5500/7168 Train_loss 2.180520132240914 
Epoch [5/10] Batch 5600/7168 Train_loss 2.1824921353111053 
Epoch [5/10] Batch 5700/7168 Train_loss 2.1819441717061174 
Epoch [5/10] Batch 5800/7168 Train_loss 2.181955775263042 
Epoch [5/10] Batch 5900/7168 Train_loss 2.1814305492223998 
Epoch [5/10] Batch 6000/7168 Train_loss 2.181361279829187 
Epoch [5/10] Batch 6100/7168 Train_loss 2.181542885340622 
Epoch [5/10] Batch 6200/7168 Train_loss 2.1811596980509766 
Epoch [5/10] Batch 6300/7168 Train_loss 2.1809950020767888 
Epoch [5/10] Batch 6400/7168 Train_loss 2.1799956528893896 
Epoch [5/10] Batch 6500/7168 Train_loss 2.1806763832962637 
Epoch [5/10] Batch 6600/7168 Train_loss 2.1808079658692723 
Epoch [5/10] Batch 6700/7168 Train_loss 2.1805052664044755 
Epoch [5/10] Batch 6800/7168 Train_loss 2.1801891432270692 
Epoch [5/10] Batch 6900/7168 Train_loss 2.1808646533488707 
Epoch [5/10] Batch 7000/7168 Train_loss 2.1810264901175973 
Epoch [5/10] Batch 7100/7168 Train_loss 2.179737153441163 
Epoch: 5/10 	Training Loss: 2.179586 	Validation Loss: 2.171467 Duration seconds: 954.7327077388763 
best_valid_loss_fold [2.17101803352125] Best_Epoch [5]Epoch [6/10] Batch 0/7168 Train_loss 2.843361496925354 
Epoch [6/10] Batch 100/7168 Train_loss 2.09568520731265 
Epoch [6/10] Batch 200/7168 Train_loss 2.1164794266223907 
Epoch [6/10] Batch 300/7168 Train_loss 2.1353787005541727 
Epoch [6/10] Batch 400/7168 Train_loss 2.138181756670933 
Epoch [6/10] Batch 500/7168 Train_loss 2.168633655427459 
Epoch [6/10] Batch 600/7168 Train_loss 2.1789371434245846 
Epoch [6/10] Batch 700/7168 Train_loss 2.1845786395707245 
Epoch [6/10] Batch 800/7168 Train_loss 2.184378704272406 
Epoch [6/10] Batch 900/7168 Train_loss 2.1844355203004575 
Epoch [6/10] Batch 1000/7168 Train_loss 2.188130165566574 
Epoch [6/10] Batch 1100/7168 Train_loss 2.183408706933666 
Epoch [6/10] Batch 1200/7168 Train_loss 2.1816869072622302 
Epoch [6/10] Batch 1300/7168 Train_loss 2.1842650468994527 
Epoch [6/10] Batch 1400/7168 Train_loss 2.1783688617105743 
Epoch [6/10] Batch 1500/7168 Train_loss 2.178037824390889 
Epoch [6/10] Batch 1600/7168 Train_loss 2.1799735439653922 
Epoch [6/10] Batch 1700/7168 Train_loss 2.1771838940868373 
Epoch [6/10] Batch 1800/7168 Train_loss 2.176702928908794 
Epoch [6/10] Batch 1900/7168 Train_loss 2.1736430894898966 
Epoch [6/10] Batch 2000/7168 Train_loss 2.17604642913587 
Epoch [6/10] Batch 2100/7168 Train_loss 2.177391682619824 
Epoch [6/10] Batch 2200/7168 Train_loss 2.1828591642475628 
Epoch [6/10] Batch 2300/7168 Train_loss 2.182215193933324 
Epoch [6/10] Batch 2400/7168 Train_loss 2.1831926669629103 
Epoch [6/10] Batch 2500/7168 Train_loss 2.1825354516816016 
Epoch [6/10] Batch 2600/7168 Train_loss 2.1782181890354115 
Epoch [6/10] Batch 2700/7168 Train_loss 2.1736660883553336 
Epoch [6/10] Batch 2800/7168 Train_loss 2.1727922627411753 
Epoch [6/10] Batch 2900/7168 Train_loss 2.1756224002418167 
Epoch [6/10] Batch 3000/7168 Train_loss 2.175431112023005 
Epoch [6/10] Batch 3100/7168 Train_loss 2.177437442549772 
Epoch [6/10] Batch 3200/7168 Train_loss 2.1792665762161696 
Epoch [6/10] Batch 3300/7168 Train_loss 2.1774331429029226 
Epoch [6/10] Batch 3400/7168 Train_loss 2.176851734400707 
Epoch [6/10] Batch 3500/7168 Train_loss 2.1768723681709554 
Epoch [6/10] Batch 3600/7168 Train_loss 2.177498248183075 
Epoch [6/10] Batch 3700/7168 Train_loss 2.178671259198502 
Epoch [6/10] Batch 3800/7168 Train_loss 2.178516439374137 
Epoch [6/10] Batch 3900/7168 Train_loss 2.1758907898721924 
Epoch [6/10] Batch 4000/7168 Train_loss 2.1771599584946006 
Epoch [6/10] Batch 4100/7168 Train_loss 2.179666327045069 
Epoch [6/10] Batch 4200/7168 Train_loss 2.1788173716384946 
Epoch [6/10] Batch 4300/7168 Train_loss 2.1806127677626788 
Epoch [6/10] Batch 4400/7168 Train_loss 2.1812098251367478 
Epoch [6/10] Batch 4500/7168 Train_loss 2.181577714790347 
Epoch [6/10] Batch 4600/7168 Train_loss 2.180939181065124 
Epoch [6/10] Batch 4700/7168 Train_loss 2.183044243828207 
Epoch [6/10] Batch 4800/7168 Train_loss 2.1841800306128154 
Epoch [6/10] Batch 4900/7168 Train_loss 2.1823522365535917 
Epoch [6/10] Batch 5000/7168 Train_loss 2.1819908603492486 
Epoch [6/10] Batch 5100/7168 Train_loss 2.1836002324842 
Epoch [6/10] Batch 5200/7168 Train_loss 2.1839430440024583 
Epoch [6/10] Batch 5300/7168 Train_loss 2.183762919709404 
Epoch [6/10] Batch 5400/7168 Train_loss 2.181062182231184 
Epoch [6/10] Batch 5500/7168 Train_loss 2.179974281049451 
Epoch [6/10] Batch 5600/7168 Train_loss 2.1791037719598987 
Epoch [6/10] Batch 5700/7168 Train_loss 2.1780923644590118 
Epoch [6/10] Batch 5800/7168 Train_loss 2.1793812710372484 
Epoch [6/10] Batch 5900/7168 Train_loss 2.178798362264248 
Epoch [6/10] Batch 6000/7168 Train_loss 2.1782628270392217 
Epoch [6/10] Batch 6100/7168 Train_loss 2.178914923525564 
Epoch [6/10] Batch 6200/7168 Train_loss 2.179567990027076 
Epoch [6/10] Batch 6300/7168 Train_loss 2.1792251822343425 
Epoch [6/10] Batch 6400/7168 Train_loss 2.179386668888466 
Epoch [6/10] Batch 6500/7168 Train_loss 2.178986797851611 
Epoch [6/10] Batch 6600/7168 Train_loss 2.179736779180734 
Epoch [6/10] Batch 6700/7168 Train_loss 2.179414713781657 
Epoch [6/10] Batch 6800/7168 Train_loss 2.1802224394565575 
Epoch [6/10] Batch 6900/7168 Train_loss 2.180191846509081 
Epoch [6/10] Batch 7000/7168 Train_loss 2.179213760601863 
Epoch [6/10] Batch 7100/7168 Train_loss 2.17874949719558 
Epoch: 6/10 	Training Loss: 2.179384 	Validation Loss: 2.173083 Duration seconds: 949.7896342277527 
best_valid_loss_fold [2.17101803352125] Best_Epoch [6]Epoch [7/10] Batch 0/7168 Train_loss 2.6500585675239563 
Epoch [7/10] Batch 100/7168 Train_loss 2.0903978806616057 
Epoch [7/10] Batch 200/7168 Train_loss 2.1172023179074424 
Epoch [7/10] Batch 300/7168 Train_loss 2.1181422605665023 
Epoch [7/10] Batch 400/7168 Train_loss 2.14788044405697 
Epoch [7/10] Batch 500/7168 Train_loss 2.1370768701720855 
Epoch [7/10] Batch 600/7168 Train_loss 2.155465235726012 
Epoch [7/10] Batch 700/7168 Train_loss 2.1631917319182152 
Epoch [7/10] Batch 800/7168 Train_loss 2.158003051331576 
Epoch [7/10] Batch 900/7168 Train_loss 2.1560936604732945 
Epoch [7/10] Batch 1000/7168 Train_loss 2.156506042693045 
Epoch [7/10] Batch 1100/7168 Train_loss 2.1552981612197275 
Epoch [7/10] Batch 1200/7168 Train_loss 2.1520998528061264 
Epoch [7/10] Batch 1300/7168 Train_loss 2.163263704630616 
Epoch [7/10] Batch 1400/7168 Train_loss 2.1625347052575177 
Epoch [7/10] Batch 1500/7168 Train_loss 2.1627837943184938 
Epoch [7/10] Batch 1600/7168 Train_loss 2.163290040361591 
Epoch [7/10] Batch 1700/7168 Train_loss 2.1679817669658643 
Epoch [7/10] Batch 1800/7168 Train_loss 2.167120072235669 
Epoch [7/10] Batch 1900/7168 Train_loss 2.1697168572287007 
Epoch [7/10] Batch 2000/7168 Train_loss 2.174324530033932 
Epoch [7/10] Batch 2100/7168 Train_loss 2.1776673057671108 
Epoch [7/10] Batch 2200/7168 Train_loss 2.174934042395109 
Epoch [7/10] Batch 2300/7168 Train_loss 2.1757664845552926 
Epoch [7/10] Batch 2400/7168 Train_loss 2.177744462453093 
Epoch [7/10] Batch 2500/7168 Train_loss 2.177364341065627 
Epoch [7/10] Batch 2600/7168 Train_loss 2.1794578826047055 
Epoch [7/10] Batch 2700/7168 Train_loss 2.1793054296920937 
Epoch [7/10] Batch 2800/7168 Train_loss 2.1786319429599486 
Epoch [7/10] Batch 2900/7168 Train_loss 2.179682112842418 
Epoch [7/10] Batch 3000/7168 Train_loss 2.1813343210851537 
Epoch [7/10] Batch 3100/7168 Train_loss 2.1813257238973307 
Epoch [7/10] Batch 3200/7168 Train_loss 2.183497481301813 
Epoch [7/10] Batch 3300/7168 Train_loss 2.1847781447777783 
Epoch [7/10] Batch 3400/7168 Train_loss 2.1867436230603263 
Epoch [7/10] Batch 3500/7168 Train_loss 2.1847475632722975 
Epoch [7/10] Batch 3600/7168 Train_loss 2.1849476504254692 
Epoch [7/10] Batch 3700/7168 Train_loss 2.1826724746531005 
Epoch [7/10] Batch 3800/7168 Train_loss 2.1813733250263394 
Epoch [7/10] Batch 3900/7168 Train_loss 2.182744362187887 
Epoch [7/10] Batch 4000/7168 Train_loss 2.1825446672988993 
Epoch [7/10] Batch 4100/7168 Train_loss 2.185348762216088 
Epoch [7/10] Batch 4200/7168 Train_loss 2.18472120911415 
Epoch [7/10] Batch 4300/7168 Train_loss 2.182888519438969 
Epoch [7/10] Batch 4400/7168 Train_loss 2.182917238835908 
Epoch [7/10] Batch 4500/7168 Train_loss 2.1815809836601634 
Epoch [7/10] Batch 4600/7168 Train_loss 2.1809035408460584 
Epoch [7/10] Batch 4700/7168 Train_loss 2.1799759497207116 
Epoch [7/10] Batch 4800/7168 Train_loss 2.1784380370379735 
Epoch [7/10] Batch 4900/7168 Train_loss 2.1770092637301515 
Epoch [7/10] Batch 5000/7168 Train_loss 2.179076147750482 
Epoch [7/10] Batch 5100/7168 Train_loss 2.1783000176308365 
Epoch [7/10] Batch 5200/7168 Train_loss 2.1782789847938355 
Epoch [7/10] Batch 5300/7168 Train_loss 2.1766258087490575 
Epoch [7/10] Batch 5400/7168 Train_loss 2.1768315436469914 
Epoch [7/10] Batch 5500/7168 Train_loss 2.176849964536812 
Epoch [7/10] Batch 5600/7168 Train_loss 2.1772559998908485 
Epoch [7/10] Batch 5700/7168 Train_loss 2.177606731041589 
Epoch [7/10] Batch 5800/7168 Train_loss 2.177981230613487 
Epoch [7/10] Batch 5900/7168 Train_loss 2.178639590492735 
Epoch [7/10] Batch 6000/7168 Train_loss 2.178010831158214 
Epoch [7/10] Batch 6100/7168 Train_loss 2.176135017395684 
Epoch [7/10] Batch 6200/7168 Train_loss 2.1763616679556197 
Epoch [7/10] Batch 6300/7168 Train_loss 2.1775937104268674 
Epoch [7/10] Batch 6400/7168 Train_loss 2.1775827559642056 
Epoch [7/10] Batch 6500/7168 Train_loss 2.1789204267020814 
Epoch [7/10] Batch 6600/7168 Train_loss 2.178616409483886 
Epoch [7/10] Batch 6700/7168 Train_loss 2.1811107388429187 
Epoch [7/10] Batch 6800/7168 Train_loss 2.1804394590501452 
Epoch [7/10] Batch 6900/7168 Train_loss 2.180012900103194 
Epoch [7/10] Batch 7000/7168 Train_loss 2.179502545872036 
Epoch [7/10] Batch 7100/7168 Train_loss 2.180491398819029 
Epoch: 7/10 	Training Loss: 2.179489 	Validation Loss: 2.172140 Duration seconds: 949.7928528785706 
best_valid_loss_fold [2.17101803352125] Best_Epoch [7]Epoch [8/10] Batch 0/7168 Train_loss 1.946168690919876 
Epoch [8/10] Batch 100/7168 Train_loss 2.051205421143239 
Epoch [8/10] Batch 200/7168 Train_loss 2.196648019314998 
Epoch [8/10] Batch 300/7168 Train_loss 2.190401300639409 
Epoch [8/10] Batch 400/7168 Train_loss 2.1743986043995456 
Epoch [8/10] Batch 500/7168 Train_loss 2.182757167848284 
Epoch [8/10] Batch 600/7168 Train_loss 2.168351488556719 
Epoch [8/10] Batch 700/7168 Train_loss 2.178383568214112 
Epoch [8/10] Batch 800/7168 Train_loss 2.182993075988266 
Epoch [8/10] Batch 900/7168 Train_loss 2.1768969759659287 
Epoch [8/10] Batch 1000/7168 Train_loss 2.1809541999728053 
Epoch [8/10] Batch 1100/7168 Train_loss 2.179855020085754 
Epoch [8/10] Batch 1200/7168 Train_loss 2.1817193614577968 
Epoch [8/10] Batch 1300/7168 Train_loss 2.179892254759952 
Epoch [8/10] Batch 1400/7168 Train_loss 2.177497556780084 
Epoch [8/10] Batch 1500/7168 Train_loss 2.1902074963648586 
Epoch [8/10] Batch 1600/7168 Train_loss 2.184228410596478 
Epoch [8/10] Batch 1700/7168 Train_loss 2.1836810103064073 
Epoch [8/10] Batch 1800/7168 Train_loss 2.184696268333719 
Epoch [8/10] Batch 1900/7168 Train_loss 2.185089952463793 
Epoch [8/10] Batch 2000/7168 Train_loss 2.185255602776617 
Epoch [8/10] Batch 2100/7168 Train_loss 2.1864360390760287 
Epoch [8/10] Batch 2200/7168 Train_loss 2.1859447330723887 
Epoch [8/10] Batch 2300/7168 Train_loss 2.1884977893627813 
Epoch [8/10] Batch 2400/7168 Train_loss 2.190628552600673 
Epoch [8/10] Batch 2500/7168 Train_loss 2.191207403959822 
Epoch [8/10] Batch 2600/7168 Train_loss 2.193815502975822 
Epoch [8/10] Batch 2700/7168 Train_loss 2.195408662511481 
Epoch [8/10] Batch 2800/7168 Train_loss 2.194845384038035 
Epoch [8/10] Batch 2900/7168 Train_loss 2.1938767014742306 
Epoch [8/10] Batch 3000/7168 Train_loss 2.193387282670279 
Epoch [8/10] Batch 3100/7168 Train_loss 2.190435725122796 
Epoch [8/10] Batch 3200/7168 Train_loss 2.1894448749099586 
Epoch [8/10] Batch 3300/7168 Train_loss 2.187737357373564 
Epoch [8/10] Batch 3400/7168 Train_loss 2.1905080964682 
Epoch [8/10] Batch 3500/7168 Train_loss 2.191534835576024 
Epoch [8/10] Batch 3600/7168 Train_loss 2.1908285435919166 
Epoch [8/10] Batch 3700/7168 Train_loss 2.1944812068228012 
Epoch [8/10] Batch 3800/7168 Train_loss 2.192358418248478 
Epoch [8/10] Batch 3900/7168 Train_loss 2.188515161030172 
Epoch [8/10] Batch 4000/7168 Train_loss 2.1878501347186297 
Epoch [8/10] Batch 4100/7168 Train_loss 2.185870599318673 
Epoch [8/10] Batch 4200/7168 Train_loss 2.1854553771065803 
Epoch [8/10] Batch 4300/7168 Train_loss 2.1841556443409487 
Epoch [8/10] Batch 4400/7168 Train_loss 2.183918706536076 
Epoch [8/10] Batch 4500/7168 Train_loss 2.183923437790827 
Epoch [8/10] Batch 4600/7168 Train_loss 2.185289678567649 
Epoch [8/10] Batch 4700/7168 Train_loss 2.187167929017501 
Epoch [8/10] Batch 4800/7168 Train_loss 2.1882012904224184 
Epoch [8/10] Batch 4900/7168 Train_loss 2.187505595916584 
Epoch [8/10] Batch 5000/7168 Train_loss 2.1847858571202914 
Epoch [8/10] Batch 5100/7168 Train_loss 2.184506776963372 
Epoch [8/10] Batch 5200/7168 Train_loss 2.186634249654304 
Epoch [8/10] Batch 5300/7168 Train_loss 2.185857979824264 
Epoch [8/10] Batch 5400/7168 Train_loss 2.1863119338943084 
Epoch [8/10] Batch 5500/7168 Train_loss 2.1854788293444747 
Epoch [8/10] Batch 5600/7168 Train_loss 2.1842715413128557 
Epoch [8/10] Batch 5700/7168 Train_loss 2.1840286319573874 
Epoch [8/10] Batch 5800/7168 Train_loss 2.1838422700791003 
Epoch [8/10] Batch 5900/7168 Train_loss 2.18524185390447 
Epoch [8/10] Batch 6000/7168 Train_loss 2.1860158411294615 
Epoch [8/10] Batch 6100/7168 Train_loss 2.1862654940180457 
Epoch [8/10] Batch 6200/7168 Train_loss 2.1845975263866375 
Epoch [8/10] Batch 6300/7168 Train_loss 2.182366272732864 
Epoch [8/10] Batch 6400/7168 Train_loss 2.181948330803339 
Epoch [8/10] Batch 6500/7168 Train_loss 2.181897895315247 
Epoch [8/10] Batch 6600/7168 Train_loss 2.181806515571006 
Epoch [8/10] Batch 6700/7168 Train_loss 2.181918143107542 
Epoch [8/10] Batch 6800/7168 Train_loss 2.181326917588597 
Epoch [8/10] Batch 6900/7168 Train_loss 2.181453463999176 
Epoch [8/10] Batch 7000/7168 Train_loss 2.1804218550691976 
Epoch [8/10] Batch 7100/7168 Train_loss 2.1801149123253207 
Epoch: 8/10 	Training Loss: 2.179584 	Validation Loss: 2.170926 Duration seconds: 950.9196634292603 
Validation loss decreased (2.171018 --> 2.170926).  Saving model ... 
best_valid_loss_fold [2.170926495272267] Best_Epoch [8]Epoch [9/10] Batch 0/7168 Train_loss 1.5932394564151764 
Epoch [9/10] Batch 100/7168 Train_loss 2.2234379330483995 
Epoch [9/10] Batch 200/7168 Train_loss 2.1785098903659565 
Epoch [9/10] Batch 300/7168 Train_loss 2.1715175068358645 
Epoch [9/10] Batch 400/7168 Train_loss 2.176244581615241 
Epoch [9/10] Batch 500/7168 Train_loss 2.1776760292207884 
Epoch [9/10] Batch 600/7168 Train_loss 2.1759351222665853 
Epoch [9/10] Batch 700/7168 Train_loss 2.189945293943824 
Epoch [9/10] Batch 800/7168 Train_loss 2.187362526835276 
Epoch [9/10] Batch 900/7168 Train_loss 2.179896087198623 
Epoch [9/10] Batch 1000/7168 Train_loss 2.1797661247340354 
Epoch [9/10] Batch 1100/7168 Train_loss 2.184002738273436 
Epoch [9/10] Batch 1200/7168 Train_loss 2.1797860244430174 
Epoch [9/10] Batch 1300/7168 Train_loss 2.179587610884871 
Epoch [9/10] Batch 1400/7168 Train_loss 2.1838100536392044 
Epoch [9/10] Batch 1500/7168 Train_loss 2.1791364948484278 
Epoch [9/10] Batch 1600/7168 Train_loss 2.178860981015024 
Epoch [9/10] Batch 1700/7168 Train_loss 2.178257815255129 
Epoch [9/10] Batch 1800/7168 Train_loss 2.177423919855655 
Epoch [9/10] Batch 1900/7168 Train_loss 2.176431783997718 
Epoch [9/10] Batch 2000/7168 Train_loss 2.1796286992285623 
Epoch [9/10] Batch 2100/7168 Train_loss 2.1794885302628297 
Epoch [9/10] Batch 2200/7168 Train_loss 2.18365970848366 
Epoch [9/10] Batch 2300/7168 Train_loss 2.1801648260471045 
Epoch [9/10] Batch 2400/7168 Train_loss 2.1819974870879864 
Epoch [9/10] Batch 2500/7168 Train_loss 2.183187218403063 
Epoch [9/10] Batch 2600/7168 Train_loss 2.1828917074792433 
Epoch [9/10] Batch 2700/7168 Train_loss 2.1826884739434704 
Epoch [9/10] Batch 2800/7168 Train_loss 2.1830610995822957 
Epoch [9/10] Batch 2900/7168 Train_loss 2.1872424397467336 
Epoch [9/10] Batch 3000/7168 Train_loss 2.1875020178386824 
Epoch [9/10] Batch 3100/7168 Train_loss 2.187122440446734 
Epoch [9/10] Batch 3200/7168 Train_loss 2.1848663130483565 
Epoch [9/10] Batch 3300/7168 Train_loss 2.1856177342060805 
Epoch [9/10] Batch 3400/7168 Train_loss 2.1849153073210466 
Epoch [9/10] Batch 3500/7168 Train_loss 2.183135705708742 
Epoch [9/10] Batch 3600/7168 Train_loss 2.181811367958925 
Epoch [9/10] Batch 3700/7168 Train_loss 2.1842324568859084 
Epoch [9/10] Batch 3800/7168 Train_loss 2.1818235148821525 
Epoch [9/10] Batch 3900/7168 Train_loss 2.1815135489630717 
Epoch [9/10] Batch 4000/7168 Train_loss 2.1823385833829856 
Epoch [9/10] Batch 4100/7168 Train_loss 2.1823886636952543 
Epoch [9/10] Batch 4200/7168 Train_loss 2.184355953418763 
Epoch [9/10] Batch 4300/7168 Train_loss 2.1838417613583925 
Epoch [9/10] Batch 4400/7168 Train_loss 2.1815082325539352 
Epoch [9/10] Batch 4500/7168 Train_loss 2.1831450611683985 
Epoch [9/10] Batch 4600/7168 Train_loss 2.1831197502450617 
Epoch [9/10] Batch 4700/7168 Train_loss 2.1828590278465985 
Epoch [9/10] Batch 4800/7168 Train_loss 2.1826767758965815 
Epoch [9/10] Batch 4900/7168 Train_loss 2.1836575470337256 
Epoch [9/10] Batch 5000/7168 Train_loss 2.1841176449090476 
Epoch [9/10] Batch 5100/7168 Train_loss 2.184178632463317 
Epoch [9/10] Batch 5200/7168 Train_loss 2.181516027629914 
Epoch [9/10] Batch 5300/7168 Train_loss 2.1798328022122764 
Epoch [9/10] Batch 5400/7168 Train_loss 2.179552103728856 
Epoch [9/10] Batch 5500/7168 Train_loss 2.1791262196272294 
Epoch [9/10] Batch 5600/7168 Train_loss 2.1793271350985104 
Epoch [9/10] Batch 5700/7168 Train_loss 2.1812628852580853 
Epoch [9/10] Batch 5800/7168 Train_loss 2.18153702802513 
Epoch [9/10] Batch 5900/7168 Train_loss 2.1807838923354166 
Epoch [9/10] Batch 6000/7168 Train_loss 2.180496215611731 
Epoch [9/10] Batch 6100/7168 Train_loss 2.1795432760253965 
Epoch [9/10] Batch 6200/7168 Train_loss 2.179001473386602 
Epoch [9/10] Batch 6300/7168 Train_loss 2.1793748310772583 
Epoch [9/10] Batch 6400/7168 Train_loss 2.1796004591607763 
Epoch [9/10] Batch 6500/7168 Train_loss 2.1798201562592365 
Epoch [9/10] Batch 6600/7168 Train_loss 2.1802968082396736 
Epoch [9/10] Batch 6700/7168 Train_loss 2.1804747737387085 
Epoch [9/10] Batch 6800/7168 Train_loss 2.1815412107254195 
Epoch [9/10] Batch 6900/7168 Train_loss 2.1811624231601585 
Epoch [9/10] Batch 7000/7168 Train_loss 2.180546870735301 
Epoch [9/10] Batch 7100/7168 Train_loss 2.180669149631783 
Epoch: 9/10 	Training Loss: 2.179517 	Validation Loss: 2.172493 Duration seconds: 969.8215224742889 
best_valid_loss_fold [2.170926495272267] Best_Epoch [9]Fold: 2/5 
Epoch [0/10] Batch 0/7168 Train_loss 1.987040489912033 
Epoch [0/10] Batch 100/7168 Train_loss 2.1746214663628303 
Epoch [0/10] Batch 200/7168 Train_loss 2.1662142465067147 
Epoch [0/10] Batch 300/7168 Train_loss 2.166621981071079 
Epoch [0/10] Batch 400/7168 Train_loss 2.1610160907753686 
Epoch [0/10] Batch 500/7168 Train_loss 2.185917751726038 
Epoch [0/10] Batch 600/7168 Train_loss 2.1720484272265397 
Epoch [0/10] Batch 700/7168 Train_loss 2.1735546024736765 
Epoch [0/10] Batch 800/7168 Train_loss 2.1632241414913373 
Epoch [0/10] Batch 900/7168 Train_loss 2.1624922611803377 
Epoch [0/10] Batch 1000/7168 Train_loss 2.154318026372126 
Epoch [0/10] Batch 1100/7168 Train_loss 2.154712752996522 
Epoch [0/10] Batch 1200/7168 Train_loss 2.1534419850718467 
Epoch [0/10] Batch 1300/7168 Train_loss 2.154809664491046 
Epoch [0/10] Batch 1400/7168 Train_loss 2.1522855229521545 
Epoch [0/10] Batch 1500/7168 Train_loss 2.1558600818014875 
Epoch [0/10] Batch 1600/7168 Train_loss 2.1544672254191424 
Epoch [0/10] Batch 1700/7168 Train_loss 2.1547041471164974 
Epoch [0/10] Batch 1800/7168 Train_loss 2.1544012439740756 
Epoch [0/10] Batch 1900/7168 Train_loss 2.151717844172756 
Epoch [0/10] Batch 2000/7168 Train_loss 2.15290111193116 
Epoch [0/10] Batch 2100/7168 Train_loss 2.15063352256602 
Epoch [0/10] Batch 2200/7168 Train_loss 2.1550535535213893 
Epoch [0/10] Batch 2300/7168 Train_loss 2.1541698836194905 
Epoch [0/10] Batch 2400/7168 Train_loss 2.157050877070983 
Epoch [0/10] Batch 2500/7168 Train_loss 2.1552330157200084 
Epoch [0/10] Batch 2600/7168 Train_loss 2.155401525372315 
Epoch [0/10] Batch 2700/7168 Train_loss 2.1585089309221726 
Epoch [0/10] Batch 2800/7168 Train_loss 2.1610922418782135 
Epoch [0/10] Batch 2900/7168 Train_loss 2.1612635557146493 
Epoch [0/10] Batch 3000/7168 Train_loss 2.1642636749067927 
Epoch [0/10] Batch 3100/7168 Train_loss 2.1658012432865235 
Epoch [0/10] Batch 3200/7168 Train_loss 2.1671819366847527 
Epoch [0/10] Batch 3300/7168 Train_loss 2.169663581237653 
Epoch [0/10] Batch 3400/7168 Train_loss 2.1700511161228095 
Epoch [0/10] Batch 3500/7168 Train_loss 2.1687405381508125 
Epoch [0/10] Batch 3600/7168 Train_loss 2.169250753485504 
Epoch [0/10] Batch 3700/7168 Train_loss 2.1696803315374793 
Epoch [0/10] Batch 3800/7168 Train_loss 2.1713308703621825 
Epoch [0/10] Batch 3900/7168 Train_loss 2.1704684846319013 
Epoch [0/10] Batch 4000/7168 Train_loss 2.1711827172313263 
Epoch [0/10] Batch 4100/7168 Train_loss 2.168024828381813 
Epoch [0/10] Batch 4200/7168 Train_loss 2.16838149377361 
Epoch [0/10] Batch 4300/7168 Train_loss 2.1681267619167643 
Epoch [0/10] Batch 4400/7168 Train_loss 2.1684160022925476 
Epoch [0/10] Batch 4500/7168 Train_loss 2.1684626502954703 
Epoch [0/10] Batch 4600/7168 Train_loss 2.1687148613913942 
Epoch [0/10] Batch 4700/7168 Train_loss 2.168324040706628 
Epoch [0/10] Batch 4800/7168 Train_loss 2.1688110612284714 
Epoch [0/10] Batch 4900/7168 Train_loss 2.1686652532446704 
Epoch [0/10] Batch 5000/7168 Train_loss 2.168457708542191 
Epoch [0/10] Batch 5100/7168 Train_loss 2.1676554190974637 
Epoch [0/10] Batch 5200/7168 Train_loss 2.166408692353896 
Epoch [0/10] Batch 5300/7168 Train_loss 2.1658449597801495 
Epoch [0/10] Batch 5400/7168 Train_loss 2.1661764387868327 
Epoch [0/10] Batch 5500/7168 Train_loss 2.1670397081752837 
Epoch [0/10] Batch 5600/7168 Train_loss 2.1673135513164086 
Epoch [0/10] Batch 5700/7168 Train_loss 2.1689073114128745 
Epoch [0/10] Batch 5800/7168 Train_loss 2.1683350211201073 
Epoch [0/10] Batch 5900/7168 Train_loss 2.1689392927414763 
Epoch [0/10] Batch 6000/7168 Train_loss 2.1676917670741913 
Epoch [0/10] Batch 6100/7168 Train_loss 2.1686118085000228 
Epoch [0/10] Batch 6200/7168 Train_loss 2.1704674936402055 
Epoch [0/10] Batch 6300/7168 Train_loss 2.170701815772295 
Epoch [0/10] Batch 6400/7168 Train_loss 2.170917183682241 
Epoch [0/10] Batch 6500/7168 Train_loss 2.171289636181182 
Epoch [0/10] Batch 6600/7168 Train_loss 2.1717461785269117 
Epoch [0/10] Batch 6700/7168 Train_loss 2.171592052801921 
Epoch [0/10] Batch 6800/7168 Train_loss 2.1729541518678213 
Epoch [0/10] Batch 6900/7168 Train_loss 2.1726740965582705 
Epoch [0/10] Batch 7000/7168 Train_loss 2.17394104066993 
Epoch [0/10] Batch 7100/7168 Train_loss 2.174406453637636 
Epoch: 0/10 	Training Loss: 2.174404 	Validation Loss: 2.191866 Duration seconds: 950.6572036743164 
Validation loss decreased (inf --> 2.191866).  Saving model ... 
best_valid_loss_fold [2.1918655882306797] Best_Epoch [0]Epoch [1/10] Batch 0/7168 Train_loss 2.268299162387848 
Epoch [1/10] Batch 100/7168 Train_loss 2.176736608737766 
Epoch [1/10] Batch 200/7168 Train_loss 2.2003523920157657 
Epoch [1/10] Batch 300/7168 Train_loss 2.1976818874527844 
Epoch [1/10] Batch 400/7168 Train_loss 2.192629997309604 
Epoch [1/10] Batch 500/7168 Train_loss 2.184870580951135 
Epoch [1/10] Batch 600/7168 Train_loss 2.1989441577487696 
Epoch [1/10] Batch 700/7168 Train_loss 2.2042911911061758 
Epoch [1/10] Batch 800/7168 Train_loss 2.203458963745691 
Epoch [1/10] Batch 900/7168 Train_loss 2.206828966902178 
Epoch [1/10] Batch 1000/7168 Train_loss 2.204419883725407 
Epoch [1/10] Batch 1100/7168 Train_loss 2.1999046075869213 
Epoch [1/10] Batch 1200/7168 Train_loss 2.1908755373398927 
Epoch [1/10] Batch 1300/7168 Train_loss 2.1924968921707557 
Epoch [1/10] Batch 1400/7168 Train_loss 2.1917095669225484 
Epoch [1/10] Batch 1500/7168 Train_loss 2.1889463144489163 
Epoch [1/10] Batch 1600/7168 Train_loss 2.185996741959261 
Epoch [1/10] Batch 1700/7168 Train_loss 2.183906215164747 
Epoch [1/10] Batch 1800/7168 Train_loss 2.178561103130499 
Epoch [1/10] Batch 1900/7168 Train_loss 2.1747007690907028 
Epoch [1/10] Batch 2000/7168 Train_loss 2.177207490716917 
Epoch [1/10] Batch 2100/7168 Train_loss 2.1724421716073987 
Epoch [1/10] Batch 2200/7168 Train_loss 2.172236052231321 
Epoch [1/10] Batch 2300/7168 Train_loss 2.1742328632621337 
Epoch [1/10] Batch 2400/7168 Train_loss 2.171968552828927 
Epoch [1/10] Batch 2500/7168 Train_loss 2.172527299528835 
Epoch [1/10] Batch 2600/7168 Train_loss 2.1700152491535514 
Epoch [1/10] Batch 2700/7168 Train_loss 2.1698677918009297 
Epoch [1/10] Batch 2800/7168 Train_loss 2.169711339882296 
Epoch [1/10] Batch 2900/7168 Train_loss 2.1691887167220445 
Epoch [1/10] Batch 3000/7168 Train_loss 2.168529382157389 
Epoch [1/10] Batch 3100/7168 Train_loss 2.1672450714786375 
Epoch [1/10] Batch 3200/7168 Train_loss 2.1669915375980278 
Epoch [1/10] Batch 3300/7168 Train_loss 2.169242283587238 
Epoch [1/10] Batch 3400/7168 Train_loss 2.168608445524118 
Epoch [1/10] Batch 3500/7168 Train_loss 2.167766617663722 
Epoch [1/10] Batch 3600/7168 Train_loss 2.1704367738991968 
Epoch [1/10] Batch 3700/7168 Train_loss 2.169432198637339 
Epoch [1/10] Batch 3800/7168 Train_loss 2.169916959863153 
Epoch [1/10] Batch 3900/7168 Train_loss 2.1688457529144816 
Epoch [1/10] Batch 4000/7168 Train_loss 2.1681699018401224 
Epoch [1/10] Batch 4100/7168 Train_loss 2.1672266409785768 
Epoch [1/10] Batch 4200/7168 Train_loss 2.1671552267976386 
Epoch [1/10] Batch 4300/7168 Train_loss 2.1677225058167737 
Epoch [1/10] Batch 4400/7168 Train_loss 2.170104531419329 
Epoch [1/10] Batch 4500/7168 Train_loss 2.170111600792135 
Epoch [1/10] Batch 4600/7168 Train_loss 2.171689174912256 
Epoch [1/10] Batch 4700/7168 Train_loss 2.173375865895047 
Epoch [1/10] Batch 4800/7168 Train_loss 2.1734891446853175 
Epoch [1/10] Batch 4900/7168 Train_loss 2.1753615339303156 
Epoch [1/10] Batch 5000/7168 Train_loss 2.175189917983472 
Epoch [1/10] Batch 5100/7168 Train_loss 2.1755781022941707 
Epoch [1/10] Batch 5200/7168 Train_loss 2.175540663882119 
Epoch [1/10] Batch 5300/7168 Train_loss 2.1776006194813657 
Epoch [1/10] Batch 5400/7168 Train_loss 2.1773721765636447 
Epoch [1/10] Batch 5500/7168 Train_loss 2.1769681490701753 
Epoch [1/10] Batch 5600/7168 Train_loss 2.177169964614031 
Epoch [1/10] Batch 5700/7168 Train_loss 2.176768102341212 
Epoch [1/10] Batch 5800/7168 Train_loss 2.1763220671907906 
Epoch [1/10] Batch 5900/7168 Train_loss 2.1754614385866025 
Epoch [1/10] Batch 6000/7168 Train_loss 2.1767658093306963 
Epoch [1/10] Batch 6100/7168 Train_loss 2.1754360851465218 
Epoch [1/10] Batch 6200/7168 Train_loss 2.175499046823033 
Epoch [1/10] Batch 6300/7168 Train_loss 2.174181860614875 
Epoch [1/10] Batch 6400/7168 Train_loss 2.175063980375765 
Epoch [1/10] Batch 6500/7168 Train_loss 2.1744638327620907 
Epoch [1/10] Batch 6600/7168 Train_loss 2.174009145651599 
Epoch [1/10] Batch 6700/7168 Train_loss 2.1734624441311157 
Epoch [1/10] Batch 6800/7168 Train_loss 2.1739158944585992 
Epoch [1/10] Batch 6900/7168 Train_loss 2.1750929647317885 
Epoch [1/10] Batch 7000/7168 Train_loss 2.174065303264422 
Epoch [1/10] Batch 7100/7168 Train_loss 2.1748257657349606 
Epoch: 1/10 	Training Loss: 2.174377 	Validation Loss: 2.193618 Duration seconds: 958.2088844776154 
best_valid_loss_fold [2.1918655882306797] Best_Epoch [1]Epoch [2/10] Batch 0/7168 Train_loss 2.5462785363197327 
Epoch [2/10] Batch 100/7168 Train_loss 2.1561123297946287 
Epoch [2/10] Batch 200/7168 Train_loss 2.1581975437367142 
Epoch [2/10] Batch 300/7168 Train_loss 2.1549664766982546 
Epoch [2/10] Batch 400/7168 Train_loss 2.1716761447842283 
Epoch [2/10] Batch 500/7168 Train_loss 2.161717367713561 
Epoch [2/10] Batch 600/7168 Train_loss 2.171745930059182 
Epoch [2/10] Batch 700/7168 Train_loss 2.1760673470231846 
Epoch [2/10] Batch 800/7168 Train_loss 2.1733087065923824 
Epoch [2/10] Batch 900/7168 Train_loss 2.171152715677691 
Epoch [2/10] Batch 1000/7168 Train_loss 2.1644272690975703 
Epoch [2/10] Batch 1100/7168 Train_loss 2.162395884079028 
Epoch [2/10] Batch 1200/7168 Train_loss 2.1564340553588415 
Epoch [2/10] Batch 1300/7168 Train_loss 2.160724050363241 
Epoch [2/10] Batch 1400/7168 Train_loss 2.1691469863578305 
Epoch [2/10] Batch 1500/7168 Train_loss 2.1715670682584185 
Epoch [2/10] Batch 1600/7168 Train_loss 2.1719963561457294 
Epoch [2/10] Batch 1700/7168 Train_loss 2.168675921704053 
Epoch [2/10] Batch 1800/7168 Train_loss 2.171621032675593 
Epoch [2/10] Batch 1900/7168 Train_loss 2.169213603980161 
Epoch [2/10] Batch 2000/7168 Train_loss 2.169963018573087 
Epoch [2/10] Batch 2100/7168 Train_loss 2.1663792811153275 
Epoch [2/10] Batch 2200/7168 Train_loss 2.1639736174778847 
Epoch [2/10] Batch 2300/7168 Train_loss 2.1661556192853046 
Epoch [2/10] Batch 2400/7168 Train_loss 2.1647703861418193 
Epoch [2/10] Batch 2500/7168 Train_loss 2.1610673259599644 
Epoch [2/10] Batch 2600/7168 Train_loss 2.162556954441461 
Epoch [2/10] Batch 2700/7168 Train_loss 2.161917675897529 
Epoch [2/10] Batch 2800/7168 Train_loss 2.1620990747172932 
Epoch [2/10] Batch 2900/7168 Train_loss 2.158251453302762 
Epoch [2/10] Batch 3000/7168 Train_loss 2.1587844032351073 
Epoch [2/10] Batch 3100/7168 Train_loss 2.1566989535921737 
Epoch [2/10] Batch 3200/7168 Train_loss 2.1575570071127146 
Epoch [2/10] Batch 3300/7168 Train_loss 2.160759321810332 
Epoch [2/10] Batch 3400/7168 Train_loss 2.1615952916248373 
Epoch [2/10] Batch 3500/7168 Train_loss 2.161558899976328 
Epoch [2/10] Batch 3600/7168 Train_loss 2.161666804723394 
Epoch [2/10] Batch 3700/7168 Train_loss 2.161638124392214 
Epoch [2/10] Batch 3800/7168 Train_loss 2.161621701867133 
Epoch [2/10] Batch 3900/7168 Train_loss 2.1624561683607846 
Epoch [2/10] Batch 4000/7168 Train_loss 2.165138665824138 
Epoch [2/10] Batch 4100/7168 Train_loss 2.1647562432677185 
Epoch [2/10] Batch 4200/7168 Train_loss 2.166066243641577 
Epoch [2/10] Batch 4300/7168 Train_loss 2.164350980483524 
Epoch [2/10] Batch 4400/7168 Train_loss 2.1626812151056125 
Epoch [2/10] Batch 4500/7168 Train_loss 2.1623064403440178 
Epoch [2/10] Batch 4600/7168 Train_loss 2.1624463756617915 
Epoch [2/10] Batch 4700/7168 Train_loss 2.164757701532051 
Epoch [2/10] Batch 4800/7168 Train_loss 2.1644537315514407 
Epoch [2/10] Batch 4900/7168 Train_loss 2.1640134630149004 
Epoch [2/10] Batch 5000/7168 Train_loss 2.1636013105925835 
Epoch [2/10] Batch 5100/7168 Train_loss 2.160983883345438 
Epoch [2/10] Batch 5200/7168 Train_loss 2.159357292692585 
Epoch [2/10] Batch 5300/7168 Train_loss 2.1594253951503384 
Epoch [2/10] Batch 5400/7168 Train_loss 2.1611151049681623 
Epoch [2/10] Batch 5500/7168 Train_loss 2.1620311875047302 
Epoch [2/10] Batch 5600/7168 Train_loss 2.162855972143991 
Epoch [2/10] Batch 5700/7168 Train_loss 2.163897978775419 
Epoch [2/10] Batch 5800/7168 Train_loss 2.1629823463920927 
Epoch [2/10] Batch 5900/7168 Train_loss 2.1635912967711017 
Epoch [2/10] Batch 6000/7168 Train_loss 2.16351054096059 
Epoch [2/10] Batch 6100/7168 Train_loss 2.1650034297545258 
Epoch [2/10] Batch 6200/7168 Train_loss 2.166780214387262 
Epoch [2/10] Batch 6300/7168 Train_loss 2.168561569876925 
Epoch [2/10] Batch 6400/7168 Train_loss 2.170386364151865 
Epoch [2/10] Batch 6500/7168 Train_loss 2.1715613334701716 
Epoch [2/10] Batch 6600/7168 Train_loss 2.1704195117796035 
Epoch [2/10] Batch 6700/7168 Train_loss 2.1712273460366873 
Epoch [2/10] Batch 6800/7168 Train_loss 2.172032074323298 
Epoch [2/10] Batch 6900/7168 Train_loss 2.172554205002396 
Epoch [2/10] Batch 7000/7168 Train_loss 2.1725743421996055 
Epoch [2/10] Batch 7100/7168 Train_loss 2.1737576415821325 
Epoch: 2/10 	Training Loss: 2.174006 	Validation Loss: 2.193317 Duration seconds: 955.3101708889008 
best_valid_loss_fold [2.1918655882306797] Best_Epoch [2]Epoch [3/10] Batch 0/7168 Train_loss 2.3109716176986694 
Epoch [3/10] Batch 100/7168 Train_loss 2.035096923608591 
Epoch [3/10] Batch 200/7168 Train_loss 2.1033895749654343 
Epoch [3/10] Batch 300/7168 Train_loss 2.1097621613760724 
Epoch [3/10] Batch 400/7168 Train_loss 2.138007278194154 
Epoch [3/10] Batch 500/7168 Train_loss 2.141512055656391 
Epoch [3/10] Batch 600/7168 Train_loss 2.149213583591377 
Epoch [3/10] Batch 700/7168 Train_loss 2.1497351618831746 
Epoch [3/10] Batch 800/7168 Train_loss 2.1510157988089302 
Epoch [3/10] Batch 900/7168 Train_loss 2.1623418227119267 
Epoch [3/10] Batch 1000/7168 Train_loss 2.1713047814029793 
Epoch [3/10] Batch 1100/7168 Train_loss 2.1778664737263123 
Epoch [3/10] Batch 1200/7168 Train_loss 2.1774954271256974 
Epoch [3/10] Batch 1300/7168 Train_loss 2.179070262268449 
Epoch [3/10] Batch 1400/7168 Train_loss 2.177730954433662 
Epoch [3/10] Batch 1500/7168 Train_loss 2.1765937143409357 
Epoch [3/10] Batch 1600/7168 Train_loss 2.1757218356172716 
Epoch [3/10] Batch 1700/7168 Train_loss 2.175532140928391 
Epoch [3/10] Batch 1800/7168 Train_loss 2.1741148371090167 
Epoch [3/10] Batch 1900/7168 Train_loss 2.1723204779819336 
Epoch [3/10] Batch 2000/7168 Train_loss 2.1719461426265476 
Epoch [3/10] Batch 2100/7168 Train_loss 2.171012476150448 
Epoch [3/10] Batch 2200/7168 Train_loss 2.1741644388529346 
Epoch [3/10] Batch 2300/7168 Train_loss 2.173399306875838 
Epoch [3/10] Batch 2400/7168 Train_loss 2.172173331431526 
Epoch [3/10] Batch 2500/7168 Train_loss 2.174252731675675 
Epoch [3/10] Batch 2600/7168 Train_loss 2.1751432331341225 
Epoch [3/10] Batch 2700/7168 Train_loss 2.1781461398465156 
Epoch [3/10] Batch 2800/7168 Train_loss 2.1806323821152676 
Epoch [3/10] Batch 2900/7168 Train_loss 2.179949914549228 
Epoch [3/10] Batch 3000/7168 Train_loss 2.1773852328734176 
Epoch [3/10] Batch 3100/7168 Train_loss 2.1767817572917294 
Epoch [3/10] Batch 3200/7168 Train_loss 2.176362497616246 
Epoch [3/10] Batch 3300/7168 Train_loss 2.176744052291061 
Epoch [3/10] Batch 3400/7168 Train_loss 2.175578112590492 
Epoch [3/10] Batch 3500/7168 Train_loss 2.1751447965999358 
Epoch [3/10] Batch 3600/7168 Train_loss 2.1765733595086085 
Epoch [3/10] Batch 3700/7168 Train_loss 2.1785654370582157 
Epoch [3/10] Batch 3800/7168 Train_loss 2.1787302703316103 
Epoch [3/10] Batch 3900/7168 Train_loss 2.1798576274970287 
Epoch [3/10] Batch 4000/7168 Train_loss 2.18144787773881 
Epoch [3/10] Batch 4100/7168 Train_loss 2.1811868339391838 
Epoch [3/10] Batch 4200/7168 Train_loss 2.1787679119674515 
Epoch [3/10] Batch 4300/7168 Train_loss 2.1782915991243996 
Epoch [3/10] Batch 4400/7168 Train_loss 2.176912805944566 
Epoch [3/10] Batch 4500/7168 Train_loss 2.1737156747460658 
Epoch [3/10] Batch 4600/7168 Train_loss 2.1734012910961407 
Epoch [3/10] Batch 4700/7168 Train_loss 2.1732235566919544 
Epoch [3/10] Batch 4800/7168 Train_loss 2.174167666096187 
Epoch [3/10] Batch 4900/7168 Train_loss 2.1750344596902003 
Epoch [3/10] Batch 5000/7168 Train_loss 2.1730888726394957 
Epoch [3/10] Batch 5100/7168 Train_loss 2.173911504618352 
Epoch [3/10] Batch 5200/7168 Train_loss 2.1746527893231296 
Epoch [3/10] Batch 5300/7168 Train_loss 2.1739647796223287 
Epoch [3/10] Batch 5400/7168 Train_loss 2.1731118009617276 
Epoch [3/10] Batch 5500/7168 Train_loss 2.1740165237561504 
Epoch [3/10] Batch 5600/7168 Train_loss 2.1739821022396706 
Epoch [3/10] Batch 5700/7168 Train_loss 2.173938915732032 
Epoch [3/10] Batch 5800/7168 Train_loss 2.1735373935171745 
Epoch [3/10] Batch 5900/7168 Train_loss 2.1736266167789893 
Epoch [3/10] Batch 6000/7168 Train_loss 2.1740361775627455 
Epoch [3/10] Batch 6100/7168 Train_loss 2.1749283874905356 
Epoch [3/10] Batch 6200/7168 Train_loss 2.1763225350215962 
Epoch [3/10] Batch 6300/7168 Train_loss 2.1783074941985703 
Epoch [3/10] Batch 6400/7168 Train_loss 2.177799534639834 
Epoch [3/10] Batch 6500/7168 Train_loss 2.1777160419416215 
Epoch [3/10] Batch 6600/7168 Train_loss 2.176710396165325 
Epoch [3/10] Batch 6700/7168 Train_loss 2.1761029101702514 
Epoch [3/10] Batch 6800/7168 Train_loss 2.1752797075162618 
Epoch [3/10] Batch 6900/7168 Train_loss 2.175146703132079 
Epoch [3/10] Batch 7000/7168 Train_loss 2.175114340968446 
Epoch [3/10] Batch 7100/7168 Train_loss 2.174813562405813 
Epoch: 3/10 	Training Loss: 2.174476 	Validation Loss: 2.192442 Duration seconds: 954.428792476654 
best_valid_loss_fold [2.1918655882306797] Best_Epoch [3]Epoch [4/10] Batch 0/7168 Train_loss 2.8159883618354797 
Epoch [4/10] Batch 100/7168 Train_loss 2.115474406740453 
Epoch [4/10] Batch 200/7168 Train_loss 2.1209585807513243 
Epoch [4/10] Batch 300/7168 Train_loss 2.1051573533057373 
Epoch [4/10] Batch 400/7168 Train_loss 2.1024015562567033 
Epoch [4/10] Batch 500/7168 Train_loss 2.1095236521697567 
Epoch [4/10] Batch 600/7168 Train_loss 2.1252372915455187 
Epoch [4/10] Batch 700/7168 Train_loss 2.1322232449743446 
Epoch [4/10] Batch 800/7168 Train_loss 2.1395552238349462 
Epoch [4/10] Batch 900/7168 Train_loss 2.1400396317475114 
Epoch [4/10] Batch 1000/7168 Train_loss 2.152013225289253 
Epoch [4/10] Batch 1100/7168 Train_loss 2.151403300376181 
Epoch [4/10] Batch 1200/7168 Train_loss 2.149705939323678 
Epoch [4/10] Batch 1300/7168 Train_loss 2.1534972494721503 
Epoch [4/10] Batch 1400/7168 Train_loss 2.1589653937945106 
Epoch [4/10] Batch 1500/7168 Train_loss 2.158096485351182 
Epoch [4/10] Batch 1600/7168 Train_loss 2.1615287841650788 
Epoch [4/10] Batch 1700/7168 Train_loss 2.15290228692242 
Epoch [4/10] Batch 1800/7168 Train_loss 2.1560495897821426 
Epoch [4/10] Batch 1900/7168 Train_loss 2.1564845963803045 
Epoch [4/10] Batch 2000/7168 Train_loss 2.1569655101740737 
Epoch [4/10] Batch 2100/7168 Train_loss 2.161701073863857 
Epoch [4/10] Batch 2200/7168 Train_loss 2.1635030859278204 
Epoch [4/10] Batch 2300/7168 Train_loss 2.1641892545420625 
Epoch [4/10] Batch 2400/7168 Train_loss 2.166322894620925 
Epoch [4/10] Batch 2500/7168 Train_loss 2.1669271257354086 
Epoch [4/10] Batch 2600/7168 Train_loss 2.1627331644250813 
Epoch [4/10] Batch 2700/7168 Train_loss 2.1614799962986933 
Epoch [4/10] Batch 2800/7168 Train_loss 2.163493251058937 
Epoch [4/10] Batch 2900/7168 Train_loss 2.1645046430275547 
Epoch [4/10] Batch 3000/7168 Train_loss 2.1690850583504377 
Epoch [4/10] Batch 3100/7168 Train_loss 2.1717336690500835 
Epoch [4/10] Batch 3200/7168 Train_loss 2.172193034799052 
Epoch [4/10] Batch 3300/7168 Train_loss 2.1696841686747357 
Epoch [4/10] Batch 3400/7168 Train_loss 2.168213390964019 
Epoch [4/10] Batch 3500/7168 Train_loss 2.1682557821171655 
Epoch [4/10] Batch 3600/7168 Train_loss 2.166677699614419 
Epoch [4/10] Batch 3700/7168 Train_loss 2.1647784017656533 
Epoch [4/10] Batch 3800/7168 Train_loss 2.1658953035216495 
Epoch [4/10] Batch 3900/7168 Train_loss 2.1682137812606435 
Epoch [4/10] Batch 4000/7168 Train_loss 2.168348235760412 
Epoch [4/10] Batch 4100/7168 Train_loss 2.168945413054678 
Epoch [4/10] Batch 4200/7168 Train_loss 2.1685700253507005 
Epoch [4/10] Batch 4300/7168 Train_loss 2.1696384276582372 
Epoch [4/10] Batch 4400/7168 Train_loss 2.170666267122363 
Epoch [4/10] Batch 4500/7168 Train_loss 2.1719472481227933 
Epoch [4/10] Batch 4600/7168 Train_loss 2.1726260154020833 
Epoch [4/10] Batch 4700/7168 Train_loss 2.173754434554761 
Epoch [4/10] Batch 4800/7168 Train_loss 2.174000945815169 
Epoch [4/10] Batch 4900/7168 Train_loss 2.1748317641899115 
Epoch [4/10] Batch 5000/7168 Train_loss 2.1743908683178876 
Epoch [4/10] Batch 5100/7168 Train_loss 2.173961124579548 
Epoch [4/10] Batch 5200/7168 Train_loss 2.1735787334647965 
Epoch [4/10] Batch 5300/7168 Train_loss 2.175473670150689 
Epoch [4/10] Batch 5400/7168 Train_loss 2.1755580233961447 
Epoch [4/10] Batch 5500/7168 Train_loss 2.175895032723304 
Epoch [4/10] Batch 5600/7168 Train_loss 2.1753092629077115 
Epoch [4/10] Batch 5700/7168 Train_loss 2.1744323832108208 
Epoch [4/10] Batch 5800/7168 Train_loss 2.1746356337569 
Epoch [4/10] Batch 5900/7168 Train_loss 2.1746797451092577 
Epoch [4/10] Batch 6000/7168 Train_loss 2.17472846272925 
Epoch [4/10] Batch 6100/7168 Train_loss 2.173937968622757 
Epoch [4/10] Batch 6200/7168 Train_loss 2.174339886575125 
Epoch [4/10] Batch 6300/7168 Train_loss 2.172907669392205 
Epoch [4/10] Batch 6400/7168 Train_loss 2.173010237413707 
Epoch [4/10] Batch 6500/7168 Train_loss 2.174057703596448 
Epoch [4/10] Batch 6600/7168 Train_loss 2.1750607335884413 
Epoch [4/10] Batch 6700/7168 Train_loss 2.175278939977188 
Epoch [4/10] Batch 6800/7168 Train_loss 2.1758569594389936 
Epoch [4/10] Batch 6900/7168 Train_loss 2.1745054877364898 
Epoch [4/10] Batch 7000/7168 Train_loss 2.174184733389565 
Epoch [4/10] Batch 7100/7168 Train_loss 2.174838806359957 
Epoch: 4/10 	Training Loss: 2.174366 	Validation Loss: 2.191264 Duration seconds: 958.7674076557159 
Validation loss decreased (2.191866 --> 2.191264).  Saving model ... 
best_valid_loss_fold [2.191264061781112] Best_Epoch [4]Epoch [5/10] Batch 0/7168 Train_loss 1.8826721608638763 
Epoch [5/10] Batch 100/7168 Train_loss 2.1750925311652742 
Epoch [5/10] Batch 200/7168 Train_loss 2.144327450969919 
Epoch [5/10] Batch 300/7168 Train_loss 2.153791079043946 
Epoch [5/10] Batch 400/7168 Train_loss 2.1683367128086806 
Epoch [5/10] Batch 500/7168 Train_loss 2.172224887176426 
Epoch [5/10] Batch 600/7168 Train_loss 2.1854436267384276 
Epoch [5/10] Batch 700/7168 Train_loss 2.17601414073721 
Epoch [5/10] Batch 800/7168 Train_loss 2.170258624183849 
Epoch [5/10] Batch 900/7168 Train_loss 2.1685297886420303 
Epoch [5/10] Batch 1000/7168 Train_loss 2.171799503527321 
Epoch [5/10] Batch 1100/7168 Train_loss 2.174212702505162 
Epoch [5/10] Batch 1200/7168 Train_loss 2.1790217061051718 
Epoch [5/10] Batch 1300/7168 Train_loss 2.1749193844980317 
Epoch [5/10] Batch 1400/7168 Train_loss 2.174935131882192 
Epoch [5/10] Batch 1500/7168 Train_loss 2.1757841949598697 
Epoch [5/10] Batch 1600/7168 Train_loss 2.177325225040214 
Epoch [5/10] Batch 1700/7168 Train_loss 2.1696597876915296 
Epoch [5/10] Batch 1800/7168 Train_loss 2.1690998187517208 
Epoch [5/10] Batch 1900/7168 Train_loss 2.1687867017810687 
Epoch [5/10] Batch 2000/7168 Train_loss 2.16789319627229 
Epoch [5/10] Batch 2100/7168 Train_loss 2.172897506233966 
Epoch [5/10] Batch 2200/7168 Train_loss 2.169893506113804 
Epoch [5/10] Batch 2300/7168 Train_loss 2.1725300511669876 
Epoch [5/10] Batch 2400/7168 Train_loss 2.1719788837835026 
Epoch [5/10] Batch 2500/7168 Train_loss 2.1724834278231855 
Epoch [5/10] Batch 2600/7168 Train_loss 2.1742723979877994 
Epoch [5/10] Batch 2700/7168 Train_loss 2.1733630891863127 
Epoch [5/10] Batch 2800/7168 Train_loss 2.1746734081528434 
Epoch [5/10] Batch 2900/7168 Train_loss 2.1740107278614693 
Epoch [5/10] Batch 3000/7168 Train_loss 2.1750443296426933 
Epoch [5/10] Batch 3100/7168 Train_loss 2.1784525795430376 
Epoch [5/10] Batch 3200/7168 Train_loss 2.1793541862457406 
Epoch [5/10] Batch 3300/7168 Train_loss 2.1767187491849276 
Epoch [5/10] Batch 3400/7168 Train_loss 2.1786683119813754 
Epoch [5/10] Batch 3500/7168 Train_loss 2.177840218211848 
Epoch [5/10] Batch 3600/7168 Train_loss 2.1783145943951916 
Epoch [5/10] Batch 3700/7168 Train_loss 2.177834415602478 
Epoch [5/10] Batch 3800/7168 Train_loss 2.1784926478336186 
Epoch [5/10] Batch 3900/7168 Train_loss 2.178468141065931 
Epoch [5/10] Batch 4000/7168 Train_loss 2.176570798033209 
Epoch [5/10] Batch 4100/7168 Train_loss 2.1765109989315206 
Epoch [5/10] Batch 4200/7168 Train_loss 2.174944735121738 
Epoch [5/10] Batch 4300/7168 Train_loss 2.175171968116369 
Epoch [5/10] Batch 4400/7168 Train_loss 2.1766948594143107 
Epoch [5/10] Batch 4500/7168 Train_loss 2.1759032010634827 
Epoch [5/10] Batch 4600/7168 Train_loss 2.1768299951239323 
Epoch [5/10] Batch 4700/7168 Train_loss 2.177658892614845 
Epoch [5/10] Batch 4800/7168 Train_loss 2.177291130742116 
Epoch [5/10] Batch 4900/7168 Train_loss 2.1763619546275117 
Epoch [5/10] Batch 5000/7168 Train_loss 2.1780952229020216 
Epoch [5/10] Batch 5100/7168 Train_loss 2.177057353243994 
Epoch [5/10] Batch 5200/7168 Train_loss 2.176480664570981 
Epoch [5/10] Batch 5300/7168 Train_loss 2.178155800091885 
Epoch [5/10] Batch 5400/7168 Train_loss 2.17659513265518 
Epoch [5/10] Batch 5500/7168 Train_loss 2.17605806663673 
Epoch [5/10] Batch 5600/7168 Train_loss 2.1758118264910538 
Epoch [5/10] Batch 5700/7168 Train_loss 2.1750423667922685 
Epoch [5/10] Batch 5800/7168 Train_loss 2.1742384183501358 
Epoch [5/10] Batch 5900/7168 Train_loss 2.1732827988474517 
Epoch [5/10] Batch 6000/7168 Train_loss 2.1738899044644096 
Epoch [5/10] Batch 6100/7168 Train_loss 2.175267571856635 
Epoch [5/10] Batch 6200/7168 Train_loss 2.175301609140618 
Epoch [5/10] Batch 6300/7168 Train_loss 2.175444954480941 
Epoch [5/10] Batch 6400/7168 Train_loss 2.1753931622396427 
Epoch [5/10] Batch 6500/7168 Train_loss 2.1735477134404304 
Epoch [5/10] Batch 6600/7168 Train_loss 2.172918377548271 
Epoch [5/10] Batch 6700/7168 Train_loss 2.173870875786586 
Epoch [5/10] Batch 6800/7168 Train_loss 2.1732688846373063 
Epoch [5/10] Batch 6900/7168 Train_loss 2.173930757765821 
Epoch [5/10] Batch 7000/7168 Train_loss 2.174218654081304 
Epoch [5/10] Batch 7100/7168 Train_loss 2.175071242692571 
Epoch: 5/10 	Training Loss: 2.174308 	Validation Loss: 2.191950 Duration seconds: 949.5328989028931 
best_valid_loss_fold [2.191264061781112] Best_Epoch [5]Epoch [6/10] Batch 0/7168 Train_loss 2.6127883791923523 
Epoch [6/10] Batch 100/7168 Train_loss 2.173433119412696 
Epoch [6/10] Batch 200/7168 Train_loss 2.1836993561603535 
Epoch [6/10] Batch 300/7168 Train_loss 2.1726105429009346 
Epoch [6/10] Batch 400/7168 Train_loss 2.169461277060378 
Epoch [6/10] Batch 500/7168 Train_loss 2.170669134267552 
Epoch [6/10] Batch 600/7168 Train_loss 2.1715570527582915 
Epoch [6/10] Batch 700/7168 Train_loss 2.1653624989634404 
Epoch [6/10] Batch 800/7168 Train_loss 2.1697642276535616 
Epoch [6/10] Batch 900/7168 Train_loss 2.1676649240059542 
Epoch [6/10] Batch 1000/7168 Train_loss 2.162642289738317 
Epoch [6/10] Batch 1100/7168 Train_loss 2.165575473959288 
Epoch [6/10] Batch 1200/7168 Train_loss 2.1709551191349807 
Epoch [6/10] Batch 1300/7168 Train_loss 2.1607168873258042 
Epoch [6/10] Batch 1400/7168 Train_loss 2.1674241019222755 
Epoch [6/10] Batch 1500/7168 Train_loss 2.171046583901478 
Epoch [6/10] Batch 1600/7168 Train_loss 2.170392919068855 
Epoch [6/10] Batch 1700/7168 Train_loss 2.172180541179448 
Epoch [6/10] Batch 1800/7168 Train_loss 2.172307726461182 
Epoch [6/10] Batch 1900/7168 Train_loss 2.1682206272044224 
Epoch [6/10] Batch 2000/7168 Train_loss 2.1732701235327467 
Epoch [6/10] Batch 2100/7168 Train_loss 2.172202763459275 
Epoch [6/10] Batch 2200/7168 Train_loss 2.1729541470088076 
Epoch [6/10] Batch 2300/7168 Train_loss 2.1722827744919133 
Epoch [6/10] Batch 2400/7168 Train_loss 2.173246978993517 
Epoch [6/10] Batch 2500/7168 Train_loss 2.1760900843231643 
Epoch [6/10] Batch 2600/7168 Train_loss 2.1743603125758835 
Epoch [6/10] Batch 2700/7168 Train_loss 2.1719860326897344 
Epoch [6/10] Batch 2800/7168 Train_loss 2.171641125508991 
Epoch [6/10] Batch 2900/7168 Train_loss 2.170567773963122 
Epoch [6/10] Batch 3000/7168 Train_loss 2.169848796761024 
Epoch [6/10] Batch 3100/7168 Train_loss 2.1671144266891234 
Epoch [6/10] Batch 3200/7168 Train_loss 2.167816365586337 
Epoch [6/10] Batch 3300/7168 Train_loss 2.168939584283278 
Epoch [6/10] Batch 3400/7168 Train_loss 2.169297349120862 
Epoch [6/10] Batch 3500/7168 Train_loss 2.1677116266329675 
Epoch [6/10] Batch 3600/7168 Train_loss 2.168323932915421 
Epoch [6/10] Batch 3700/7168 Train_loss 2.1683552833835167 
Epoch [6/10] Batch 3800/7168 Train_loss 2.1671759636503745 
Epoch [6/10] Batch 3900/7168 Train_loss 2.1644925808553603 
Epoch [6/10] Batch 4000/7168 Train_loss 2.164622235785303 
Epoch [6/10] Batch 4100/7168 Train_loss 2.168129871793998 
Epoch [6/10] Batch 4200/7168 Train_loss 2.1675173641208865 
Epoch [6/10] Batch 4300/7168 Train_loss 2.1687854004825056 
Epoch [6/10] Batch 4400/7168 Train_loss 2.169682465085683 
Epoch [6/10] Batch 4500/7168 Train_loss 2.1712686738896916 
Epoch [6/10] Batch 4600/7168 Train_loss 2.1734384535266327 
Epoch [6/10] Batch 4700/7168 Train_loss 2.174605301271675 
Epoch [6/10] Batch 4800/7168 Train_loss 2.174736051367263 
Epoch [6/10] Batch 4900/7168 Train_loss 2.1743498534918855 
Epoch [6/10] Batch 5000/7168 Train_loss 2.1741200958346396 
Epoch [6/10] Batch 5100/7168 Train_loss 2.173923925383258 
Epoch [6/10] Batch 5200/7168 Train_loss 2.1730442622193373 
Epoch [6/10] Batch 5300/7168 Train_loss 2.1733766465738253 
Epoch [6/10] Batch 5400/7168 Train_loss 2.1742281488904553 
Epoch [6/10] Batch 5500/7168 Train_loss 2.174087510967532 
Epoch [6/10] Batch 5600/7168 Train_loss 2.1754973789205128 
Epoch [6/10] Batch 5700/7168 Train_loss 2.175258515153385 
Epoch [6/10] Batch 5800/7168 Train_loss 2.1756441285103234 
Epoch [6/10] Batch 5900/7168 Train_loss 2.1768292472955837 
Epoch [6/10] Batch 6000/7168 Train_loss 2.1765415464698465 
Epoch [6/10] Batch 6100/7168 Train_loss 2.1753450367923954 
Epoch [6/10] Batch 6200/7168 Train_loss 2.1748567496196163 
Epoch [6/10] Batch 6300/7168 Train_loss 2.175188206078287 
Epoch [6/10] Batch 6400/7168 Train_loss 2.1745953727045055 
Epoch [6/10] Batch 6500/7168 Train_loss 2.175853759992436 
Epoch [6/10] Batch 6600/7168 Train_loss 2.1772044933875243 
Epoch [6/10] Batch 6700/7168 Train_loss 2.1761564745300475 
Epoch [6/10] Batch 6800/7168 Train_loss 2.175760594838999 
Epoch [6/10] Batch 6900/7168 Train_loss 2.176224354710152 
Epoch [6/10] Batch 7000/7168 Train_loss 2.175559955160203 
Epoch [6/10] Batch 7100/7168 Train_loss 2.1744104816097387 
Epoch: 6/10 	Training Loss: 2.174336 	Validation Loss: 2.192035 Duration seconds: 950.2097945213318 
best_valid_loss_fold [2.191264061781112] Best_Epoch [6]Epoch [7/10] Batch 0/7168 Train_loss 2.544791102409363 
Epoch [7/10] Batch 100/7168 Train_loss 2.156034865886858 
Epoch [7/10] Batch 200/7168 Train_loss 2.22420925895373 
Epoch [7/10] Batch 300/7168 Train_loss 2.2166075092118445 
Epoch [7/10] Batch 400/7168 Train_loss 2.2018939828932136 
Epoch [7/10] Batch 500/7168 Train_loss 2.1819075256467104 
Epoch [7/10] Batch 600/7168 Train_loss 2.1701328302679364 
Epoch [7/10] Batch 700/7168 Train_loss 2.176756249174752 
Epoch [7/10] Batch 800/7168 Train_loss 2.1805879970391593 
Epoch [7/10] Batch 900/7168 Train_loss 2.1784150459029434 
Epoch [7/10] Batch 1000/7168 Train_loss 2.1744711234882756 
Epoch [7/10] Batch 1100/7168 Train_loss 2.1707251081268533 
Epoch [7/10] Batch 1200/7168 Train_loss 2.16425881683082 
Epoch [7/10] Batch 1300/7168 Train_loss 2.166146580344617 
Epoch [7/10] Batch 1400/7168 Train_loss 2.1709212533688222 
Epoch [7/10] Batch 1500/7168 Train_loss 2.1735311898686422 
Epoch [7/10] Batch 1600/7168 Train_loss 2.1722477031342318 
Epoch [7/10] Batch 1700/7168 Train_loss 2.1709803794496976 
Epoch [7/10] Batch 1800/7168 Train_loss 2.165990006602651 
Epoch [7/10] Batch 1900/7168 Train_loss 2.1670902493766957 
Epoch [7/10] Batch 2000/7168 Train_loss 2.1675579511988228 
Epoch [7/10] Batch 2100/7168 Train_loss 2.1658334395946177 
Epoch [7/10] Batch 2200/7168 Train_loss 2.162118980467238 
Epoch [7/10] Batch 2300/7168 Train_loss 2.1645372470712103 
Epoch [7/10] Batch 2400/7168 Train_loss 2.165460884509013 
Epoch [7/10] Batch 2500/7168 Train_loss 2.168006442668961 
Epoch [7/10] Batch 2600/7168 Train_loss 2.165106888180144 
Epoch [7/10] Batch 2700/7168 Train_loss 2.1683035414115626 
Epoch [7/10] Batch 2800/7168 Train_loss 2.1681910019363944 
Epoch [7/10] Batch 2900/7168 Train_loss 2.167740704370383 
Epoch [7/10] Batch 3000/7168 Train_loss 2.170941065754822 
Epoch [7/10] Batch 3100/7168 Train_loss 2.1692103650859385 
Epoch [7/10] Batch 3200/7168 Train_loss 2.1698040335728055 
Epoch [7/10] Batch 3300/7168 Train_loss 2.1725886976428193 
Epoch [7/10] Batch 3400/7168 Train_loss 2.1738247875473635 
Epoch [7/10] Batch 3500/7168 Train_loss 2.174688027764654 
Epoch [7/10] Batch 3600/7168 Train_loss 2.1770937337548624 
Epoch [7/10] Batch 3700/7168 Train_loss 2.1793347408492254 
Epoch [7/10] Batch 3800/7168 Train_loss 2.179621849792219 
Epoch [7/10] Batch 3900/7168 Train_loss 2.1796202015201422 
Epoch [7/10] Batch 4000/7168 Train_loss 2.179984525386109 
Epoch [7/10] Batch 4100/7168 Train_loss 2.179102992207008 
Epoch [7/10] Batch 4200/7168 Train_loss 2.1790312845823 
Epoch [7/10] Batch 4300/7168 Train_loss 2.179061251774108 
Epoch [7/10] Batch 4400/7168 Train_loss 2.179494662224312 
Epoch [7/10] Batch 4500/7168 Train_loss 2.1789918298477384 
Epoch [7/10] Batch 4600/7168 Train_loss 2.1783369722262176 
Epoch [7/10] Batch 4700/7168 Train_loss 2.177031416667063 
Epoch [7/10] Batch 4800/7168 Train_loss 2.1787939570926422 
Epoch [7/10] Batch 4900/7168 Train_loss 2.1766367332865038 
Epoch [7/10] Batch 5000/7168 Train_loss 2.174878449612583 
Epoch [7/10] Batch 5100/7168 Train_loss 2.1755478667689405 
Epoch [7/10] Batch 5200/7168 Train_loss 2.175581921561707 
Epoch [7/10] Batch 5300/7168 Train_loss 2.1744470260482176 
Epoch [7/10] Batch 5400/7168 Train_loss 2.173696156798789 
Epoch [7/10] Batch 5500/7168 Train_loss 2.173965948318443 
Epoch [7/10] Batch 5600/7168 Train_loss 2.173776912491804 
Epoch [7/10] Batch 5700/7168 Train_loss 2.1744756567082977 
Epoch [7/10] Batch 5800/7168 Train_loss 2.1756408991761873 
Epoch [7/10] Batch 5900/7168 Train_loss 2.174780155729185 
Epoch [7/10] Batch 6000/7168 Train_loss 2.1754132991402453 
Epoch [7/10] Batch 6100/7168 Train_loss 2.1759492288093414 
Epoch [7/10] Batch 6200/7168 Train_loss 2.1762557167585697 
Epoch [7/10] Batch 6300/7168 Train_loss 2.1757700010824346 
Epoch [7/10] Batch 6400/7168 Train_loss 2.1772782316926382 
Epoch [7/10] Batch 6500/7168 Train_loss 2.176626463923669 
Epoch [7/10] Batch 6600/7168 Train_loss 2.1763273778783065 
Epoch [7/10] Batch 6700/7168 Train_loss 2.1753492697444075 
Epoch [7/10] Batch 6800/7168 Train_loss 2.1749960989912474 
Epoch [7/10] Batch 6900/7168 Train_loss 2.174687953005924 
Epoch [7/10] Batch 7000/7168 Train_loss 2.175247847699741 
Epoch [7/10] Batch 7100/7168 Train_loss 2.175091146854108 
Epoch: 7/10 	Training Loss: 2.174298 	Validation Loss: 2.192007 Duration seconds: 951.3803060054779 
best_valid_loss_fold [2.191264061781112] Best_Epoch [7]Epoch [8/10] Batch 0/7168 Train_loss 2.1104681491851807 
Epoch [8/10] Batch 100/7168 Train_loss 2.207693453177367 
Epoch [8/10] Batch 200/7168 Train_loss 2.2187601867599867 
Epoch [8/10] Batch 300/7168 Train_loss 2.216220289171732 
Epoch [8/10] Batch 400/7168 Train_loss 2.2186669493628264 
Epoch [8/10] Batch 500/7168 Train_loss 2.208020686954557 
Epoch [8/10] Batch 600/7168 Train_loss 2.2147001753779696 
Epoch [8/10] Batch 700/7168 Train_loss 2.2030365166284898 
Epoch [8/10] Batch 800/7168 Train_loss 2.19630529399445 
Epoch [8/10] Batch 900/7168 Train_loss 2.195995732083437 
Epoch [8/10] Batch 1000/7168 Train_loss 2.193544698687462 
Epoch [8/10] Batch 1100/7168 Train_loss 2.1842514680229894 
Epoch [8/10] Batch 1200/7168 Train_loss 2.1886163698098344 
Epoch [8/10] Batch 1300/7168 Train_loss 2.193127324221961 
Epoch [8/10] Batch 1400/7168 Train_loss 2.1893709549131266 
Epoch [8/10] Batch 1500/7168 Train_loss 2.1889079648065692 
Epoch [8/10] Batch 1600/7168 Train_loss 2.189276163686744 
Epoch [8/10] Batch 1700/7168 Train_loss 2.185178595140077 
Epoch [8/10] Batch 1800/7168 Train_loss 2.184392779047856 
Epoch [8/10] Batch 1900/7168 Train_loss 2.179651940452118 
Epoch [8/10] Batch 2000/7168 Train_loss 2.179454970008311 
Epoch [8/10] Batch 2100/7168 Train_loss 2.1852474019590984 
Epoch [8/10] Batch 2200/7168 Train_loss 2.1846767305745582 
Epoch [8/10] Batch 2300/7168 Train_loss 2.1879093421186795 
Epoch [8/10] Batch 2400/7168 Train_loss 2.186480337961025 
Epoch [8/10] Batch 2500/7168 Train_loss 2.1867206426965193 
Epoch [8/10] Batch 2600/7168 Train_loss 2.186396262282273 
Epoch [8/10] Batch 2700/7168 Train_loss 2.1883150883865814 
Epoch [8/10] Batch 2800/7168 Train_loss 2.1900639655344487 
Epoch [8/10] Batch 2900/7168 Train_loss 2.190109362600385 
Epoch [8/10] Batch 3000/7168 Train_loss 2.18693558914945 
Epoch [8/10] Batch 3100/7168 Train_loss 2.186000142000214 
Epoch [8/10] Batch 3200/7168 Train_loss 2.185373125910312 
Epoch [8/10] Batch 3300/7168 Train_loss 2.1836455794256118 
Epoch [8/10] Batch 3400/7168 Train_loss 2.1823966197978395 
Epoch [8/10] Batch 3500/7168 Train_loss 2.1830851610210478 
Epoch [8/10] Batch 3600/7168 Train_loss 2.1841887638730095 
Epoch [8/10] Batch 3700/7168 Train_loss 2.1826779939208087 
Epoch [8/10] Batch 3800/7168 Train_loss 2.181872924482531 
Epoch [8/10] Batch 3900/7168 Train_loss 2.1814336286992853 
Epoch [8/10] Batch 4000/7168 Train_loss 2.1820917214767186 
Epoch [8/10] Batch 4100/7168 Train_loss 2.181844864748246 
Epoch [8/10] Batch 4200/7168 Train_loss 2.1806988265685483 
Epoch [8/10] Batch 4300/7168 Train_loss 2.1805894000326247 
Epoch [8/10] Batch 4400/7168 Train_loss 2.1804498839963435 
Epoch [8/10] Batch 4500/7168 Train_loss 2.1796668519201186 
Epoch [8/10] Batch 4600/7168 Train_loss 2.1797728548553086 
Epoch [8/10] Batch 4700/7168 Train_loss 2.1799018385371762 
Epoch [8/10] Batch 4800/7168 Train_loss 2.1800507325737257 
Epoch [8/10] Batch 4900/7168 Train_loss 2.1805840192206465 
Epoch [8/10] Batch 5000/7168 Train_loss 2.1810857009778046 
Epoch [8/10] Batch 5100/7168 Train_loss 2.179485240319831 
Epoch [8/10] Batch 5200/7168 Train_loss 2.1801308770978296 
Epoch [8/10] Batch 5300/7168 Train_loss 2.1798715983594597 
Epoch [8/10] Batch 5400/7168 Train_loss 2.1785441322682897 
Epoch [8/10] Batch 5500/7168 Train_loss 2.1782801314643936 
Epoch [8/10] Batch 5600/7168 Train_loss 2.178335849077994 
Epoch [8/10] Batch 5700/7168 Train_loss 2.1783688662345315 
Epoch [8/10] Batch 5800/7168 Train_loss 2.177284638009037 
Epoch [8/10] Batch 5900/7168 Train_loss 2.177765722651842 
Epoch [8/10] Batch 6000/7168 Train_loss 2.1779559279278784 
Epoch [8/10] Batch 6100/7168 Train_loss 2.177582820787369 
Epoch [8/10] Batch 6200/7168 Train_loss 2.177298015196503 
Epoch [8/10] Batch 6300/7168 Train_loss 2.1780412750331357 
Epoch [8/10] Batch 6400/7168 Train_loss 2.17708974768223 
Epoch [8/10] Batch 6500/7168 Train_loss 2.175475047278672 
Epoch [8/10] Batch 6600/7168 Train_loss 2.175666756522762 
Epoch [8/10] Batch 6700/7168 Train_loss 2.1763730674555117 
Epoch [8/10] Batch 6800/7168 Train_loss 2.1766884927867247 
Epoch [8/10] Batch 6900/7168 Train_loss 2.176936791307977 
Epoch [8/10] Batch 7000/7168 Train_loss 2.1762443004382743 
Epoch [8/10] Batch 7100/7168 Train_loss 2.1751748166328886 
Epoch: 8/10 	Training Loss: 2.174271 	Validation Loss: 2.193720 Duration seconds: 949.317547082901 
best_valid_loss_fold [2.191264061781112] Best_Epoch [8]Epoch [9/10] Batch 0/7168 Train_loss 2.918941378593445 
Epoch [9/10] Batch 100/7168 Train_loss 2.0824091431528036 
Epoch [9/10] Batch 200/7168 Train_loss 2.115452801113698 
Epoch [9/10] Batch 300/7168 Train_loss 2.112137653007856 
Epoch [9/10] Batch 400/7168 Train_loss 2.1504954155766756 
Epoch [9/10] Batch 500/7168 Train_loss 2.161793149933368 
Epoch [9/10] Batch 600/7168 Train_loss 2.161926143429244 
Epoch [9/10] Batch 700/7168 Train_loss 2.146388310062222 
Epoch [9/10] Batch 800/7168 Train_loss 2.15867691132683 
Epoch [9/10] Batch 900/7168 Train_loss 2.154207302796986 
Epoch [9/10] Batch 1000/7168 Train_loss 2.1523545773980857 
Epoch [9/10] Batch 1100/7168 Train_loss 2.161880432138326 
Epoch [9/10] Batch 1200/7168 Train_loss 2.152550299461934 
Epoch [9/10] Batch 1300/7168 Train_loss 2.1583227682672583 
Epoch [9/10] Batch 1400/7168 Train_loss 2.153663063298201 
Epoch [9/10] Batch 1500/7168 Train_loss 2.157972079318893 
Epoch [9/10] Batch 1600/7168 Train_loss 2.161319454262809 
Epoch [9/10] Batch 1700/7168 Train_loss 2.1648906710514386 
Epoch [9/10] Batch 1800/7168 Train_loss 2.1711781779423083 
Epoch [9/10] Batch 1900/7168 Train_loss 2.1738857648941416 
Epoch [9/10] Batch 2000/7168 Train_loss 2.1741509553732006 
Epoch [9/10] Batch 2100/7168 Train_loss 2.175288551842979 
Epoch [9/10] Batch 2200/7168 Train_loss 2.173782772418858 
Epoch [9/10] Batch 2300/7168 Train_loss 2.176368889252759 
Epoch [9/10] Batch 2400/7168 Train_loss 2.177848219561209 
Epoch [9/10] Batch 2500/7168 Train_loss 2.1742990383597576 
Epoch [9/10] Batch 2600/7168 Train_loss 2.174402572664331 
Epoch [9/10] Batch 2700/7168 Train_loss 2.173553638405731 
Epoch [9/10] Batch 2800/7168 Train_loss 2.175821471920783 
Epoch [9/10] Batch 2900/7168 Train_loss 2.1754349840587275 
Epoch [9/10] Batch 3000/7168 Train_loss 2.173807784264503 
Epoch [9/10] Batch 3100/7168 Train_loss 2.1723969374662904 
Epoch [9/10] Batch 3200/7168 Train_loss 2.175190015691625 
Epoch [9/10] Batch 3300/7168 Train_loss 2.173978172556374 
Epoch [9/10] Batch 3400/7168 Train_loss 2.173613883802485 
Epoch [9/10] Batch 3500/7168 Train_loss 2.174114118947604 
Epoch [9/10] Batch 3600/7168 Train_loss 2.17521123974994 
Epoch [9/10] Batch 3700/7168 Train_loss 2.1767707147314175 
Epoch [9/10] Batch 3800/7168 Train_loss 2.1753659138935735 
Epoch [9/10] Batch 3900/7168 Train_loss 2.1772610030206954 
Epoch [9/10] Batch 4000/7168 Train_loss 2.176488532152393 
Epoch [9/10] Batch 4100/7168 Train_loss 2.1759035844347645 
Epoch [9/10] Batch 4200/7168 Train_loss 2.1766424276975758 
Epoch [9/10] Batch 4300/7168 Train_loss 2.177534308808539 
Epoch [9/10] Batch 4400/7168 Train_loss 2.1782312434021924 
Epoch [9/10] Batch 4500/7168 Train_loss 2.1784843798247584 
Epoch [9/10] Batch 4600/7168 Train_loss 2.1795504137961093 
Epoch [9/10] Batch 4700/7168 Train_loss 2.1787812598834164 
Epoch [9/10] Batch 4800/7168 Train_loss 2.1793079200702965 
Epoch [9/10] Batch 4900/7168 Train_loss 2.1784755100652506 
Epoch [9/10] Batch 5000/7168 Train_loss 2.1785054749469714 
Epoch [9/10] Batch 5100/7168 Train_loss 2.1774326592035282 
Epoch [9/10] Batch 5200/7168 Train_loss 2.178726073091614 
Epoch [9/10] Batch 5300/7168 Train_loss 2.178016230941853 
Epoch [9/10] Batch 5400/7168 Train_loss 2.177402719046932 
Epoch [9/10] Batch 5500/7168 Train_loss 2.176965416368951 
Epoch [9/10] Batch 5600/7168 Train_loss 2.1780237241837197 
Epoch [9/10] Batch 5700/7168 Train_loss 2.1774332219049652 
Epoch [9/10] Batch 5800/7168 Train_loss 2.176782332888617 
Epoch [9/10] Batch 5900/7168 Train_loss 2.178005990050664 
Epoch [9/10] Batch 6000/7168 Train_loss 2.177866627424186 
Epoch [9/10] Batch 6100/7168 Train_loss 2.177509306332378 
Epoch [9/10] Batch 6200/7168 Train_loss 2.177108861331766 
Epoch [9/10] Batch 6300/7168 Train_loss 2.1755331656068604 
Epoch [9/10] Batch 6400/7168 Train_loss 2.1739561865337564 
Epoch [9/10] Batch 6500/7168 Train_loss 2.1741612790328064 
Epoch [9/10] Batch 6600/7168 Train_loss 2.173532434422994 
Epoch [9/10] Batch 6700/7168 Train_loss 2.1745740141624514 
Epoch [9/10] Batch 6800/7168 Train_loss 2.1742906491207106 
Epoch [9/10] Batch 6900/7168 Train_loss 2.1738826944277503 
Epoch [9/10] Batch 7000/7168 Train_loss 2.175103656776274 
Epoch [9/10] Batch 7100/7168 Train_loss 2.1755530620029218 
Epoch: 9/10 	Training Loss: 2.174626 	Validation Loss: 2.191365 Duration seconds: 824.9100031852722 
best_valid_loss_fold [2.191264061781112] Best_Epoch [9]Fold: 3/5 
Epoch [0/10] Batch 0/7168 Train_loss 2.2572022676467896 
Epoch [0/10] Batch 100/7168 Train_loss 2.1877474854193113 
Epoch [0/10] Batch 200/7168 Train_loss 2.1821404734061134 
Epoch [0/10] Batch 300/7168 Train_loss 2.1642123457304265 
Epoch [0/10] Batch 400/7168 Train_loss 2.1639936998895277 
Epoch [0/10] Batch 500/7168 Train_loss 2.172496414737787 
Epoch [0/10] Batch 600/7168 Train_loss 2.1597630609936407 
Epoch [0/10] Batch 700/7168 Train_loss 2.1510305658471736 
Epoch [0/10] Batch 800/7168 Train_loss 2.158797945404023 
Epoch [0/10] Batch 900/7168 Train_loss 2.159464820954829 
Epoch [0/10] Batch 1000/7168 Train_loss 2.1504535998676446 
Epoch [0/10] Batch 1100/7168 Train_loss 2.155403164735607 
Epoch [0/10] Batch 1200/7168 Train_loss 2.1606462278235066 
Epoch [0/10] Batch 1300/7168 Train_loss 2.1653106642012228 
Epoch [0/10] Batch 1400/7168 Train_loss 2.1672464951840915 
Epoch [0/10] Batch 1500/7168 Train_loss 2.168396054546806 
Epoch [0/10] Batch 1600/7168 Train_loss 2.169158107913486 
Epoch [0/10] Batch 1700/7168 Train_loss 2.1661157103179836 
Epoch [0/10] Batch 1800/7168 Train_loss 2.16341278383892 
Epoch [0/10] Batch 1900/7168 Train_loss 2.1647651965040584 
Epoch [0/10] Batch 2000/7168 Train_loss 2.162786433058998 
Epoch [0/10] Batch 2100/7168 Train_loss 2.1602198736177405 
Epoch [0/10] Batch 2200/7168 Train_loss 2.15954402738379 
Epoch [0/10] Batch 2300/7168 Train_loss 2.1629297916252477 
Epoch [0/10] Batch 2400/7168 Train_loss 2.166003366402416 
Epoch [0/10] Batch 2500/7168 Train_loss 2.1687250931124264 
Epoch [0/10] Batch 2600/7168 Train_loss 2.167223823650302 
Epoch [0/10] Batch 2700/7168 Train_loss 2.1678378691456133 
Epoch [0/10] Batch 2800/7168 Train_loss 2.1641262999733444 
Epoch [0/10] Batch 2900/7168 Train_loss 2.1635390670186117 
Epoch [0/10] Batch 3000/7168 Train_loss 2.1624204693923827 
Epoch [0/10] Batch 3100/7168 Train_loss 2.16329215107061 
Epoch [0/10] Batch 3200/7168 Train_loss 2.164279961774067 
Epoch [0/10] Batch 3300/7168 Train_loss 2.164035795769414 
Epoch [0/10] Batch 3400/7168 Train_loss 2.162349361184764 
Epoch [0/10] Batch 3500/7168 Train_loss 2.161824757015661 
Epoch [0/10] Batch 3600/7168 Train_loss 2.1623356954624704 
Epoch [0/10] Batch 3700/7168 Train_loss 2.1652472696612572 
Epoch [0/10] Batch 3800/7168 Train_loss 2.1636231579190333 
Epoch [0/10] Batch 3900/7168 Train_loss 2.1639862555367797 
Epoch [0/10] Batch 4000/7168 Train_loss 2.1640467201842575 
Epoch [0/10] Batch 4100/7168 Train_loss 2.1664462294325078 
Epoch [0/10] Batch 4200/7168 Train_loss 2.169310712113434 
Epoch [0/10] Batch 4300/7168 Train_loss 2.1701521033382614 
Epoch [0/10] Batch 4400/7168 Train_loss 2.1704303166306147 
Epoch [0/10] Batch 4500/7168 Train_loss 2.1699849828511604 
Epoch [0/10] Batch 4600/7168 Train_loss 2.1719900197216906 
Epoch [0/10] Batch 4700/7168 Train_loss 2.173817816363373 
Epoch [0/10] Batch 4800/7168 Train_loss 2.1724813387893533 
Epoch [0/10] Batch 4900/7168 Train_loss 2.173354032300633 
Epoch [0/10] Batch 5000/7168 Train_loss 2.172203676473782 
Epoch [0/10] Batch 5100/7168 Train_loss 2.1734428488140223 
Epoch [0/10] Batch 5200/7168 Train_loss 2.1724094038478596 
Epoch [0/10] Batch 5300/7168 Train_loss 2.1728280438800684 
Epoch [0/10] Batch 5400/7168 Train_loss 2.1726262361815496 
Epoch [0/10] Batch 5500/7168 Train_loss 2.1725007233549696 
Epoch [0/10] Batch 5600/7168 Train_loss 2.172627107314586 
Epoch [0/10] Batch 5700/7168 Train_loss 2.1742893801177097 
Epoch [0/10] Batch 5800/7168 Train_loss 2.175330814116064 
Epoch [0/10] Batch 5900/7168 Train_loss 2.1745143244047727 
Epoch [0/10] Batch 6000/7168 Train_loss 2.174618636006376 
Epoch [0/10] Batch 6100/7168 Train_loss 2.173631774623987 
Epoch [0/10] Batch 6200/7168 Train_loss 2.172383733827863 
Epoch [0/10] Batch 6300/7168 Train_loss 2.1729560880798364 
Epoch [0/10] Batch 6400/7168 Train_loss 2.175126905400126 
Epoch [0/10] Batch 6500/7168 Train_loss 2.177825972926083 
Epoch [0/10] Batch 6600/7168 Train_loss 2.1794578620853287 
Epoch [0/10] Batch 6700/7168 Train_loss 2.1808629494881457 
Epoch [0/10] Batch 6800/7168 Train_loss 2.180840278934721 
Epoch [0/10] Batch 6900/7168 Train_loss 2.1818243670728723 
Epoch [0/10] Batch 7000/7168 Train_loss 2.181542978369667 
Epoch [0/10] Batch 7100/7168 Train_loss 2.18107695843557 
Epoch: 0/10 	Training Loss: 2.180456 	Validation Loss: 2.166426 Duration seconds: 830.9381353855133 
Validation loss decreased (inf --> 2.166426).  Saving model ... 
best_valid_loss_fold [2.1664264701373344] Best_Epoch [0]Epoch [1/10] Batch 0/7168 Train_loss 2.307540476322174 
Epoch [1/10] Batch 100/7168 Train_loss 2.1797319560652912 
Epoch [1/10] Batch 200/7168 Train_loss 2.1513171786840877 
Epoch [1/10] Batch 300/7168 Train_loss 2.163906353355642 
Epoch [1/10] Batch 400/7168 Train_loss 2.1576200048450818 
Epoch [1/10] Batch 500/7168 Train_loss 2.148411208968201 
Epoch [1/10] Batch 600/7168 Train_loss 2.1472766477236536 
Epoch [1/10] Batch 700/7168 Train_loss 2.153291329954048 
Epoch [1/10] Batch 800/7168 Train_loss 2.1591443286629652 
Epoch [1/10] Batch 900/7168 Train_loss 2.1616521323891513 
Epoch [1/10] Batch 1000/7168 Train_loss 2.1579164501045134 
Epoch [1/10] Batch 1100/7168 Train_loss 2.156796781490003 
Epoch [1/10] Batch 1200/7168 Train_loss 2.1585914094953313 
Epoch [1/10] Batch 1300/7168 Train_loss 2.1593864148140502 
Epoch [1/10] Batch 1400/7168 Train_loss 2.1579698392912627 
Epoch [1/10] Batch 1500/7168 Train_loss 2.154405234278559 
Epoch [1/10] Batch 1600/7168 Train_loss 2.1595075928945233 
Epoch [1/10] Batch 1700/7168 Train_loss 2.1555880513228365 
Epoch [1/10] Batch 1800/7168 Train_loss 2.1574910002854186 
Epoch [1/10] Batch 1900/7168 Train_loss 2.1573507335470077 
Epoch [1/10] Batch 2000/7168 Train_loss 2.1597407237700614 
Epoch [1/10] Batch 2100/7168 Train_loss 2.1625982624244147 
Epoch [1/10] Batch 2200/7168 Train_loss 2.1678929497360477 
Epoch [1/10] Batch 2300/7168 Train_loss 2.171578932292873 
Epoch [1/10] Batch 2400/7168 Train_loss 2.1737272373987504 
Epoch [1/10] Batch 2500/7168 Train_loss 2.1756985719825495 
Epoch [1/10] Batch 2600/7168 Train_loss 2.1758639621979823 
Epoch [1/10] Batch 2700/7168 Train_loss 2.1779188320664113 
Epoch [1/10] Batch 2800/7168 Train_loss 2.1761271879156756 
Epoch [1/10] Batch 2900/7168 Train_loss 2.1758780614574054 
Epoch [1/10] Batch 3000/7168 Train_loss 2.179511417198761 
Epoch [1/10] Batch 3100/7168 Train_loss 2.180190048138775 
Epoch [1/10] Batch 3200/7168 Train_loss 2.181379562758796 
Epoch [1/10] Batch 3300/7168 Train_loss 2.1813812068929024 
Epoch [1/10] Batch 3400/7168 Train_loss 2.180136608962661 
Epoch [1/10] Batch 3500/7168 Train_loss 2.1804111077636694 
Epoch [1/10] Batch 3600/7168 Train_loss 2.1793553279286058 
Epoch [1/10] Batch 3700/7168 Train_loss 2.1819133008131946 
Epoch [1/10] Batch 3800/7168 Train_loss 2.1814597712452866 
Epoch [1/10] Batch 3900/7168 Train_loss 2.18009587666662 
Epoch [1/10] Batch 4000/7168 Train_loss 2.180249061797655 
Epoch [1/10] Batch 4100/7168 Train_loss 2.181721059153557 
Epoch [1/10] Batch 4200/7168 Train_loss 2.1799189301544186 
Epoch [1/10] Batch 4300/7168 Train_loss 2.1784561112873 
Epoch [1/10] Batch 4400/7168 Train_loss 2.1779115399368187 
Epoch [1/10] Batch 4500/7168 Train_loss 2.1776242018858345 
Epoch [1/10] Batch 4600/7168 Train_loss 2.178821106354482 
Epoch [1/10] Batch 4700/7168 Train_loss 2.1779786739906486 
Epoch [1/10] Batch 4800/7168 Train_loss 2.1786577502834725 
Epoch [1/10] Batch 4900/7168 Train_loss 2.176970939576273 
Epoch [1/10] Batch 5000/7168 Train_loss 2.1765478257625444 
Epoch [1/10] Batch 5100/7168 Train_loss 2.177583081824853 
Epoch [1/10] Batch 5200/7168 Train_loss 2.1780663564116467 
Epoch [1/10] Batch 5300/7168 Train_loss 2.1783197674548216 
Epoch [1/10] Batch 5400/7168 Train_loss 2.1774493722761368 
Epoch [1/10] Batch 5500/7168 Train_loss 2.176848814254391 
Epoch [1/10] Batch 5600/7168 Train_loss 2.175586420275807 
Epoch [1/10] Batch 5700/7168 Train_loss 2.175638373615114 
Epoch [1/10] Batch 5800/7168 Train_loss 2.177036199985219 
Epoch [1/10] Batch 5900/7168 Train_loss 2.1775935491027116 
Epoch [1/10] Batch 6000/7168 Train_loss 2.177983816304002 
Epoch [1/10] Batch 6100/7168 Train_loss 2.180080090320769 
Epoch [1/10] Batch 6200/7168 Train_loss 2.1804639557098655 
Epoch [1/10] Batch 6300/7168 Train_loss 2.1803696867663263 
Epoch [1/10] Batch 6400/7168 Train_loss 2.1807626772999598 
Epoch [1/10] Batch 6500/7168 Train_loss 2.1796577509651365 
Epoch [1/10] Batch 6600/7168 Train_loss 2.1791995593389406 
Epoch [1/10] Batch 6700/7168 Train_loss 2.1790055421159757 
Epoch [1/10] Batch 6800/7168 Train_loss 2.1793069214027536 
Epoch [1/10] Batch 6900/7168 Train_loss 2.179425590214237 
Epoch [1/10] Batch 7000/7168 Train_loss 2.179672806197159 
Epoch [1/10] Batch 7100/7168 Train_loss 2.179852869236256 
Epoch: 1/10 	Training Loss: 2.180479 	Validation Loss: 2.168361 Duration seconds: 896.3595635890961 
best_valid_loss_fold [2.1664264701373344] Best_Epoch [1]Epoch [2/10] Batch 0/7168 Train_loss 1.4418306946754456 
Epoch [2/10] Batch 100/7168 Train_loss 2.195298692377487 
Epoch [2/10] Batch 200/7168 Train_loss 2.2001614251095263 
Epoch [2/10] Batch 300/7168 Train_loss 2.1907550863847383 
Epoch [2/10] Batch 400/7168 Train_loss 2.1766409707113987 
Epoch [2/10] Batch 500/7168 Train_loss 2.166145310519698 
Epoch [2/10] Batch 600/7168 Train_loss 2.1676878514583415 
Epoch [2/10] Batch 700/7168 Train_loss 2.1699732083949486 
Epoch [2/10] Batch 800/7168 Train_loss 2.176877223280783 
Epoch [2/10] Batch 900/7168 Train_loss 2.1816176348898177 
Epoch [2/10] Batch 1000/7168 Train_loss 2.181151164146689 
Epoch [2/10] Batch 1100/7168 Train_loss 2.1772344160713355 
Epoch [2/10] Batch 1200/7168 Train_loss 2.1693222781799912 
Epoch [2/10] Batch 1300/7168 Train_loss 2.1707422986660987 
Epoch [2/10] Batch 1400/7168 Train_loss 2.173389240322923 
Epoch [2/10] Batch 1500/7168 Train_loss 2.1650073704602004 
Epoch [2/10] Batch 1600/7168 Train_loss 2.1652937800548435 
Epoch [2/10] Batch 1700/7168 Train_loss 2.167862243140045 
Epoch [2/10] Batch 1800/7168 Train_loss 2.1686206594200548 
Epoch [2/10] Batch 1900/7168 Train_loss 2.172458674846857 
Epoch [2/10] Batch 2000/7168 Train_loss 2.174804566026866 
Epoch [2/10] Batch 2100/7168 Train_loss 2.178494215032874 
Epoch [2/10] Batch 2200/7168 Train_loss 2.1820557896709616 
Epoch [2/10] Batch 2300/7168 Train_loss 2.1799061557473127 
Epoch [2/10] Batch 2400/7168 Train_loss 2.179515676162532 
Epoch [2/10] Batch 2500/7168 Train_loss 2.1811779339115223 
Epoch [2/10] Batch 2600/7168 Train_loss 2.1801993793097885 
Epoch [2/10] Batch 2700/7168 Train_loss 2.183103822268234 
Epoch [2/10] Batch 2800/7168 Train_loss 2.183023673541756 
Epoch [2/10] Batch 2900/7168 Train_loss 2.1838873005075645 
Epoch [2/10] Batch 3000/7168 Train_loss 2.186181761674466 
Epoch [2/10] Batch 3100/7168 Train_loss 2.185480730095743 
Epoch [2/10] Batch 3200/7168 Train_loss 2.1866328876974137 
Epoch [2/10] Batch 3300/7168 Train_loss 2.187478742823208 
Epoch [2/10] Batch 3400/7168 Train_loss 2.1860339214997375 
Epoch [2/10] Batch 3500/7168 Train_loss 2.1875230232304212 
Epoch [2/10] Batch 3600/7168 Train_loss 2.188754639185094 
Epoch [2/10] Batch 3700/7168 Train_loss 2.1911531240875353 
Epoch [2/10] Batch 3800/7168 Train_loss 2.191748916710781 
Epoch [2/10] Batch 3900/7168 Train_loss 2.1898495656875303 
Epoch [2/10] Batch 4000/7168 Train_loss 2.1914818984430986 
Epoch [2/10] Batch 4100/7168 Train_loss 2.1917735305618176 
Epoch [2/10] Batch 4200/7168 Train_loss 2.189326947699045 
Epoch [2/10] Batch 4300/7168 Train_loss 2.1878542769782574 
Epoch [2/10] Batch 4400/7168 Train_loss 2.1897642412793226 
Epoch [2/10] Batch 4500/7168 Train_loss 2.1894601167367314 
Epoch [2/10] Batch 4600/7168 Train_loss 2.188606562610684 
Epoch [2/10] Batch 4700/7168 Train_loss 2.1871333559937387 
Epoch [2/10] Batch 4800/7168 Train_loss 2.1868132967156586 
Epoch [2/10] Batch 4900/7168 Train_loss 2.1863401323325826 
Epoch [2/10] Batch 5000/7168 Train_loss 2.1864671553165715 
Epoch [2/10] Batch 5100/7168 Train_loss 2.1870680607474102 
Epoch [2/10] Batch 5200/7168 Train_loss 2.185809917503012 
Epoch [2/10] Batch 5300/7168 Train_loss 2.1855203855984437 
Epoch [2/10] Batch 5400/7168 Train_loss 2.1864045973763027 
Epoch [2/10] Batch 5500/7168 Train_loss 2.18637008178648 
Epoch [2/10] Batch 5600/7168 Train_loss 2.1864851725658037 
Epoch [2/10] Batch 5700/7168 Train_loss 2.1856916767842853 
Epoch [2/10] Batch 5800/7168 Train_loss 2.185598685566854 
Epoch [2/10] Batch 5900/7168 Train_loss 2.1853201699734663 
Epoch [2/10] Batch 6000/7168 Train_loss 2.184451186985716 
Epoch [2/10] Batch 6100/7168 Train_loss 2.1850206265877827 
Epoch [2/10] Batch 6200/7168 Train_loss 2.183809332012496 
Epoch [2/10] Batch 6300/7168 Train_loss 2.184569814083236 
Epoch [2/10] Batch 6400/7168 Train_loss 2.183867437700111 
Epoch [2/10] Batch 6500/7168 Train_loss 2.1832975807661756 
Epoch [2/10] Batch 6600/7168 Train_loss 2.182216163255684 
Epoch [2/10] Batch 6700/7168 Train_loss 2.182292846270398 
Epoch [2/10] Batch 6800/7168 Train_loss 2.181860023106815 
Epoch [2/10] Batch 6900/7168 Train_loss 2.1812540021642812 
Epoch [2/10] Batch 7000/7168 Train_loss 2.1817101261910667 
Epoch [2/10] Batch 7100/7168 Train_loss 2.1815486255560503 
Epoch: 2/10 	Training Loss: 2.180421 	Validation Loss: 2.167602 Duration seconds: 897.2220883369446 
best_valid_loss_fold [2.1664264701373344] Best_Epoch [2]Epoch [3/10] Batch 0/7168 Train_loss 3.0462294816970825 
Epoch [3/10] Batch 100/7168 Train_loss 2.166284017338611 
Epoch [3/10] Batch 200/7168 Train_loss 2.133427857240634 
Epoch [3/10] Batch 300/7168 Train_loss 2.104939049066499 
Epoch [3/10] Batch 400/7168 Train_loss 2.1289113598720095 
Epoch [3/10] Batch 500/7168 Train_loss 2.1569250244461373 
Epoch [3/10] Batch 600/7168 Train_loss 2.178100553184698 
Epoch [3/10] Batch 700/7168 Train_loss 2.1738949049258878 
Epoch [3/10] Batch 800/7168 Train_loss 2.165775170152107 
Epoch [3/10] Batch 900/7168 Train_loss 2.171403818188047 
Epoch [3/10] Batch 1000/7168 Train_loss 2.1729187185054535 
Epoch [3/10] Batch 1100/7168 Train_loss 2.1649545788440134 
Epoch [3/10] Batch 1200/7168 Train_loss 2.1663375837816785 
Epoch [3/10] Batch 1300/7168 Train_loss 2.1639407834440445 
Epoch [3/10] Batch 1400/7168 Train_loss 2.164553354343374 
Epoch [3/10] Batch 1500/7168 Train_loss 2.167302443237006 
Epoch [3/10] Batch 1600/7168 Train_loss 2.1685392885171644 
Epoch [3/10] Batch 1700/7168 Train_loss 2.164337310035533 
Epoch [3/10] Batch 1800/7168 Train_loss 2.1615240984761273 
Epoch [3/10] Batch 1900/7168 Train_loss 2.1632808008706927 
Epoch [3/10] Batch 2000/7168 Train_loss 2.1598547112488973 
Epoch [3/10] Batch 2100/7168 Train_loss 2.158749203726486 
Epoch [3/10] Batch 2200/7168 Train_loss 2.166245567431671 
Epoch [3/10] Batch 2300/7168 Train_loss 2.1681097775749514 
Epoch [3/10] Batch 2400/7168 Train_loss 2.1675739269773446 
Epoch [3/10] Batch 2500/7168 Train_loss 2.168635782547018 
Epoch [3/10] Batch 2600/7168 Train_loss 2.166882843947878 
Epoch [3/10] Batch 2700/7168 Train_loss 2.1657858250517177 
Epoch [3/10] Batch 2800/7168 Train_loss 2.165482388340503 
Epoch [3/10] Batch 2900/7168 Train_loss 2.166081709451447 
Epoch [3/10] Batch 3000/7168 Train_loss 2.1661446651928586 
Epoch [3/10] Batch 3100/7168 Train_loss 2.1665686830008504 
Epoch [3/10] Batch 3200/7168 Train_loss 2.169097337949764 
Epoch [3/10] Batch 3300/7168 Train_loss 2.168642034253652 
Epoch [3/10] Batch 3400/7168 Train_loss 2.169262823577279 
Epoch [3/10] Batch 3500/7168 Train_loss 2.1682597057703936 
Epoch [3/10] Batch 3600/7168 Train_loss 2.1679476502288617 
Epoch [3/10] Batch 3700/7168 Train_loss 2.1692607387340703 
Epoch [3/10] Batch 3800/7168 Train_loss 2.1699933931172692 
Epoch [3/10] Batch 3900/7168 Train_loss 2.1710900713490755 
Epoch [3/10] Batch 4000/7168 Train_loss 2.1728979237703525 
Epoch [3/10] Batch 4100/7168 Train_loss 2.174471579755524 
Epoch [3/10] Batch 4200/7168 Train_loss 2.175848118143262 
Epoch [3/10] Batch 4300/7168 Train_loss 2.176473549052023 
Epoch [3/10] Batch 4400/7168 Train_loss 2.1787596962900766 
Epoch [3/10] Batch 4500/7168 Train_loss 2.17628316966607 
Epoch [3/10] Batch 4600/7168 Train_loss 2.1766676217434173 
Epoch [3/10] Batch 4700/7168 Train_loss 2.178340872848346 
Epoch [3/10] Batch 4800/7168 Train_loss 2.179006337374604 
Epoch [3/10] Batch 4900/7168 Train_loss 2.176913683194575 
Epoch [3/10] Batch 5000/7168 Train_loss 2.177419881061706 
Epoch [3/10] Batch 5100/7168 Train_loss 2.1787619540514234 
Epoch [3/10] Batch 5200/7168 Train_loss 2.1802793313702398 
Epoch [3/10] Batch 5300/7168 Train_loss 2.1822197119136324 
Epoch [3/10] Batch 5400/7168 Train_loss 2.1812777485373593 
Epoch [3/10] Batch 5500/7168 Train_loss 2.182604107891315 
Epoch [3/10] Batch 5600/7168 Train_loss 2.1843600383939923 
Epoch [3/10] Batch 5700/7168 Train_loss 2.1862965290707637 
Epoch [3/10] Batch 5800/7168 Train_loss 2.1848633497891767 
Epoch [3/10] Batch 5900/7168 Train_loss 2.184129469380745 
Epoch [3/10] Batch 6000/7168 Train_loss 2.18239732659016 
Epoch [3/10] Batch 6100/7168 Train_loss 2.183233511555861 
Epoch [3/10] Batch 6200/7168 Train_loss 2.1847323546571475 
Epoch [3/10] Batch 6300/7168 Train_loss 2.183688190121099 
Epoch [3/10] Batch 6400/7168 Train_loss 2.182938642118931 
Epoch [3/10] Batch 6500/7168 Train_loss 2.1831218570635844 
Epoch [3/10] Batch 6600/7168 Train_loss 2.1832005244943162 
Epoch [3/10] Batch 6700/7168 Train_loss 2.183092536132045 
Epoch [3/10] Batch 6800/7168 Train_loss 2.183399086692389 
Epoch [3/10] Batch 6900/7168 Train_loss 2.1831353268615925 
Epoch [3/10] Batch 7000/7168 Train_loss 2.1829132881526894 
Epoch [3/10] Batch 7100/7168 Train_loss 2.1809854546141882 
Epoch: 3/10 	Training Loss: 2.180525 	Validation Loss: 2.167484 Duration seconds: 915.4881963729858 
best_valid_loss_fold [2.1664264701373344] Best_Epoch [3]Epoch [4/10] Batch 0/7168 Train_loss 2.368558496236801 
Epoch [4/10] Batch 100/7168 Train_loss 2.16898742214878 
Epoch [4/10] Batch 200/7168 Train_loss 2.172349821127469 
Epoch [4/10] Batch 300/7168 Train_loss 2.1963090848091036 
Epoch [4/10] Batch 400/7168 Train_loss 2.172995675308746 
Epoch [4/10] Batch 500/7168 Train_loss 2.162886706297983 
Epoch [4/10] Batch 600/7168 Train_loss 2.177470277083694 
Epoch [4/10] Batch 700/7168 Train_loss 2.172396325010716 
Epoch [4/10] Batch 800/7168 Train_loss 2.1674903024560055 
Epoch [4/10] Batch 900/7168 Train_loss 2.1596938507273777 
Epoch [4/10] Batch 1000/7168 Train_loss 2.1596270635887818 
Epoch [4/10] Batch 1100/7168 Train_loss 2.158741109770174 
Epoch [4/10] Batch 1200/7168 Train_loss 2.1564434538227037 
Epoch [4/10] Batch 1300/7168 Train_loss 2.1610296547962644 
Epoch [4/10] Batch 1400/7168 Train_loss 2.161651506626461 
Epoch [4/10] Batch 1500/7168 Train_loss 2.1755628224256274 
Epoch [4/10] Batch 1600/7168 Train_loss 2.17338737395873 
Epoch [4/10] Batch 1700/7168 Train_loss 2.1718715824655472 
Epoch [4/10] Batch 1800/7168 Train_loss 2.1733127043751597 
Epoch [4/10] Batch 1900/7168 Train_loss 2.1757142333169788 
Epoch [4/10] Batch 2000/7168 Train_loss 2.176788403079666 
Epoch [4/10] Batch 2100/7168 Train_loss 2.1782664206767297 
Epoch [4/10] Batch 2200/7168 Train_loss 2.180357527111347 
Epoch [4/10] Batch 2300/7168 Train_loss 2.1835946734640093 
Epoch [4/10] Batch 2400/7168 Train_loss 2.1810386289859105 
Epoch [4/10] Batch 2500/7168 Train_loss 2.1791684839938554 
Epoch [4/10] Batch 2600/7168 Train_loss 2.179908607812489 
Epoch [4/10] Batch 2700/7168 Train_loss 2.176588479897041 
Epoch [4/10] Batch 2800/7168 Train_loss 2.1764713904020065 
Epoch [4/10] Batch 2900/7168 Train_loss 2.175596562162509 
Epoch [4/10] Batch 3000/7168 Train_loss 2.175834461033682 
Epoch [4/10] Batch 3100/7168 Train_loss 2.178320498552602 
Epoch [4/10] Batch 3200/7168 Train_loss 2.177457616957453 
Epoch [4/10] Batch 3300/7168 Train_loss 2.1787387645901855 
Epoch [4/10] Batch 3400/7168 Train_loss 2.17706182727881 
Epoch [4/10] Batch 3500/7168 Train_loss 2.1783557576354795 
Epoch [4/10] Batch 3600/7168 Train_loss 2.1795116043428484 
Epoch [4/10] Batch 3700/7168 Train_loss 2.1805602306853884 
Epoch [4/10] Batch 3800/7168 Train_loss 2.1803928517693127 
Epoch [4/10] Batch 3900/7168 Train_loss 2.179186719082252 
Epoch [4/10] Batch 4000/7168 Train_loss 2.1765734866108786 
Epoch [4/10] Batch 4100/7168 Train_loss 2.1787441396525766 
Epoch [4/10] Batch 4200/7168 Train_loss 2.177736411468383 
Epoch [4/10] Batch 4300/7168 Train_loss 2.1780275719937987 
Epoch [4/10] Batch 4400/7168 Train_loss 2.1780374312171933 
Epoch [4/10] Batch 4500/7168 Train_loss 2.1798196334331412 
Epoch [4/10] Batch 4600/7168 Train_loss 2.178613401980173 
Epoch [4/10] Batch 4700/7168 Train_loss 2.1775304336061985 
Epoch [4/10] Batch 4800/7168 Train_loss 2.1788433256701265 
Epoch [4/10] Batch 4900/7168 Train_loss 2.179599377016733 
Epoch [4/10] Batch 5000/7168 Train_loss 2.180697571212853 
Epoch [4/10] Batch 5100/7168 Train_loss 2.1816925758375794 
Epoch [4/10] Batch 5200/7168 Train_loss 2.1814990498038167 
Epoch [4/10] Batch 5300/7168 Train_loss 2.1825211660307864 
Epoch [4/10] Batch 5400/7168 Train_loss 2.181218118344583 
Epoch [4/10] Batch 5500/7168 Train_loss 2.1819932408965386 
Epoch [4/10] Batch 5600/7168 Train_loss 2.182730066350072 
Epoch [4/10] Batch 5700/7168 Train_loss 2.1830287036237497 
Epoch [4/10] Batch 5800/7168 Train_loss 2.183406246279236 
Epoch [4/10] Batch 5900/7168 Train_loss 2.1821915986446623 
Epoch [4/10] Batch 6000/7168 Train_loss 2.1820208394781706 
Epoch [4/10] Batch 6100/7168 Train_loss 2.181250260020256 
Epoch [4/10] Batch 6200/7168 Train_loss 2.1801630661571587 
Epoch [4/10] Batch 6300/7168 Train_loss 2.1798954729077704 
Epoch [4/10] Batch 6400/7168 Train_loss 2.179060316720413 
Epoch [4/10] Batch 6500/7168 Train_loss 2.1783189338008326 
Epoch [4/10] Batch 6600/7168 Train_loss 2.1786661993403484 
Epoch [4/10] Batch 6700/7168 Train_loss 2.1795320413242862 
Epoch [4/10] Batch 6800/7168 Train_loss 2.179281650010566 
Epoch [4/10] Batch 6900/7168 Train_loss 2.179956563802806 
Epoch [4/10] Batch 7000/7168 Train_loss 2.1805415252816114 
Epoch [4/10] Batch 7100/7168 Train_loss 2.180407143653975 
Epoch: 4/10 	Training Loss: 2.180517 	Validation Loss: 2.167516 Duration seconds: 916.1039056777954 
best_valid_loss_fold [2.1664264701373344] Best_Epoch [4]Epoch [5/10] Batch 0/7168 Train_loss 2.731966972351074 
Epoch [5/10] Batch 100/7168 Train_loss 2.17368483543396 
Epoch [5/10] Batch 200/7168 Train_loss 2.183460980208952 
Epoch [5/10] Batch 300/7168 Train_loss 2.173617875011656 
Epoch [5/10] Batch 400/7168 Train_loss 2.165003793309454 
Epoch [5/10] Batch 500/7168 Train_loss 2.1670613031901285 
Epoch [5/10] Batch 600/7168 Train_loss 2.1657291879124334 
Epoch [5/10] Batch 700/7168 Train_loss 2.172024598857986 
Epoch [5/10] Batch 800/7168 Train_loss 2.1669827252812452 
Epoch [5/10] Batch 900/7168 Train_loss 2.1693713093935187 
Epoch [5/10] Batch 1000/7168 Train_loss 2.176968940904924 
Epoch [5/10] Batch 1100/7168 Train_loss 2.1822649335233217 
Epoch [5/10] Batch 1200/7168 Train_loss 2.1869797986016284 
Epoch [5/10] Batch 1300/7168 Train_loss 2.1814393758384383 
Epoch [5/10] Batch 1400/7168 Train_loss 2.178810564781154 
Epoch [5/10] Batch 1500/7168 Train_loss 2.178593359386262 
Epoch [5/10] Batch 1600/7168 Train_loss 2.181083062974309 
Epoch [5/10] Batch 1700/7168 Train_loss 2.1849274394846465 
Epoch [5/10] Batch 1800/7168 Train_loss 2.182467576780298 
Epoch [5/10] Batch 1900/7168 Train_loss 2.183396689621227 
Epoch [5/10] Batch 2000/7168 Train_loss 2.182761452671351 
Epoch [5/10] Batch 2100/7168 Train_loss 2.181858729636709 
Epoch [5/10] Batch 2200/7168 Train_loss 2.178746228398707 
Epoch [5/10] Batch 2300/7168 Train_loss 2.1808477142410454 
Epoch [5/10] Batch 2400/7168 Train_loss 2.176777031994124 
Epoch [5/10] Batch 2500/7168 Train_loss 2.177498741853671 
Epoch [5/10] Batch 2600/7168 Train_loss 2.177555314431691 
Epoch [5/10] Batch 2700/7168 Train_loss 2.179713311933756 
Epoch [5/10] Batch 2800/7168 Train_loss 2.180216935506807 
Epoch [5/10] Batch 2900/7168 Train_loss 2.1808621005862387 
Epoch [5/10] Batch 3000/7168 Train_loss 2.1801886687186194 
Epoch [5/10] Batch 3100/7168 Train_loss 2.178155377444741 
Epoch [5/10] Batch 3200/7168 Train_loss 2.177840388429161 
Epoch [5/10] Batch 3300/7168 Train_loss 2.180098115707195 
Epoch [5/10] Batch 3400/7168 Train_loss 2.1791644048923886 
Epoch [5/10] Batch 3500/7168 Train_loss 2.1790823228476355 
Epoch [5/10] Batch 3600/7168 Train_loss 2.1777436840669937 
Epoch [5/10] Batch 3700/7168 Train_loss 2.1756003962734587 
Epoch [5/10] Batch 3800/7168 Train_loss 2.1766591503514454 
Epoch [5/10] Batch 3900/7168 Train_loss 2.1778100463510386 
Epoch [5/10] Batch 4000/7168 Train_loss 2.181094713067746 
Epoch [5/10] Batch 4100/7168 Train_loss 2.1800705042373143 
Epoch [5/10] Batch 4200/7168 Train_loss 2.1818037878946304 
Epoch [5/10] Batch 4300/7168 Train_loss 2.1825371262217033 
Epoch [5/10] Batch 4400/7168 Train_loss 2.1838489047026695 
Epoch [5/10] Batch 4500/7168 Train_loss 2.184097532433712 
Epoch [5/10] Batch 4600/7168 Train_loss 2.1850923796959014 
Epoch [5/10] Batch 4700/7168 Train_loss 2.185579340693368 
Epoch [5/10] Batch 4800/7168 Train_loss 2.18439806017608 
Epoch [5/10] Batch 4900/7168 Train_loss 2.1844375526794386 
Epoch [5/10] Batch 5000/7168 Train_loss 2.1852081576226308 
Epoch [5/10] Batch 5100/7168 Train_loss 2.1849435463422093 
Epoch [5/10] Batch 5200/7168 Train_loss 2.1850596724296163 
Epoch [5/10] Batch 5300/7168 Train_loss 2.1829593485339682 
Epoch [5/10] Batch 5400/7168 Train_loss 2.182356079647528 
Epoch [5/10] Batch 5500/7168 Train_loss 2.1820836618117387 
Epoch [5/10] Batch 5600/7168 Train_loss 2.182095838091114 
Epoch [5/10] Batch 5700/7168 Train_loss 2.1833660803673833 
Epoch [5/10] Batch 5800/7168 Train_loss 2.1829053872598694 
Epoch [5/10] Batch 5900/7168 Train_loss 2.1817054069611888 
Epoch [5/10] Batch 6000/7168 Train_loss 2.1830449888501002 
Epoch [5/10] Batch 6100/7168 Train_loss 2.1845595881897317 
Epoch [5/10] Batch 6200/7168 Train_loss 2.183445530422867 
Epoch [5/10] Batch 6300/7168 Train_loss 2.1826174249584342 
Epoch [5/10] Batch 6400/7168 Train_loss 2.1826555349900003 
Epoch [5/10] Batch 6500/7168 Train_loss 2.1824182051729193 
Epoch [5/10] Batch 6600/7168 Train_loss 2.181455479270851 
Epoch [5/10] Batch 6700/7168 Train_loss 2.181930612095024 
Epoch [5/10] Batch 6800/7168 Train_loss 2.181875442270145 
Epoch [5/10] Batch 6900/7168 Train_loss 2.181091411051379 
Epoch [5/10] Batch 7000/7168 Train_loss 2.180265398729854 
Epoch [5/10] Batch 7100/7168 Train_loss 2.1808852394262415 
Epoch: 5/10 	Training Loss: 2.180645 	Validation Loss: 2.166584 Duration seconds: 942.8959536552429 
best_valid_loss_fold [2.1664264701373344] Best_Epoch [5]Epoch [6/10] Batch 0/7168 Train_loss 2.0617492496967316 
Epoch [6/10] Batch 100/7168 Train_loss 2.1649282211714453 
Epoch [6/10] Batch 200/7168 Train_loss 2.1909310197948817 
Epoch [6/10] Batch 300/7168 Train_loss 2.1614015228625547 
Epoch [6/10] Batch 400/7168 Train_loss 2.173398817678045 
Epoch [6/10] Batch 500/7168 Train_loss 2.163047717954822 
Epoch [6/10] Batch 600/7168 Train_loss 2.1638343044356776 
Epoch [6/10] Batch 700/7168 Train_loss 2.159737382461443 
Epoch [6/10] Batch 800/7168 Train_loss 2.166818491882004 
Epoch [6/10] Batch 900/7168 Train_loss 2.1694828244081217 
Epoch [6/10] Batch 1000/7168 Train_loss 2.1707079083948106 
Epoch [6/10] Batch 1100/7168 Train_loss 2.1672501217414855 
Epoch [6/10] Batch 1200/7168 Train_loss 2.1732232735168924 
Epoch [6/10] Batch 1300/7168 Train_loss 2.1646297197906352 
Epoch [6/10] Batch 1400/7168 Train_loss 2.164751538085478 
Epoch [6/10] Batch 1500/7168 Train_loss 2.1726106636370126 
Epoch [6/10] Batch 1600/7168 Train_loss 2.1711090900967225 
Epoch [6/10] Batch 1700/7168 Train_loss 2.1692675163313475 
Epoch [6/10] Batch 1800/7168 Train_loss 2.172076795239835 
Epoch [6/10] Batch 1900/7168 Train_loss 2.172261082509955 
Epoch [6/10] Batch 2000/7168 Train_loss 2.1719736126677387 
Epoch [6/10] Batch 2100/7168 Train_loss 2.1737753624350953 
Epoch [6/10] Batch 2200/7168 Train_loss 2.173516132929335 
Epoch [6/10] Batch 2300/7168 Train_loss 2.1770344491965457 
Epoch [6/10] Batch 2400/7168 Train_loss 2.176368022923269 
Epoch [6/10] Batch 2500/7168 Train_loss 2.17549979609282 
Epoch [6/10] Batch 2600/7168 Train_loss 2.1755562057544617 
Epoch [6/10] Batch 2700/7168 Train_loss 2.1767050621466297 
Epoch [6/10] Batch 2800/7168 Train_loss 2.1784134338696655 
Epoch [6/10] Batch 2900/7168 Train_loss 2.1790395259487347 
Epoch [6/10] Batch 3000/7168 Train_loss 2.178520353281947 
Epoch [6/10] Batch 3100/7168 Train_loss 2.179523355410738 
Epoch [6/10] Batch 3200/7168 Train_loss 2.1801263114198304 
Epoch [6/10] Batch 3300/7168 Train_loss 2.181423536618346 
Epoch [6/10] Batch 3400/7168 Train_loss 2.1832685904033533 
Epoch [6/10] Batch 3500/7168 Train_loss 2.184238656206119 
Epoch [6/10] Batch 3600/7168 Train_loss 2.182957585678601 
Epoch [6/10] Batch 3700/7168 Train_loss 2.1838754875310142 
Epoch [6/10] Batch 3800/7168 Train_loss 2.182047214634448 
Epoch [6/10] Batch 3900/7168 Train_loss 2.1831532897963886 
Epoch [6/10] Batch 4000/7168 Train_loss 2.1867544697288035 
Epoch [6/10] Batch 4100/7168 Train_loss 2.186296554207366 
Epoch [6/10] Batch 4200/7168 Train_loss 2.1861378469372954 
Epoch [6/10] Batch 4300/7168 Train_loss 2.1861562014000273 
Epoch [6/10] Batch 4400/7168 Train_loss 2.1858784368473736 
Epoch [6/10] Batch 4500/7168 Train_loss 2.185458634210094 
Epoch [6/10] Batch 4600/7168 Train_loss 2.1853479210726983 
Epoch [6/10] Batch 4700/7168 Train_loss 2.1853083090797636 
Epoch [6/10] Batch 4800/7168 Train_loss 2.186093909995329 
Epoch [6/10] Batch 4900/7168 Train_loss 2.1838652991386267 
Epoch [6/10] Batch 5000/7168 Train_loss 2.1838239793585816 
Epoch [6/10] Batch 5100/7168 Train_loss 2.1835575753323027 
Epoch [6/10] Batch 5200/7168 Train_loss 2.18216848671952 
Epoch [6/10] Batch 5300/7168 Train_loss 2.181262212965853 
Epoch [6/10] Batch 5400/7168 Train_loss 2.1793006964881783 
Epoch [6/10] Batch 5500/7168 Train_loss 2.180046917153822 
Epoch [6/10] Batch 5600/7168 Train_loss 2.1795764725094626 
Epoch [6/10] Batch 5700/7168 Train_loss 2.179910623725099 
Epoch [6/10] Batch 5800/7168 Train_loss 2.1802662380541875 
Epoch [6/10] Batch 5900/7168 Train_loss 2.1803541897267857 
Epoch [6/10] Batch 6000/7168 Train_loss 2.1800753839253146 
Epoch [6/10] Batch 6100/7168 Train_loss 2.1793172394539604 
Epoch [6/10] Batch 6200/7168 Train_loss 2.1802026764161546 
Epoch [6/10] Batch 6300/7168 Train_loss 2.1794098017093133 
Epoch [6/10] Batch 6400/7168 Train_loss 2.178226630797779 
Epoch [6/10] Batch 6500/7168 Train_loss 2.179022979388382 
Epoch [6/10] Batch 6600/7168 Train_loss 2.1792602955667455 
Epoch [6/10] Batch 6700/7168 Train_loss 2.1794891041208606 
Epoch [6/10] Batch 6800/7168 Train_loss 2.179736507605444 
Epoch [6/10] Batch 6900/7168 Train_loss 2.1804650321466785 
Epoch [6/10] Batch 7000/7168 Train_loss 2.180999135046478 
Epoch [6/10] Batch 7100/7168 Train_loss 2.1805915555858624 
Epoch: 6/10 	Training Loss: 2.180436 	Validation Loss: 2.167055 Duration seconds: 902.7096874713898 
best_valid_loss_fold [2.1664264701373344] Best_Epoch [6]Epoch [7/10] Batch 0/7168 Train_loss 1.7961552739143372 
Epoch [7/10] Batch 100/7168 Train_loss 2.0656017855842514 
Epoch [7/10] Batch 200/7168 Train_loss 2.1214142523298216 
Epoch [7/10] Batch 300/7168 Train_loss 2.133703326614592 
Epoch [7/10] Batch 400/7168 Train_loss 2.123539994965468 
Epoch [7/10] Batch 500/7168 Train_loss 2.1387932558140594 
Epoch [7/10] Batch 600/7168 Train_loss 2.1458110425267956 
Epoch [7/10] Batch 700/7168 Train_loss 2.1437727680134877 
Epoch [7/10] Batch 800/7168 Train_loss 2.1468616606256044 
Epoch [7/10] Batch 900/7168 Train_loss 2.138545833172465 
Epoch [7/10] Batch 1000/7168 Train_loss 2.1429673672377407 
Epoch [7/10] Batch 1100/7168 Train_loss 2.1514142977985657 
Epoch [7/10] Batch 1200/7168 Train_loss 2.149072190270535 
Epoch [7/10] Batch 1300/7168 Train_loss 2.1494965074865933 
Epoch [7/10] Batch 1400/7168 Train_loss 2.154940725128026 
Epoch [7/10] Batch 1500/7168 Train_loss 2.1513288590726973 
Epoch [7/10] Batch 1600/7168 Train_loss 2.1540175178213317 
Epoch [7/10] Batch 1700/7168 Train_loss 2.1545967168838818 
Epoch [7/10] Batch 1800/7168 Train_loss 2.151376905125555 
Epoch [7/10] Batch 1900/7168 Train_loss 2.1527024403190937 
Epoch [7/10] Batch 2000/7168 Train_loss 2.156192120993096 
Epoch [7/10] Batch 2100/7168 Train_loss 2.1589797766775476 
Epoch [7/10] Batch 2200/7168 Train_loss 2.1615645668889543 
Epoch [7/10] Batch 2300/7168 Train_loss 2.1598777041868873 
Epoch [7/10] Batch 2400/7168 Train_loss 2.1609074274533393 
Epoch [7/10] Batch 2500/7168 Train_loss 2.163225403461443 
Epoch [7/10] Batch 2600/7168 Train_loss 2.1605084008640714 
Epoch [7/10] Batch 2700/7168 Train_loss 2.1589275047607837 
Epoch [7/10] Batch 2800/7168 Train_loss 2.1603482781308587 
Epoch [7/10] Batch 2900/7168 Train_loss 2.1610550499967114 
Epoch [7/10] Batch 3000/7168 Train_loss 2.160520375753116 
Epoch [7/10] Batch 3100/7168 Train_loss 2.1601548113628035 
Epoch [7/10] Batch 3200/7168 Train_loss 2.1629539506858455 
Epoch [7/10] Batch 3300/7168 Train_loss 2.162228009938038 
Epoch [7/10] Batch 3400/7168 Train_loss 2.1631933204770544 
Epoch [7/10] Batch 3500/7168 Train_loss 2.165361708149812 
Epoch [7/10] Batch 3600/7168 Train_loss 2.1675171948694185 
Epoch [7/10] Batch 3700/7168 Train_loss 2.1699350300076135 
Epoch [7/10] Batch 3800/7168 Train_loss 2.170264290170869 
Epoch [7/10] Batch 3900/7168 Train_loss 2.171233384002175 
Epoch [7/10] Batch 4000/7168 Train_loss 2.171270130213813 
Epoch [7/10] Batch 4100/7168 Train_loss 2.1723463952948308 
Epoch [7/10] Batch 4200/7168 Train_loss 2.170684881733588 
Epoch [7/10] Batch 4300/7168 Train_loss 2.173817173526526 
Epoch [7/10] Batch 4400/7168 Train_loss 2.172190593104827 
Epoch [7/10] Batch 4500/7168 Train_loss 2.173844440626028 
Epoch [7/10] Batch 4600/7168 Train_loss 2.173600091225063 
Epoch [7/10] Batch 4700/7168 Train_loss 2.1736862063186013 
Epoch [7/10] Batch 4800/7168 Train_loss 2.173795186571344 
Epoch [7/10] Batch 4900/7168 Train_loss 2.174147507230911 
Epoch [7/10] Batch 5000/7168 Train_loss 2.1720551846891945 
Epoch [7/10] Batch 5100/7168 Train_loss 2.1719938334478535 
Epoch [7/10] Batch 5200/7168 Train_loss 2.172928011373267 
Epoch [7/10] Batch 5300/7168 Train_loss 2.173488287529593 
Epoch [7/10] Batch 5400/7168 Train_loss 2.174357087485019 
Epoch [7/10] Batch 5500/7168 Train_loss 2.173772508912012 
Epoch [7/10] Batch 5600/7168 Train_loss 2.1746977496745226 
Epoch [7/10] Batch 5700/7168 Train_loss 2.175790510145097 
Epoch [7/10] Batch 5800/7168 Train_loss 2.1767129290157343 
Epoch [7/10] Batch 5900/7168 Train_loss 2.1782591040382706 
Epoch [7/10] Batch 6000/7168 Train_loss 2.1781364290381724 
Epoch [7/10] Batch 6100/7168 Train_loss 2.17858913489061 
Epoch [7/10] Batch 6200/7168 Train_loss 2.1779528272125996 
Epoch [7/10] Batch 6300/7168 Train_loss 2.1771070473706073 
Epoch [7/10] Batch 6400/7168 Train_loss 2.177914713119887 
Epoch [7/10] Batch 6500/7168 Train_loss 2.1777592640650343 
Epoch [7/10] Batch 6600/7168 Train_loss 2.177417061104411 
Epoch [7/10] Batch 6700/7168 Train_loss 2.1781554259554414 
Epoch [7/10] Batch 6800/7168 Train_loss 2.179332673199088 
Epoch [7/10] Batch 6900/7168 Train_loss 2.1799208464471005 
Epoch [7/10] Batch 7000/7168 Train_loss 2.1803134920626532 
Epoch [7/10] Batch 7100/7168 Train_loss 2.179965097538486 
Epoch: 7/10 	Training Loss: 2.180544 	Validation Loss: 2.167660 Duration seconds: 1011.5434079170227 
best_valid_loss_fold [2.1664264701373344] Best_Epoch [7]Epoch [8/10] Batch 0/7168 Train_loss 1.4113563299179077 
Epoch [8/10] Batch 100/7168 Train_loss 2.261725646434444 
Epoch [8/10] Batch 200/7168 Train_loss 2.2341073289587725 
Epoch [8/10] Batch 300/7168 Train_loss 2.2225385948867102 
Epoch [8/10] Batch 400/7168 Train_loss 2.2054251705173247 
Epoch [8/10] Batch 500/7168 Train_loss 2.2023018471852036 
Epoch [8/10] Batch 600/7168 Train_loss 2.2029317538571633 
Epoch [8/10] Batch 700/7168 Train_loss 2.1933141416642874 
Epoch [8/10] Batch 800/7168 Train_loss 2.194455163588238 
Epoch [8/10] Batch 900/7168 Train_loss 2.184786524346613 
Epoch [8/10] Batch 1000/7168 Train_loss 2.1834350540325955 
Epoch [8/10] Batch 1100/7168 Train_loss 2.1825625829324196 
Epoch [8/10] Batch 1200/7168 Train_loss 2.180338432449385 
Epoch [8/10] Batch 1300/7168 Train_loss 2.17809328561696 
Epoch [8/10] Batch 1400/7168 Train_loss 2.1828548814955138 
Epoch [8/10] Batch 1500/7168 Train_loss 2.1841271412245518 
Epoch [8/10] Batch 1600/7168 Train_loss 2.1866644407011138 
Epoch [8/10] Batch 1700/7168 Train_loss 2.1896591394491436 
Epoch [8/10] Batch 1800/7168 Train_loss 2.18725386413351 
Epoch [8/10] Batch 1900/7168 Train_loss 2.1876913748450306 
Epoch [8/10] Batch 2000/7168 Train_loss 2.185212355004675 
Epoch [8/10] Batch 2100/7168 Train_loss 2.1837935954162133 
Epoch [8/10] Batch 2200/7168 Train_loss 2.1836819702836396 
Epoch [8/10] Batch 2300/7168 Train_loss 2.1843024627546392 
Epoch [8/10] Batch 2400/7168 Train_loss 2.1822695072890816 
Epoch [8/10] Batch 2500/7168 Train_loss 2.1828092102323327 
Epoch [8/10] Batch 2600/7168 Train_loss 2.1833495199623036 
Epoch [8/10] Batch 2700/7168 Train_loss 2.1847697606421277 
Epoch [8/10] Batch 2800/7168 Train_loss 2.189073895836719 
Epoch [8/10] Batch 2900/7168 Train_loss 2.1914494135479647 
Epoch [8/10] Batch 3000/7168 Train_loss 2.1896387333027407 
Epoch [8/10] Batch 3100/7168 Train_loss 2.19079645222635 
Epoch [8/10] Batch 3200/7168 Train_loss 2.19015510819715 
Epoch [8/10] Batch 3300/7168 Train_loss 2.1887615242263685 
Epoch [8/10] Batch 3400/7168 Train_loss 2.1884715412073854 
Epoch [8/10] Batch 3500/7168 Train_loss 2.1875688438116225 
Epoch [8/10] Batch 3600/7168 Train_loss 2.1867777147092147 
Epoch [8/10] Batch 3700/7168 Train_loss 2.1855580088247386 
Epoch [8/10] Batch 3800/7168 Train_loss 2.1871925392368223 
Epoch [8/10] Batch 3900/7168 Train_loss 2.1832711458534684 
Epoch [8/10] Batch 4000/7168 Train_loss 2.1822490334562943 
Epoch [8/10] Batch 4100/7168 Train_loss 2.183164835622967 
Epoch [8/10] Batch 4200/7168 Train_loss 2.1824782426138487 
Epoch [8/10] Batch 4300/7168 Train_loss 2.183260877734283 
Epoch [8/10] Batch 4400/7168 Train_loss 2.1838557847154 
Epoch [8/10] Batch 4500/7168 Train_loss 2.184610517935153 
Epoch [8/10] Batch 4600/7168 Train_loss 2.1862462148559376 
Epoch [8/10] Batch 4700/7168 Train_loss 2.186690572838635 
Epoch [8/10] Batch 4800/7168 Train_loss 2.1868134078613743 
Epoch [8/10] Batch 4900/7168 Train_loss 2.186576867089591 
Epoch [8/10] Batch 5000/7168 Train_loss 2.187177511041962 
Epoch [8/10] Batch 5100/7168 Train_loss 2.186083638333288 
Epoch [8/10] Batch 5200/7168 Train_loss 2.1857083522799017 
Epoch [8/10] Batch 5300/7168 Train_loss 2.1848534106353705 
Epoch [8/10] Batch 5400/7168 Train_loss 2.1843640110908096 
Epoch [8/10] Batch 5500/7168 Train_loss 2.183201799412637 
Epoch [8/10] Batch 5600/7168 Train_loss 2.181758851443438 
Epoch [8/10] Batch 5700/7168 Train_loss 2.1811321833597486 
Epoch [8/10] Batch 5800/7168 Train_loss 2.179449041870273 
Epoch [8/10] Batch 5900/7168 Train_loss 2.1810919023170205 
Epoch [8/10] Batch 6000/7168 Train_loss 2.1816367078296266 
Epoch [8/10] Batch 6100/7168 Train_loss 2.183046834578457 
Epoch [8/10] Batch 6200/7168 Train_loss 2.18310451023599 
Epoch [8/10] Batch 6300/7168 Train_loss 2.1826344783689993 
Epoch [8/10] Batch 6400/7168 Train_loss 2.1821605853667725 
Epoch [8/10] Batch 6500/7168 Train_loss 2.1826624191255686 
Epoch [8/10] Batch 6600/7168 Train_loss 2.183514114612071 
Epoch [8/10] Batch 6700/7168 Train_loss 2.1828112842640475 
Epoch [8/10] Batch 6800/7168 Train_loss 2.1836275151833386 
Epoch [8/10] Batch 6900/7168 Train_loss 2.183031779003392 
Epoch [8/10] Batch 7000/7168 Train_loss 2.1820856269287767 
Epoch [8/10] Batch 7100/7168 Train_loss 2.180635572820713 
Epoch: 8/10 	Training Loss: 2.180419 	Validation Loss: 2.166904 Duration seconds: 1024.0938367843628 
best_valid_loss_fold [2.1664264701373344] Best_Epoch [8]Epoch [9/10] Batch 0/7168 Train_loss 2.3625893890857697 
Epoch [9/10] Batch 100/7168 Train_loss 2.1416297223013228 
Epoch [9/10] Batch 200/7168 Train_loss 2.1702564166701257 
Epoch [9/10] Batch 300/7168 Train_loss 2.1678314112943666 
Epoch [9/10] Batch 400/7168 Train_loss 2.170392890226217 
Epoch [9/10] Batch 500/7168 Train_loss 2.1768713702341755 
Epoch [9/10] Batch 600/7168 Train_loss 2.1787217259407043 
Epoch [9/10] Batch 700/7168 Train_loss 2.1735529610076747 
Epoch [9/10] Batch 800/7168 Train_loss 2.177083893643039 
Epoch [9/10] Batch 900/7168 Train_loss 2.187330054075816 
Epoch [9/10] Batch 1000/7168 Train_loss 2.1835708260893463 
Epoch [9/10] Batch 1100/7168 Train_loss 2.193992074974876 
Epoch [9/10] Batch 1200/7168 Train_loss 2.1927430886437156 
Epoch [9/10] Batch 1300/7168 Train_loss 2.196722460723125 
Epoch [9/10] Batch 1400/7168 Train_loss 2.1896784340458884 
Epoch [9/10] Batch 1500/7168 Train_loss 2.1934909939388687 
Epoch [9/10] Batch 1600/7168 Train_loss 2.1914472953090067 
Epoch [9/10] Batch 1700/7168 Train_loss 2.197349959483993 
Epoch [9/10] Batch 1800/7168 Train_loss 2.196226377228377 
Epoch [9/10] Batch 1900/7168 Train_loss 2.1931246729241365 
Epoch [9/10] Batch 2000/7168 Train_loss 2.190411765871198 
Epoch [9/10] Batch 2100/7168 Train_loss 2.1828117077617857 
Epoch [9/10] Batch 2200/7168 Train_loss 2.185999642361949 
Epoch [9/10] Batch 2300/7168 Train_loss 2.18785205871346 
Epoch [9/10] Batch 2400/7168 Train_loss 2.1885537169020357 
Epoch [9/10] Batch 2500/7168 Train_loss 2.187541130928696 
Epoch [9/10] Batch 2600/7168 Train_loss 2.185402493587323 
Epoch [9/10] Batch 2700/7168 Train_loss 2.1827309661134886 
Epoch [9/10] Batch 2800/7168 Train_loss 2.183686877889788 
Epoch [9/10] Batch 2900/7168 Train_loss 2.1828927356532755 
Epoch [9/10] Batch 3000/7168 Train_loss 2.181445417683032 
Epoch [9/10] Batch 3100/7168 Train_loss 2.183554287698337 
Epoch [9/10] Batch 3200/7168 Train_loss 2.1836863288243316 
Epoch [9/10] Batch 3300/7168 Train_loss 2.18240372109814 
Epoch [9/10] Batch 3400/7168 Train_loss 2.1820582440016167 
Epoch [9/10] Batch 3500/7168 Train_loss 2.1838308753532467 
Epoch [9/10] Batch 3600/7168 Train_loss 2.181977204765356 
Epoch [9/10] Batch 3700/7168 Train_loss 2.183718653272468 
Epoch [9/10] Batch 3800/7168 Train_loss 2.182758021349971 
Epoch [9/10] Batch 3900/7168 Train_loss 2.1819234246536574 
Epoch [9/10] Batch 4000/7168 Train_loss 2.1827068685055138 
Epoch [9/10] Batch 4100/7168 Train_loss 2.1833946467012755 
Epoch [9/10] Batch 4200/7168 Train_loss 2.1821030767772402 
Epoch [9/10] Batch 4300/7168 Train_loss 2.1812553280066735 
Epoch [9/10] Batch 4400/7168 Train_loss 2.1806111453886983 
Epoch [9/10] Batch 4500/7168 Train_loss 2.181343107379375 
Epoch [9/10] Batch 4600/7168 Train_loss 2.18096114727459 
Epoch [9/10] Batch 4700/7168 Train_loss 2.181986941872975 
Epoch [9/10] Batch 4800/7168 Train_loss 2.1827961349892036 
Epoch [9/10] Batch 4900/7168 Train_loss 2.183396669643992 
Epoch [9/10] Batch 5000/7168 Train_loss 2.181924306090129 
Epoch [9/10] Batch 5100/7168 Train_loss 2.1808422831834893 
Epoch [9/10] Batch 5200/7168 Train_loss 2.1798058080801574 
Epoch [9/10] Batch 5300/7168 Train_loss 2.1802070743069293 
Epoch [9/10] Batch 5400/7168 Train_loss 2.179732410014062 
Epoch [9/10] Batch 5500/7168 Train_loss 2.1783307201346362 
Epoch [9/10] Batch 5600/7168 Train_loss 2.178231698288447 
Epoch [9/10] Batch 5700/7168 Train_loss 2.1789036346953954 
Epoch [9/10] Batch 5800/7168 Train_loss 2.179894295720638 
Epoch [9/10] Batch 5900/7168 Train_loss 2.181093736768395 
Epoch [9/10] Batch 6000/7168 Train_loss 2.180528945350544 
Epoch [9/10] Batch 6100/7168 Train_loss 2.1805942423826083 
Epoch [9/10] Batch 6200/7168 Train_loss 2.179606606262554 
Epoch [9/10] Batch 6300/7168 Train_loss 2.1799775710339167 
Epoch [9/10] Batch 6400/7168 Train_loss 2.1798170974381805 
Epoch [9/10] Batch 6500/7168 Train_loss 2.178971763139247 
Epoch [9/10] Batch 6600/7168 Train_loss 2.17942994103172 
Epoch [9/10] Batch 6700/7168 Train_loss 2.1800184882415654 
Epoch [9/10] Batch 6800/7168 Train_loss 2.1802830554237085 
Epoch [9/10] Batch 6900/7168 Train_loss 2.1797977938520408 
Epoch [9/10] Batch 7000/7168 Train_loss 2.180254863687642 
Epoch [9/10] Batch 7100/7168 Train_loss 2.1810441598172354 
Epoch: 9/10 	Training Loss: 2.180672 	Validation Loss: 2.167811 Duration seconds: 903.114296913147 
best_valid_loss_fold [2.1664264701373344] Best_Epoch [9]Fold: 4/5 
Epoch [0/10] Batch 0/7168 Train_loss 1.8060599267482758 
Epoch [0/10] Batch 100/7168 Train_loss 2.1008800361121054 
Epoch [0/10] Batch 200/7168 Train_loss 2.1321035532364205 
Epoch [0/10] Batch 300/7168 Train_loss 2.153798778637303 
Epoch [0/10] Batch 400/7168 Train_loss 2.1605500742384325 
Epoch [0/10] Batch 500/7168 Train_loss 2.1390726586837254 
Epoch [0/10] Batch 600/7168 Train_loss 2.152172964865872 
Epoch [0/10] Batch 700/7168 Train_loss 2.1394333665321623 
Epoch [0/10] Batch 800/7168 Train_loss 2.151900963688016 
Epoch [0/10] Batch 900/7168 Train_loss 2.1443331281299995 
Epoch [0/10] Batch 1000/7168 Train_loss 2.1516632427136737 
Epoch [0/10] Batch 1100/7168 Train_loss 2.1497092382486684 
Epoch [0/10] Batch 1200/7168 Train_loss 2.1570373641685285 
Epoch [0/10] Batch 1300/7168 Train_loss 2.162582470099803 
Epoch [0/10] Batch 1400/7168 Train_loss 2.1630935956970614 
Epoch [0/10] Batch 1500/7168 Train_loss 2.1654083164809625 
Epoch [0/10] Batch 1600/7168 Train_loss 2.1614231330241953 
Epoch [0/10] Batch 1700/7168 Train_loss 2.16918261230062 
Epoch [0/10] Batch 1800/7168 Train_loss 2.167289950065253 
Epoch [0/10] Batch 1900/7168 Train_loss 2.1667618645077815 
Epoch [0/10] Batch 2000/7168 Train_loss 2.1655193798947727 
Epoch [0/10] Batch 2100/7168 Train_loss 2.1650591401869996 
Epoch [0/10] Batch 2200/7168 Train_loss 2.1672658509541405 
Epoch [0/10] Batch 2300/7168 Train_loss 2.16554037637293 
Epoch [0/10] Batch 2400/7168 Train_loss 2.164679812806589 
Epoch [0/10] Batch 2500/7168 Train_loss 2.1655874332634273 
Epoch [0/10] Batch 2600/7168 Train_loss 2.1668956893389155 
Epoch [0/10] Batch 2700/7168 Train_loss 2.1697390202249434 
Epoch [0/10] Batch 2800/7168 Train_loss 2.1699235924000573 
Epoch [0/10] Batch 2900/7168 Train_loss 2.170573337987882 
Epoch [0/10] Batch 3000/7168 Train_loss 2.1668429264065745 
Epoch [0/10] Batch 3100/7168 Train_loss 2.1678835592061154 
Epoch [0/10] Batch 3200/7168 Train_loss 2.1673131187384445 
Epoch [0/10] Batch 3300/7168 Train_loss 2.1672796845995848 
Epoch [0/10] Batch 3400/7168 Train_loss 2.169490430270774 
Epoch [0/10] Batch 3500/7168 Train_loss 2.1737699020176198 
Epoch [0/10] Batch 3600/7168 Train_loss 2.1722681485830297 
Epoch [0/10] Batch 3700/7168 Train_loss 2.175923428042487 
Epoch [0/10] Batch 3800/7168 Train_loss 2.1744626830068583 
Epoch [0/10] Batch 3900/7168 Train_loss 2.1753180539820383 
Epoch [0/10] Batch 4000/7168 Train_loss 2.176583992823366 
Epoch [0/10] Batch 4100/7168 Train_loss 2.1755317511376275 
Epoch [0/10] Batch 4200/7168 Train_loss 2.1748508942344307 
Epoch [0/10] Batch 4300/7168 Train_loss 2.1759011536850483 
Epoch [0/10] Batch 4400/7168 Train_loss 2.1777666989740148 
Epoch [0/10] Batch 4500/7168 Train_loss 2.176774350143729 
Epoch [0/10] Batch 4600/7168 Train_loss 2.1758924836932096 
Epoch [0/10] Batch 4700/7168 Train_loss 2.1744611098731585 
Epoch [0/10] Batch 4800/7168 Train_loss 2.1747488769031618 
Epoch [0/10] Batch 4900/7168 Train_loss 2.1748634498475936 
Epoch [0/10] Batch 5000/7168 Train_loss 2.1742337123468336 
Epoch [0/10] Batch 5100/7168 Train_loss 2.174794209883321 
Epoch [0/10] Batch 5200/7168 Train_loss 2.1733209862803706 
Epoch [0/10] Batch 5300/7168 Train_loss 2.1725824999401104 
Epoch [0/10] Batch 5400/7168 Train_loss 2.1731059143546996 
Epoch [0/10] Batch 5500/7168 Train_loss 2.174274601344303 
Epoch [0/10] Batch 5600/7168 Train_loss 2.1742386719681837 
Epoch [0/10] Batch 5700/7168 Train_loss 2.175207069306034 
Epoch [0/10] Batch 5800/7168 Train_loss 2.176728033375748 
Epoch [0/10] Batch 5900/7168 Train_loss 2.1778987920162054 
Epoch [0/10] Batch 6000/7168 Train_loss 2.1778913119885748 
Epoch [0/10] Batch 6100/7168 Train_loss 2.177880964544323 
Epoch [0/10] Batch 6200/7168 Train_loss 2.176707971254158 
Epoch [0/10] Batch 6300/7168 Train_loss 2.17686653624374 
Epoch [0/10] Batch 6400/7168 Train_loss 2.177823759269722 
Epoch [0/10] Batch 6500/7168 Train_loss 2.179119144516933 
Epoch [0/10] Batch 6600/7168 Train_loss 2.1791227151725963 
Epoch [0/10] Batch 6700/7168 Train_loss 2.1799201964820574 
Epoch [0/10] Batch 6800/7168 Train_loss 2.1813233800362775 
Epoch [0/10] Batch 6900/7168 Train_loss 2.1808739891930293 
Epoch [0/10] Batch 7000/7168 Train_loss 2.1792968404393522 
Epoch [0/10] Batch 7100/7168 Train_loss 2.178719215438141 
Epoch: 0/10 	Training Loss: 2.177075 	Validation Loss: 2.182396 Duration seconds: 925.1281650066376 
Validation loss decreased (inf --> 2.182396).  Saving model ... 
best_valid_loss_fold [2.1823960527794304] Best_Epoch [0]Epoch [1/10] Batch 0/7168 Train_loss 1.4575152099132538 
Epoch [1/10] Batch 100/7168 Train_loss 2.2277262482312645 
Epoch [1/10] Batch 200/7168 Train_loss 2.2072154691266777 
Epoch [1/10] Batch 300/7168 Train_loss 2.222619812155879 
Epoch [1/10] Batch 400/7168 Train_loss 2.1991050735747724 
Epoch [1/10] Batch 500/7168 Train_loss 2.1975373424693734 
Epoch [1/10] Batch 600/7168 Train_loss 2.1911190152664153 
Epoch [1/10] Batch 700/7168 Train_loss 2.186241928015048 
Epoch [1/10] Batch 800/7168 Train_loss 2.193608641643054 
Epoch [1/10] Batch 900/7168 Train_loss 2.199451753437585 
Epoch [1/10] Batch 1000/7168 Train_loss 2.2006667770080632 
Epoch [1/10] Batch 1100/7168 Train_loss 2.2047060670552745 
Epoch [1/10] Batch 1200/7168 Train_loss 2.2024243137644888 
Epoch [1/10] Batch 1300/7168 Train_loss 2.198236269345199 
Epoch [1/10] Batch 1400/7168 Train_loss 2.1905063620790424 
Epoch [1/10] Batch 1500/7168 Train_loss 2.185613580946522 
Epoch [1/10] Batch 1600/7168 Train_loss 2.1860135099539377 
Epoch [1/10] Batch 1700/7168 Train_loss 2.1839055188574 
Epoch [1/10] Batch 1800/7168 Train_loss 2.1827443936495303 
Epoch [1/10] Batch 1900/7168 Train_loss 2.1770100286137364 
Epoch [1/10] Batch 2000/7168 Train_loss 2.1804585604966134 
Epoch [1/10] Batch 2100/7168 Train_loss 2.1821974063277074 
Epoch [1/10] Batch 2200/7168 Train_loss 2.1863475673811266 
Epoch [1/10] Batch 2300/7168 Train_loss 2.185975444097977 
Epoch [1/10] Batch 2400/7168 Train_loss 2.184756693896876 
Epoch [1/10] Batch 2500/7168 Train_loss 2.1811291632927308 
Epoch [1/10] Batch 2600/7168 Train_loss 2.179215668239808 
Epoch [1/10] Batch 2700/7168 Train_loss 2.1795881529928622 
Epoch [1/10] Batch 2800/7168 Train_loss 2.1780993404802533 
Epoch [1/10] Batch 2900/7168 Train_loss 2.1784039934013926 
Epoch [1/10] Batch 3000/7168 Train_loss 2.178832337617755 
Epoch [1/10] Batch 3100/7168 Train_loss 2.179395665673816 
Epoch [1/10] Batch 3200/7168 Train_loss 2.178863195777498 
Epoch [1/10] Batch 3300/7168 Train_loss 2.181899375904332 
Epoch [1/10] Batch 3400/7168 Train_loss 2.1812053216893474 
Epoch [1/10] Batch 3500/7168 Train_loss 2.18018310776066 
Epoch [1/10] Batch 3600/7168 Train_loss 2.180766152180224 
Epoch [1/10] Batch 3700/7168 Train_loss 2.178451192379803 
Epoch [1/10] Batch 3800/7168 Train_loss 2.176597530952851 
Epoch [1/10] Batch 3900/7168 Train_loss 2.1764088606175873 
Epoch [1/10] Batch 4000/7168 Train_loss 2.176144004363443 
Epoch [1/10] Batch 4100/7168 Train_loss 2.1748731826186556 
Epoch [1/10] Batch 4200/7168 Train_loss 2.1778406989706385 
Epoch [1/10] Batch 4300/7168 Train_loss 2.1764801550371597 
Epoch [1/10] Batch 4400/7168 Train_loss 2.176428813012425 
Epoch [1/10] Batch 4500/7168 Train_loss 2.175733672975964 
Epoch [1/10] Batch 4600/7168 Train_loss 2.176507856325206 
Epoch [1/10] Batch 4700/7168 Train_loss 2.1751918520249856 
Epoch [1/10] Batch 4800/7168 Train_loss 2.1730220177550534 
Epoch [1/10] Batch 4900/7168 Train_loss 2.1734365106078952 
Epoch [1/10] Batch 5000/7168 Train_loss 2.173197761511736 
Epoch [1/10] Batch 5100/7168 Train_loss 2.172651263229568 
Epoch [1/10] Batch 5200/7168 Train_loss 2.1733415880453766 
Epoch [1/10] Batch 5300/7168 Train_loss 2.173484465034587 
Epoch [1/10] Batch 5400/7168 Train_loss 2.170721566198689 
Epoch [1/10] Batch 5500/7168 Train_loss 2.169772993364869 
Epoch [1/10] Batch 5600/7168 Train_loss 2.1701553321335916 
Epoch [1/10] Batch 5700/7168 Train_loss 2.1700630114175126 
Epoch [1/10] Batch 5800/7168 Train_loss 2.1716104171447066 
Epoch [1/10] Batch 5900/7168 Train_loss 2.1724814276935853 
Epoch [1/10] Batch 6000/7168 Train_loss 2.1732162946418927 
Epoch [1/10] Batch 6100/7168 Train_loss 2.174362532966666 
Epoch [1/10] Batch 6200/7168 Train_loss 2.174659928397351 
Epoch [1/10] Batch 6300/7168 Train_loss 2.1744776032823663 
Epoch [1/10] Batch 6400/7168 Train_loss 2.1748575888873263 
Epoch [1/10] Batch 6500/7168 Train_loss 2.1752930286549144 
Epoch [1/10] Batch 6600/7168 Train_loss 2.176228452938341 
Epoch [1/10] Batch 6700/7168 Train_loss 2.1768802884505267 
Epoch [1/10] Batch 6800/7168 Train_loss 2.1764600874549096 
Epoch [1/10] Batch 6900/7168 Train_loss 2.1748039393066723 
Epoch [1/10] Batch 7000/7168 Train_loss 2.175727355413787 
Epoch [1/10] Batch 7100/7168 Train_loss 2.176082890059491 
Epoch: 1/10 	Training Loss: 2.176988 	Validation Loss: 2.182641 Duration seconds: 873.9306828975677 
best_valid_loss_fold [2.1823960527794304] Best_Epoch [1]Epoch [2/10] Batch 0/7168 Train_loss 2.9047805070877075 
Epoch [2/10] Batch 100/7168 Train_loss 2.2092346625752968 
Epoch [2/10] Batch 200/7168 Train_loss 2.2174030523839874 
Epoch [2/10] Batch 300/7168 Train_loss 2.190615419682078 
Epoch [2/10] Batch 400/7168 Train_loss 2.19910454515953 
Epoch [2/10] Batch 500/7168 Train_loss 2.181617873990369 
Epoch [2/10] Batch 600/7168 Train_loss 2.172880775842214 
Epoch [2/10] Batch 700/7168 Train_loss 2.185610273000007 
Epoch [2/10] Batch 800/7168 Train_loss 2.183249458652013 
Epoch [2/10] Batch 900/7168 Train_loss 2.185447759074588 
Epoch [2/10] Batch 1000/7168 Train_loss 2.171413366953572 
Epoch [2/10] Batch 1100/7168 Train_loss 2.178665781757379 
Epoch [2/10] Batch 1200/7168 Train_loss 2.1729718502316246 
Epoch [2/10] Batch 1300/7168 Train_loss 2.1726530281788197 
Epoch [2/10] Batch 1400/7168 Train_loss 2.172056995412012 
Epoch [2/10] Batch 1500/7168 Train_loss 2.174544730419163 
Epoch [2/10] Batch 1600/7168 Train_loss 2.1717127299007215 
Epoch [2/10] Batch 1700/7168 Train_loss 2.168813429067655 
Epoch [2/10] Batch 1800/7168 Train_loss 2.1677908072582155 
Epoch [2/10] Batch 1900/7168 Train_loss 2.1698123638737146 
Epoch [2/10] Batch 2000/7168 Train_loss 2.1665057729462394 
Epoch [2/10] Batch 2100/7168 Train_loss 2.164510139386067 
Epoch [2/10] Batch 2200/7168 Train_loss 2.1602874710607507 
Epoch [2/10] Batch 2300/7168 Train_loss 2.1586463180755544 
Epoch [2/10] Batch 2400/7168 Train_loss 2.1601503541610083 
Epoch [2/10] Batch 2500/7168 Train_loss 2.1619570328861846 
Epoch [2/10] Batch 2600/7168 Train_loss 2.1608032229449923 
Epoch [2/10] Batch 2700/7168 Train_loss 2.160545579098808 
Epoch [2/10] Batch 2800/7168 Train_loss 2.165194776538269 
Epoch [2/10] Batch 2900/7168 Train_loss 2.165098856827671 
Epoch [2/10] Batch 3000/7168 Train_loss 2.166593696987855 
Epoch [2/10] Batch 3100/7168 Train_loss 2.1690634856230595 
Epoch [2/10] Batch 3200/7168 Train_loss 2.174818642979784 
Epoch [2/10] Batch 3300/7168 Train_loss 2.1755897255105996 
Epoch [2/10] Batch 3400/7168 Train_loss 2.1732396414457997 
Epoch [2/10] Batch 3500/7168 Train_loss 2.17175749228175 
Epoch [2/10] Batch 3600/7168 Train_loss 2.1698900825511878 
Epoch [2/10] Batch 3700/7168 Train_loss 2.1680336516230727 
Epoch [2/10] Batch 3800/7168 Train_loss 2.168147511303942 
Epoch [2/10] Batch 3900/7168 Train_loss 2.1693600646786675 
Epoch [2/10] Batch 4000/7168 Train_loss 2.167517570082738 
Epoch [2/10] Batch 4100/7168 Train_loss 2.1678502957970887 
Epoch [2/10] Batch 4200/7168 Train_loss 2.1672526870603024 
Epoch [2/10] Batch 4300/7168 Train_loss 2.1667491989173437 
Epoch [2/10] Batch 4400/7168 Train_loss 2.1678080590908646 
Epoch [2/10] Batch 4500/7168 Train_loss 2.1697720850349613 
Epoch [2/10] Batch 4600/7168 Train_loss 2.168527970273089 
Epoch [2/10] Batch 4700/7168 Train_loss 2.168605506001724 
Epoch [2/10] Batch 4800/7168 Train_loss 2.1694297119245705 
Epoch [2/10] Batch 4900/7168 Train_loss 2.1689312904161038 
Epoch [2/10] Batch 5000/7168 Train_loss 2.1678736082013192 
Epoch [2/10] Batch 5100/7168 Train_loss 2.1677709871403166 
Epoch [2/10] Batch 5200/7168 Train_loss 2.1683139528042763 
Epoch [2/10] Batch 5300/7168 Train_loss 2.1704568414812018 
Epoch [2/10] Batch 5400/7168 Train_loss 2.1716932315083484 
Epoch [2/10] Batch 5500/7168 Train_loss 2.1712913243223637 
Epoch [2/10] Batch 5600/7168 Train_loss 2.171365536058977 
Epoch [2/10] Batch 5700/7168 Train_loss 2.1721593073623637 
Epoch [2/10] Batch 5800/7168 Train_loss 2.17026965421318 
Epoch [2/10] Batch 5900/7168 Train_loss 2.1708428218438045 
Epoch [2/10] Batch 6000/7168 Train_loss 2.170535162093738 
Epoch [2/10] Batch 6100/7168 Train_loss 2.170679624809279 
Epoch [2/10] Batch 6200/7168 Train_loss 2.1722019131679415 
Epoch [2/10] Batch 6300/7168 Train_loss 2.172944899592262 
Epoch [2/10] Batch 6400/7168 Train_loss 2.1725978366850205 
Epoch [2/10] Batch 6500/7168 Train_loss 2.1735694148695077 
Epoch [2/10] Batch 6600/7168 Train_loss 2.1742049678471607 
Epoch [2/10] Batch 6700/7168 Train_loss 2.1739779347035446 
Epoch [2/10] Batch 6800/7168 Train_loss 2.174930781785521 
Epoch [2/10] Batch 6900/7168 Train_loss 2.1748080636527014 
Epoch [2/10] Batch 7000/7168 Train_loss 2.1755144727232625 
Epoch [2/10] Batch 7100/7168 Train_loss 2.176591391366832 
Epoch: 2/10 	Training Loss: 2.176972 	Validation Loss: 2.183047 Duration seconds: 876.095691204071 
best_valid_loss_fold [2.1823960527794304] Best_Epoch [2]Epoch [3/10] Batch 0/7168 Train_loss 1.0849857330322266 
Epoch [3/10] Batch 100/7168 Train_loss 2.194183073421516 
Epoch [3/10] Batch 200/7168 Train_loss 2.203495689292452 
Epoch [3/10] Batch 300/7168 Train_loss 2.190157173331394 
Epoch [3/10] Batch 400/7168 Train_loss 2.1776742683235844 
Epoch [3/10] Batch 500/7168 Train_loss 2.1789988774263453 
Epoch [3/10] Batch 600/7168 Train_loss 2.1746493795400053 
Epoch [3/10] Batch 700/7168 Train_loss 2.175240727638382 
Epoch [3/10] Batch 800/7168 Train_loss 2.1598556740081563 
Epoch [3/10] Batch 900/7168 Train_loss 2.1584762259077417 
Epoch [3/10] Batch 1000/7168 Train_loss 2.1561903851134674 
Epoch [3/10] Batch 1100/7168 Train_loss 2.1563614123956816 
Epoch [3/10] Batch 1200/7168 Train_loss 2.1618704018892596 
Epoch [3/10] Batch 1300/7168 Train_loss 2.1582530082692375 
Epoch [3/10] Batch 1400/7168 Train_loss 2.164311479939298 
Epoch [3/10] Batch 1500/7168 Train_loss 2.158658542706917 
Epoch [3/10] Batch 1600/7168 Train_loss 2.157992790967952 
Epoch [3/10] Batch 1700/7168 Train_loss 2.161416641049424 
Epoch [3/10] Batch 1800/7168 Train_loss 2.1593798506283086 
Epoch [3/10] Batch 1900/7168 Train_loss 2.1617780217429576 
Epoch [3/10] Batch 2000/7168 Train_loss 2.1690870798390725 
Epoch [3/10] Batch 2100/7168 Train_loss 2.16846099599437 
Epoch [3/10] Batch 2200/7168 Train_loss 2.1664997108994912 
Epoch [3/10] Batch 2300/7168 Train_loss 2.168140178016091 
Epoch [3/10] Batch 2400/7168 Train_loss 2.1692001834815864 
Epoch [3/10] Batch 2500/7168 Train_loss 2.1723862425213762 
Epoch [3/10] Batch 2600/7168 Train_loss 2.1720636525448813 
Epoch [3/10] Batch 2700/7168 Train_loss 2.1724035484686643 
Epoch [3/10] Batch 2800/7168 Train_loss 2.17272144604989 
Epoch [3/10] Batch 2900/7168 Train_loss 2.176223099360134 
Epoch [3/10] Batch 3000/7168 Train_loss 2.177658851659723 
Epoch [3/10] Batch 3100/7168 Train_loss 2.174814805623909 
Epoch [3/10] Batch 3200/7168 Train_loss 2.173914666270435 
Epoch [3/10] Batch 3300/7168 Train_loss 2.1738905128981987 
Epoch [3/10] Batch 3400/7168 Train_loss 2.1755182394312604 
Epoch [3/10] Batch 3500/7168 Train_loss 2.1774867843114047 
Epoch [3/10] Batch 3600/7168 Train_loss 2.178080230185768 
Epoch [3/10] Batch 3700/7168 Train_loss 2.1764108573857337 
Epoch [3/10] Batch 3800/7168 Train_loss 2.179125840335857 
Epoch [3/10] Batch 3900/7168 Train_loss 2.178220955707024 
Epoch [3/10] Batch 4000/7168 Train_loss 2.177015791845691 
Epoch [3/10] Batch 4100/7168 Train_loss 2.1784623141653277 
Epoch [3/10] Batch 4200/7168 Train_loss 2.176791738450059 
Epoch [3/10] Batch 4300/7168 Train_loss 2.1782008918828115 
Epoch [3/10] Batch 4400/7168 Train_loss 2.179984365101295 
Epoch [3/10] Batch 4500/7168 Train_loss 2.1792840751554667 
Epoch [3/10] Batch 4600/7168 Train_loss 2.1817776396420436 
Epoch [3/10] Batch 4700/7168 Train_loss 2.1830718611699678 
Epoch [3/10] Batch 4800/7168 Train_loss 2.182783577820083 
Epoch [3/10] Batch 4900/7168 Train_loss 2.182658006131369 
Epoch [3/10] Batch 5000/7168 Train_loss 2.1822053077095296 
Epoch [3/10] Batch 5100/7168 Train_loss 2.181506625368791 
Epoch [3/10] Batch 5200/7168 Train_loss 2.1825014222094565 
Epoch [3/10] Batch 5300/7168 Train_loss 2.182599151852423 
Epoch [3/10] Batch 5400/7168 Train_loss 2.1830150917644215 
Epoch [3/10] Batch 5500/7168 Train_loss 2.183105727175846 
Epoch [3/10] Batch 5600/7168 Train_loss 2.182848301902879 
Epoch [3/10] Batch 5700/7168 Train_loss 2.1821884222562598 
Epoch [3/10] Batch 5800/7168 Train_loss 2.1814129942245595 
Epoch [3/10] Batch 5900/7168 Train_loss 2.180204967829438 
Epoch [3/10] Batch 6000/7168 Train_loss 2.1783198303623887 
Epoch [3/10] Batch 6100/7168 Train_loss 2.1788331847457765 
Epoch [3/10] Batch 6200/7168 Train_loss 2.1784382260743502 
Epoch [3/10] Batch 6300/7168 Train_loss 2.179268782213744 
Epoch [3/10] Batch 6400/7168 Train_loss 2.1788833579003413 
Epoch [3/10] Batch 6500/7168 Train_loss 2.1801830009439693 
Epoch [3/10] Batch 6600/7168 Train_loss 2.1793628180501967 
Epoch [3/10] Batch 6700/7168 Train_loss 2.178626820453956 
Epoch [3/10] Batch 6800/7168 Train_loss 2.179067267775711 
Epoch [3/10] Batch 6900/7168 Train_loss 2.178481171168619 
Epoch [3/10] Batch 7000/7168 Train_loss 2.1778624417432426 
Epoch [3/10] Batch 7100/7168 Train_loss 2.177415159821443 
Epoch: 3/10 	Training Loss: 2.177007 	Validation Loss: 2.180101 Duration seconds: 877.6907057762146 
Validation loss decreased (2.182396 --> 2.180101).  Saving model ... 
best_valid_loss_fold [2.180100522577829] Best_Epoch [3]Epoch [4/10] Batch 0/7168 Train_loss 1.681446135044098 
Epoch [4/10] Batch 100/7168 Train_loss 2.151708563070486 
Epoch [4/10] Batch 200/7168 Train_loss 2.172237690260161 
Epoch [4/10] Batch 300/7168 Train_loss 2.1895916302041756 
Epoch [4/10] Batch 400/7168 Train_loss 2.169132051139401 
Epoch [4/10] Batch 500/7168 Train_loss 2.161623383175113 
Epoch [4/10] Batch 600/7168 Train_loss 2.1728582788079036 
Epoch [4/10] Batch 700/7168 Train_loss 2.1748861443044114 
Epoch [4/10] Batch 800/7168 Train_loss 2.179946138636301 
Epoch [4/10] Batch 900/7168 Train_loss 2.184554157607034 
Epoch [4/10] Batch 1000/7168 Train_loss 2.1903435199648946 
Epoch [4/10] Batch 1100/7168 Train_loss 2.1910132795274526 
Epoch [4/10] Batch 1200/7168 Train_loss 2.1938439588885226 
Epoch [4/10] Batch 1300/7168 Train_loss 2.196085309805914 
Epoch [4/10] Batch 1400/7168 Train_loss 2.1960051896464052 
Epoch [4/10] Batch 1500/7168 Train_loss 2.1958660554738936 
Epoch [4/10] Batch 1600/7168 Train_loss 2.195234171334168 
Epoch [4/10] Batch 1700/7168 Train_loss 2.1959702899216342 
Epoch [4/10] Batch 1800/7168 Train_loss 2.197640000541034 
Epoch [4/10] Batch 1900/7168 Train_loss 2.1933369368299065 
Epoch [4/10] Batch 2000/7168 Train_loss 2.1967782179216693 
Epoch [4/10] Batch 2100/7168 Train_loss 2.1965581408104744 
Epoch [4/10] Batch 2200/7168 Train_loss 2.1943955571948806 
Epoch [4/10] Batch 2300/7168 Train_loss 2.1982732648009686 
Epoch [4/10] Batch 2400/7168 Train_loss 2.1978739567135137 
Epoch [4/10] Batch 2500/7168 Train_loss 2.1968658319691188 
Epoch [4/10] Batch 2600/7168 Train_loss 2.191851217097752 
Epoch [4/10] Batch 2700/7168 Train_loss 2.1932732174017833 
Epoch [4/10] Batch 2800/7168 Train_loss 2.1913900719513344 
Epoch [4/10] Batch 2900/7168 Train_loss 2.18959338855164 
Epoch [4/10] Batch 3000/7168 Train_loss 2.18696980886422 
Epoch [4/10] Batch 3100/7168 Train_loss 2.185831896093383 
Epoch [4/10] Batch 3200/7168 Train_loss 2.1862340517404935 
Epoch [4/10] Batch 3300/7168 Train_loss 2.1854613751615406 
Epoch [4/10] Batch 3400/7168 Train_loss 2.185040459812336 
Epoch [4/10] Batch 3500/7168 Train_loss 2.181467009600283 
Epoch [4/10] Batch 3600/7168 Train_loss 2.1804855900468247 
Epoch [4/10] Batch 3700/7168 Train_loss 2.1796671314413114 
Epoch [4/10] Batch 3800/7168 Train_loss 2.1799991291797465 
Epoch [4/10] Batch 3900/7168 Train_loss 2.1827771872381345 
Epoch [4/10] Batch 4000/7168 Train_loss 2.1829699612809432 
Epoch [4/10] Batch 4100/7168 Train_loss 2.183358769875653 
Epoch [4/10] Batch 4200/7168 Train_loss 2.182574383958809 
Epoch [4/10] Batch 4300/7168 Train_loss 2.182116873061704 
Epoch [4/10] Batch 4400/7168 Train_loss 2.182550552256458 
Epoch [4/10] Batch 4500/7168 Train_loss 2.182489004556377 
Epoch [4/10] Batch 4600/7168 Train_loss 2.1824093655130703 
Epoch [4/10] Batch 4700/7168 Train_loss 2.1814899489210657 
Epoch [4/10] Batch 4800/7168 Train_loss 2.182496883280822 
Epoch [4/10] Batch 4900/7168 Train_loss 2.1820430488452405 
Epoch [4/10] Batch 5000/7168 Train_loss 2.1823893786162003 
Epoch [4/10] Batch 5100/7168 Train_loss 2.183949895709946 
Epoch [4/10] Batch 5200/7168 Train_loss 2.1842423661858548 
Epoch [4/10] Batch 5300/7168 Train_loss 2.184317591948861 
Epoch [4/10] Batch 5400/7168 Train_loss 2.1828395717150855 
Epoch [4/10] Batch 5500/7168 Train_loss 2.182212345921068 
Epoch [4/10] Batch 5600/7168 Train_loss 2.1829892556340837 
Epoch [4/10] Batch 5700/7168 Train_loss 2.1823660463586103 
Epoch [4/10] Batch 5800/7168 Train_loss 2.184158989225579 
Epoch [4/10] Batch 5900/7168 Train_loss 2.1826770213312505 
Epoch [4/10] Batch 6000/7168 Train_loss 2.1831055094531884 
Epoch [4/10] Batch 6100/7168 Train_loss 2.1824805097435562 
Epoch [4/10] Batch 6200/7168 Train_loss 2.182135871078214 
Epoch [4/10] Batch 6300/7168 Train_loss 2.181549466705345 
Epoch [4/10] Batch 6400/7168 Train_loss 2.180032622051079 
Epoch [4/10] Batch 6500/7168 Train_loss 2.17905353637287 
Epoch [4/10] Batch 6600/7168 Train_loss 2.17877415920131 
Epoch [4/10] Batch 6700/7168 Train_loss 2.176715184918732 
Epoch [4/10] Batch 6800/7168 Train_loss 2.1771710425935202 
Epoch [4/10] Batch 6900/7168 Train_loss 2.1754014442155647 
Epoch [4/10] Batch 7000/7168 Train_loss 2.1766405678735326 
Epoch [4/10] Batch 7100/7168 Train_loss 2.1771181513076767 
Epoch: 4/10 	Training Loss: 2.177066 	Validation Loss: 2.180020 Duration seconds: 924.8351681232452 
Validation loss decreased (2.180101 --> 2.180020).  Saving model ... 
best_valid_loss_fold [2.1800200242183303] Best_Epoch [4]Epoch [5/10] Batch 0/7168 Train_loss 1.823660284280777 
Epoch [5/10] Batch 100/7168 Train_loss 2.0855646410791002 
Epoch [5/10] Batch 200/7168 Train_loss 2.068283935239659 
Epoch [5/10] Batch 300/7168 Train_loss 2.0993530564628964 
Epoch [5/10] Batch 400/7168 Train_loss 2.1277684456392416 
Epoch [5/10] Batch 500/7168 Train_loss 2.1433611273646593 
Epoch [5/10] Batch 600/7168 Train_loss 2.152469886990633 
Epoch [5/10] Batch 700/7168 Train_loss 2.15231531824433 
Epoch [5/10] Batch 800/7168 Train_loss 2.155162946003653 
Epoch [5/10] Batch 900/7168 Train_loss 2.156429527271601 
Epoch [5/10] Batch 1000/7168 Train_loss 2.162972598836258 
Epoch [5/10] Batch 1100/7168 Train_loss 2.168054244701804 
Epoch [5/10] Batch 1200/7168 Train_loss 2.1726790508511264 
Epoch [5/10] Batch 1300/7168 Train_loss 2.1740620243695954 
Epoch [5/10] Batch 1400/7168 Train_loss 2.165023388977905 
Epoch [5/10] Batch 1500/7168 Train_loss 2.1620674461602847 
Epoch [5/10] Batch 1600/7168 Train_loss 2.1605160121784444 
Epoch [5/10] Batch 1700/7168 Train_loss 2.153833999327531 
Epoch [5/10] Batch 1800/7168 Train_loss 2.155141429660189 
Epoch [5/10] Batch 1900/7168 Train_loss 2.1547570416232524 
Epoch [5/10] Batch 2000/7168 Train_loss 2.1588082719912594 
Epoch [5/10] Batch 2100/7168 Train_loss 2.1572916484883375 
Epoch [5/10] Batch 2200/7168 Train_loss 2.159148902784202 
Epoch [5/10] Batch 2300/7168 Train_loss 2.1577490232512515 
Epoch [5/10] Batch 2400/7168 Train_loss 2.156249222619888 
Epoch [5/10] Batch 2500/7168 Train_loss 2.161258494780093 
Epoch [5/10] Batch 2600/7168 Train_loss 2.165551365498899 
Epoch [5/10] Batch 2700/7168 Train_loss 2.1651394999565703 
Epoch [5/10] Batch 2800/7168 Train_loss 2.165196362466567 
Epoch [5/10] Batch 2900/7168 Train_loss 2.164819639090093 
Epoch [5/10] Batch 3000/7168 Train_loss 2.1652393314400262 
Epoch [5/10] Batch 3100/7168 Train_loss 2.167376146700758 
Epoch [5/10] Batch 3200/7168 Train_loss 2.167742131897637 
Epoch [5/10] Batch 3300/7168 Train_loss 2.168096562035515 
Epoch [5/10] Batch 3400/7168 Train_loss 2.1686152554774627 
Epoch [5/10] Batch 3500/7168 Train_loss 2.168006561324107 
Epoch [5/10] Batch 3600/7168 Train_loss 2.168513292431732 
Epoch [5/10] Batch 3700/7168 Train_loss 2.169926736329704 
Epoch [5/10] Batch 3800/7168 Train_loss 2.1686836997754195 
Epoch [5/10] Batch 3900/7168 Train_loss 2.1669728265179025 
Epoch [5/10] Batch 4000/7168 Train_loss 2.166937299499182 
Epoch [5/10] Batch 4100/7168 Train_loss 2.168477163521786 
Epoch [5/10] Batch 4200/7168 Train_loss 2.169810267991323 
Epoch [5/10] Batch 4300/7168 Train_loss 2.170203784660416 
Epoch [5/10] Batch 4400/7168 Train_loss 2.1708221883902197 
Epoch [5/10] Batch 4500/7168 Train_loss 2.169424089404695 
Epoch [5/10] Batch 4600/7168 Train_loss 2.1719660761044923 
Epoch [5/10] Batch 4700/7168 Train_loss 2.1741546209132108 
Epoch [5/10] Batch 4800/7168 Train_loss 2.17316988260454 
Epoch [5/10] Batch 4900/7168 Train_loss 2.1726894789120537 
Epoch [5/10] Batch 5000/7168 Train_loss 2.1739894721990014 
Epoch [5/10] Batch 5100/7168 Train_loss 2.175110782379965 
Epoch [5/10] Batch 5200/7168 Train_loss 2.176566707631804 
Epoch [5/10] Batch 5300/7168 Train_loss 2.1779699967090105 
Epoch [5/10] Batch 5400/7168 Train_loss 2.1783589538330945 
Epoch [5/10] Batch 5500/7168 Train_loss 2.1773353030363705 
Epoch [5/10] Batch 5600/7168 Train_loss 2.1765601110339188 
Epoch [5/10] Batch 5700/7168 Train_loss 2.1774316278126675 
Epoch [5/10] Batch 5800/7168 Train_loss 2.1759858391723066 
Epoch [5/10] Batch 5900/7168 Train_loss 2.1765446996354507 
Epoch [5/10] Batch 6000/7168 Train_loss 2.176173449635327 
Epoch [5/10] Batch 6100/7168 Train_loss 2.176720267315904 
Epoch [5/10] Batch 6200/7168 Train_loss 2.175307083665277 
Epoch [5/10] Batch 6300/7168 Train_loss 2.175911542213241 
Epoch [5/10] Batch 6400/7168 Train_loss 2.176514651845757 
Epoch [5/10] Batch 6500/7168 Train_loss 2.175987145372527 
Epoch [5/10] Batch 6600/7168 Train_loss 2.1759806080824173 
Epoch [5/10] Batch 6700/7168 Train_loss 2.177706801369642 
Epoch [5/10] Batch 6800/7168 Train_loss 2.177462211447575 
Epoch [5/10] Batch 6900/7168 Train_loss 2.178225661631322 
Epoch [5/10] Batch 7000/7168 Train_loss 2.1787283776245223 
Epoch [5/10] Batch 7100/7168 Train_loss 2.1773215867510785 
Epoch: 5/10 	Training Loss: 2.176943 	Validation Loss: 2.181692 Duration seconds: 947.6407260894775 
best_valid_loss_fold [2.1800200242183303] Best_Epoch [5]Epoch [6/10] Batch 0/7168 Train_loss 1.3913421332836151 
Epoch [6/10] Batch 100/7168 Train_loss 2.145600425134791 
Epoch [6/10] Batch 200/7168 Train_loss 2.138646084870865 
Epoch [6/10] Batch 300/7168 Train_loss 2.180633027094147 
Epoch [6/10] Batch 400/7168 Train_loss 2.193787309742925 
Epoch [6/10] Batch 500/7168 Train_loss 2.1733136536059026 
Epoch [6/10] Batch 600/7168 Train_loss 2.1934452288460218 
Epoch [6/10] Batch 700/7168 Train_loss 2.1827976373735067 
Epoch [6/10] Batch 800/7168 Train_loss 2.176362956861283 
Epoch [6/10] Batch 900/7168 Train_loss 2.1749949793472934 
Epoch [6/10] Batch 1000/7168 Train_loss 2.1782955042757353 
Epoch [6/10] Batch 1100/7168 Train_loss 2.178613535525255 
Epoch [6/10] Batch 1200/7168 Train_loss 2.1834064594514166 
Epoch [6/10] Batch 1300/7168 Train_loss 2.1805899544439344 
Epoch [6/10] Batch 1400/7168 Train_loss 2.18568652650231 
Epoch [6/10] Batch 1500/7168 Train_loss 2.191842638953855 
Epoch [6/10] Batch 1600/7168 Train_loss 2.187042922470288 
Epoch [6/10] Batch 1700/7168 Train_loss 2.1830474696111706 
Epoch [6/10] Batch 1800/7168 Train_loss 2.1848927513964638 
Epoch [6/10] Batch 1900/7168 Train_loss 2.188030382768221 
Epoch [6/10] Batch 2000/7168 Train_loss 2.1901842445015967 
Epoch [6/10] Batch 2100/7168 Train_loss 2.1869073070227563 
Epoch [6/10] Batch 2200/7168 Train_loss 2.1874133762190526 
Epoch [6/10] Batch 2300/7168 Train_loss 2.1859434221113108 
Epoch [6/10] Batch 2400/7168 Train_loss 2.1810001058846105 
Epoch [6/10] Batch 2500/7168 Train_loss 2.1819796304746135 
Epoch [6/10] Batch 2600/7168 Train_loss 2.1844719665147214 
Epoch [6/10] Batch 2700/7168 Train_loss 2.1792712603253994 
Epoch [6/10] Batch 2800/7168 Train_loss 2.1777271862138132 
Epoch [6/10] Batch 2900/7168 Train_loss 2.178098878231553 
Epoch [6/10] Batch 3000/7168 Train_loss 2.1772192214745676 
Epoch [6/10] Batch 3100/7168 Train_loss 2.1743076811468014 
Epoch [6/10] Batch 3200/7168 Train_loss 2.175897541701775 
Epoch [6/10] Batch 3300/7168 Train_loss 2.1759748340591956 
Epoch [6/10] Batch 3400/7168 Train_loss 2.1775947810227083 
Epoch [6/10] Batch 3500/7168 Train_loss 2.179350391970945 
Epoch [6/10] Batch 3600/7168 Train_loss 2.179857273708943 
Epoch [6/10] Batch 3700/7168 Train_loss 2.1792107977132096 
Epoch [6/10] Batch 3800/7168 Train_loss 2.1783118524682488 
Epoch [6/10] Batch 3900/7168 Train_loss 2.1776114524144203 
Epoch [6/10] Batch 4000/7168 Train_loss 2.177073273456743 
Epoch [6/10] Batch 4100/7168 Train_loss 2.177694348540721 
Epoch [6/10] Batch 4200/7168 Train_loss 2.1773450489102077 
Epoch [6/10] Batch 4300/7168 Train_loss 2.1769355123672507 
Epoch [6/10] Batch 4400/7168 Train_loss 2.1775233056722136 
Epoch [6/10] Batch 4500/7168 Train_loss 2.1759644699636973 
Epoch [6/10] Batch 4600/7168 Train_loss 2.1765594761207345 
Epoch [6/10] Batch 4700/7168 Train_loss 2.1773887873393276 
Epoch [6/10] Batch 4800/7168 Train_loss 2.1764872170064478 
Epoch [6/10] Batch 4900/7168 Train_loss 2.1748235677759444 
Epoch [6/10] Batch 5000/7168 Train_loss 2.1771082308287144 
Epoch [6/10] Batch 5100/7168 Train_loss 2.177272070461431 
Epoch [6/10] Batch 5200/7168 Train_loss 2.177644524908804 
Epoch [6/10] Batch 5300/7168 Train_loss 2.1769619660852224 
Epoch [6/10] Batch 5400/7168 Train_loss 2.17655162268536 
Epoch [6/10] Batch 5500/7168 Train_loss 2.1751551334288224 
Epoch [6/10] Batch 5600/7168 Train_loss 2.177057470584252 
Epoch [6/10] Batch 5700/7168 Train_loss 2.1774763173386624 
Epoch [6/10] Batch 5800/7168 Train_loss 2.178205266797816 
Epoch [6/10] Batch 5900/7168 Train_loss 2.177284372482153 
Epoch [6/10] Batch 6000/7168 Train_loss 2.1765800360872674 
Epoch [6/10] Batch 6100/7168 Train_loss 2.177480314439231 
Epoch [6/10] Batch 6200/7168 Train_loss 2.1771603607877648 
Epoch [6/10] Batch 6300/7168 Train_loss 2.177625404789966 
Epoch [6/10] Batch 6400/7168 Train_loss 2.178024753667455 
Epoch [6/10] Batch 6500/7168 Train_loss 2.180636718621659 
Epoch [6/10] Batch 6600/7168 Train_loss 2.179429596313977 
Epoch [6/10] Batch 6700/7168 Train_loss 2.180676407041327 
Epoch [6/10] Batch 6800/7168 Train_loss 2.1800343508694806 
Epoch [6/10] Batch 6900/7168 Train_loss 2.1795225594073444 
Epoch [6/10] Batch 7000/7168 Train_loss 2.1784603344472098 
Epoch [6/10] Batch 7100/7168 Train_loss 2.176890063245537 
Epoch: 6/10 	Training Loss: 2.176953 	Validation Loss: 2.182679 Duration seconds: 936.0941207408905 
best_valid_loss_fold [2.1800200242183303] Best_Epoch [6]Epoch [7/10] Batch 0/7168 Train_loss 1.9525855779647827 
Epoch [7/10] Batch 100/7168 Train_loss 2.154657111162006 
Epoch [7/10] Batch 200/7168 Train_loss 2.171361914914639 
Epoch [7/10] Batch 300/7168 Train_loss 2.160871678660082 
Epoch [7/10] Batch 400/7168 Train_loss 2.1550043484665213 
Epoch [7/10] Batch 500/7168 Train_loss 2.162271504719814 
Epoch [7/10] Batch 600/7168 Train_loss 2.1638843993626895 
Epoch [7/10] Batch 700/7168 Train_loss 2.1630231072108517 
Epoch [7/10] Batch 800/7168 Train_loss 2.1614835392706997 
Epoch [7/10] Batch 900/7168 Train_loss 2.1663161220812506 
Epoch [7/10] Batch 1000/7168 Train_loss 2.1679172644039966 
Epoch [7/10] Batch 1100/7168 Train_loss 2.1686906778980233 
Epoch [7/10] Batch 1200/7168 Train_loss 2.1641523727866434 
Epoch [7/10] Batch 1300/7168 Train_loss 2.17037382937229 
Epoch [7/10] Batch 1400/7168 Train_loss 2.1711281883788907 
Epoch [7/10] Batch 1500/7168 Train_loss 2.1715337152206287 
Epoch [7/10] Batch 1600/7168 Train_loss 2.1766570424527245 
Epoch [7/10] Batch 1700/7168 Train_loss 2.1755809423189034 
Epoch [7/10] Batch 1800/7168 Train_loss 2.17568607015255 
Epoch [7/10] Batch 1900/7168 Train_loss 2.170223055984521 
Epoch [7/10] Batch 2000/7168 Train_loss 2.167707056529459 
Epoch [7/10] Batch 2100/7168 Train_loss 2.1730048602348506 
Epoch [7/10] Batch 2200/7168 Train_loss 2.17722300495927 
Epoch [7/10] Batch 2300/7168 Train_loss 2.181611647428714 
Epoch [7/10] Batch 2400/7168 Train_loss 2.1816711868161414 
Epoch [7/10] Batch 2500/7168 Train_loss 2.18559765150813 
Epoch [7/10] Batch 2600/7168 Train_loss 2.184541673270247 
Epoch [7/10] Batch 2700/7168 Train_loss 2.1871363797791576 
Epoch [7/10] Batch 2800/7168 Train_loss 2.1853600076397415 
Epoch [7/10] Batch 2900/7168 Train_loss 2.1845454370531696 
Epoch [7/10] Batch 3000/7168 Train_loss 2.188379267719101 
Epoch [7/10] Batch 3100/7168 Train_loss 2.190852541235023 
Epoch [7/10] Batch 3200/7168 Train_loss 2.1906330014943554 
Epoch [7/10] Batch 3300/7168 Train_loss 2.189953053035508 
Epoch [7/10] Batch 3400/7168 Train_loss 2.1910846845231244 
Epoch [7/10] Batch 3500/7168 Train_loss 2.1894710728038347 
Epoch [7/10] Batch 3600/7168 Train_loss 2.189453467685327 
Epoch [7/10] Batch 3700/7168 Train_loss 2.1910978106832673 
Epoch [7/10] Batch 3800/7168 Train_loss 2.1898670733304315 
Epoch [7/10] Batch 3900/7168 Train_loss 2.1904694584623967 
Epoch [7/10] Batch 4000/7168 Train_loss 2.1898862254177143 
Epoch [7/10] Batch 4100/7168 Train_loss 2.190343896787767 
Epoch [7/10] Batch 4200/7168 Train_loss 2.1883658224473366 
Epoch [7/10] Batch 4300/7168 Train_loss 2.185837041601362 
Epoch [7/10] Batch 4400/7168 Train_loss 2.1840997847798898 
Epoch [7/10] Batch 4500/7168 Train_loss 2.185817011093013 
Epoch [7/10] Batch 4600/7168 Train_loss 2.1850666775570775 
Epoch [7/10] Batch 4700/7168 Train_loss 2.1851894386719706 
Epoch [7/10] Batch 4800/7168 Train_loss 2.1856912632077665 
Epoch [7/10] Batch 4900/7168 Train_loss 2.1840397946550225 
Epoch [7/10] Batch 5000/7168 Train_loss 2.1842467696535377 
Epoch [7/10] Batch 5100/7168 Train_loss 2.1829750445240315 
Epoch [7/10] Batch 5200/7168 Train_loss 2.181297983931959 
Epoch [7/10] Batch 5300/7168 Train_loss 2.181126430372071 
Epoch [7/10] Batch 5400/7168 Train_loss 2.181118834205765 
Epoch [7/10] Batch 5500/7168 Train_loss 2.181719701491341 
Epoch [7/10] Batch 5600/7168 Train_loss 2.181427689457744 
Epoch [7/10] Batch 5700/7168 Train_loss 2.178432062330988 
Epoch [7/10] Batch 5800/7168 Train_loss 2.179381948871831 
Epoch [7/10] Batch 5900/7168 Train_loss 2.1791393121831035 
Epoch [7/10] Batch 6000/7168 Train_loss 2.1790882612075273 
Epoch [7/10] Batch 6100/7168 Train_loss 2.1788079237726903 
Epoch [7/10] Batch 6200/7168 Train_loss 2.1765513382785993 
Epoch [7/10] Batch 6300/7168 Train_loss 2.1769803191788366 
Epoch [7/10] Batch 6400/7168 Train_loss 2.177795437495609 
Epoch [7/10] Batch 6500/7168 Train_loss 2.1778103808126454 
Epoch [7/10] Batch 6600/7168 Train_loss 2.178925276626567 
Epoch [7/10] Batch 6700/7168 Train_loss 2.1787691779113354 
Epoch [7/10] Batch 6800/7168 Train_loss 2.176958378439983 
Epoch [7/10] Batch 6900/7168 Train_loss 2.176776988348949 
Epoch [7/10] Batch 7000/7168 Train_loss 2.177781414960541 
Epoch [7/10] Batch 7100/7168 Train_loss 2.1772294010457616 
Epoch: 7/10 	Training Loss: 2.177062 	Validation Loss: 2.182026 Duration seconds: 938.0397214889526 
best_valid_loss_fold [2.1800200242183303] Best_Epoch [7]Epoch [8/10] Batch 0/7168 Train_loss 1.6432751715183258 
Epoch [8/10] Batch 100/7168 Train_loss 2.2381770345244076 
Epoch [8/10] Batch 200/7168 Train_loss 2.2304349589110606 
Epoch [8/10] Batch 300/7168 Train_loss 2.2117546693332173 
Epoch [8/10] Batch 400/7168 Train_loss 2.2071607399759743 
Epoch [8/10] Batch 500/7168 Train_loss 2.216556626790298 
Epoch [8/10] Batch 600/7168 Train_loss 2.1977794074922947 
Epoch [8/10] Batch 700/7168 Train_loss 2.2057972377572694 
Epoch [8/10] Batch 800/7168 Train_loss 2.2097572736376976 
Epoch [8/10] Batch 900/7168 Train_loss 2.2007961172639465 
Epoch [8/10] Batch 1000/7168 Train_loss 2.1961000550549468 
Epoch [8/10] Batch 1100/7168 Train_loss 2.185839801268833 
Epoch [8/10] Batch 1200/7168 Train_loss 2.1888465713476957 
Epoch [8/10] Batch 1300/7168 Train_loss 2.1895274103581217 
Epoch [8/10] Batch 1400/7168 Train_loss 2.1893582762159847 
Epoch [8/10] Batch 1500/7168 Train_loss 2.1876129662887323 
Epoch [8/10] Batch 1600/7168 Train_loss 2.182574538440722 
Epoch [8/10] Batch 1700/7168 Train_loss 2.18049559100525 
Epoch [8/10] Batch 1800/7168 Train_loss 2.182387169502233 
Epoch [8/10] Batch 1900/7168 Train_loss 2.1837693372881457 
Epoch [8/10] Batch 2000/7168 Train_loss 2.1834319980739654 
Epoch [8/10] Batch 2100/7168 Train_loss 2.1811964057885938 
Epoch [8/10] Batch 2200/7168 Train_loss 2.1791140363805135 
Epoch [8/10] Batch 2300/7168 Train_loss 2.1784774746676208 
Epoch [8/10] Batch 2400/7168 Train_loss 2.1779424283205095 
Epoch [8/10] Batch 2500/7168 Train_loss 2.176823464156341 
Epoch [8/10] Batch 2600/7168 Train_loss 2.1784409246722904 
Epoch [8/10] Batch 2700/7168 Train_loss 2.177818373193215 
Epoch [8/10] Batch 2800/7168 Train_loss 2.1767291905453527 
Epoch [8/10] Batch 2900/7168 Train_loss 2.1796733860834347 
Epoch [8/10] Batch 3000/7168 Train_loss 2.1784030509785546 
Epoch [8/10] Batch 3100/7168 Train_loss 2.177858539994822 
Epoch [8/10] Batch 3200/7168 Train_loss 2.1758834129244193 
Epoch [8/10] Batch 3300/7168 Train_loss 2.1750757395002345 
Epoch [8/10] Batch 3400/7168 Train_loss 2.172537737078822 
Epoch [8/10] Batch 3500/7168 Train_loss 2.173818074224234 
Epoch [8/10] Batch 3600/7168 Train_loss 2.174254134462893 
Epoch [8/10] Batch 3700/7168 Train_loss 2.17272975235187 
Epoch [8/10] Batch 3800/7168 Train_loss 2.1736890818336643 
Epoch [8/10] Batch 3900/7168 Train_loss 2.1713071387720486 
Epoch [8/10] Batch 4000/7168 Train_loss 2.1715408670503002 
Epoch [8/10] Batch 4100/7168 Train_loss 2.1697203991936926 
Epoch [8/10] Batch 4200/7168 Train_loss 2.1707658687643256 
Epoch [8/10] Batch 4300/7168 Train_loss 2.1707196172150978 
Epoch [8/10] Batch 4400/7168 Train_loss 2.171001602671033 
Epoch [8/10] Batch 4500/7168 Train_loss 2.1734681807605725 
Epoch [8/10] Batch 4600/7168 Train_loss 2.1734938631431353 
Epoch [8/10] Batch 4700/7168 Train_loss 2.1747484265287182 
Epoch [8/10] Batch 4800/7168 Train_loss 2.1733097851251566 
Epoch [8/10] Batch 4900/7168 Train_loss 2.172243702085717 
Epoch [8/10] Batch 5000/7168 Train_loss 2.17122046076174 
Epoch [8/10] Batch 5100/7168 Train_loss 2.1732396096972666 
Epoch [8/10] Batch 5200/7168 Train_loss 2.1716299037209295 
Epoch [8/10] Batch 5300/7168 Train_loss 2.1715126630467663 
Epoch [8/10] Batch 5400/7168 Train_loss 2.1716988337352263 
Epoch [8/10] Batch 5500/7168 Train_loss 2.1721254390866034 
Epoch [8/10] Batch 5600/7168 Train_loss 2.1723061035257687 
Epoch [8/10] Batch 5700/7168 Train_loss 2.174706874845572 
Epoch [8/10] Batch 5800/7168 Train_loss 2.173839061694543 
Epoch [8/10] Batch 5900/7168 Train_loss 2.175809531555481 
Epoch [8/10] Batch 6000/7168 Train_loss 2.1769274021313403 
Epoch [8/10] Batch 6100/7168 Train_loss 2.174660483247881 
Epoch [8/10] Batch 6200/7168 Train_loss 2.176498883559884 
Epoch [8/10] Batch 6300/7168 Train_loss 2.1765170082121195 
Epoch [8/10] Batch 6400/7168 Train_loss 2.176408311656221 
Epoch [8/10] Batch 6500/7168 Train_loss 2.1783323491895 
Epoch [8/10] Batch 6600/7168 Train_loss 2.177296639642306 
Epoch [8/10] Batch 6700/7168 Train_loss 2.1769248577991966 
Epoch [8/10] Batch 6800/7168 Train_loss 2.175929613872637 
Epoch [8/10] Batch 6900/7168 Train_loss 2.1755432046560115 
Epoch [8/10] Batch 7000/7168 Train_loss 2.1761142741529587 
Epoch [8/10] Batch 7100/7168 Train_loss 2.1767837097381744 
Epoch: 8/10 	Training Loss: 2.177045 	Validation Loss: 2.180770 Duration seconds: 942.1823194026947 
best_valid_loss_fold [2.1800200242183303] Best_Epoch [8]Epoch [9/10] Batch 0/7168 Train_loss 2.170746475458145 
Epoch [9/10] Batch 100/7168 Train_loss 2.2249807705088416 
Epoch [9/10] Batch 200/7168 Train_loss 2.194658955605469 
Epoch [9/10] Batch 300/7168 Train_loss 2.1877537311311577 
Epoch [9/10] Batch 400/7168 Train_loss 2.176928211031412 
Epoch [9/10] Batch 500/7168 Train_loss 2.16606257487081 
Epoch [9/10] Batch 600/7168 Train_loss 2.1490406147057124 
Epoch [9/10] Batch 700/7168 Train_loss 2.1468237722651256 
Epoch [9/10] Batch 800/7168 Train_loss 2.1436056090093283 
Epoch [9/10] Batch 900/7168 Train_loss 2.1379003092464677 
Epoch [9/10] Batch 1000/7168 Train_loss 2.143327525847561 
Epoch [9/10] Batch 1100/7168 Train_loss 2.1532911577893863 
Epoch [9/10] Batch 1200/7168 Train_loss 2.156026428262955 
Epoch [9/10] Batch 1300/7168 Train_loss 2.1514043981047615 
Epoch [9/10] Batch 1400/7168 Train_loss 2.1524674940500663 
Epoch [9/10] Batch 1500/7168 Train_loss 2.154824117882342 
Epoch [9/10] Batch 1600/7168 Train_loss 2.1553760681820244 
Epoch [9/10] Batch 1700/7168 Train_loss 2.157874095446919 
Epoch [9/10] Batch 1800/7168 Train_loss 2.161262024324315 
Epoch [9/10] Batch 1900/7168 Train_loss 2.1634409076552337 
Epoch [9/10] Batch 2000/7168 Train_loss 2.169972779727292 
Epoch [9/10] Batch 2100/7168 Train_loss 2.1699514491097918 
Epoch [9/10] Batch 2200/7168 Train_loss 2.1714134421580384 
Epoch [9/10] Batch 2300/7168 Train_loss 2.175151779923891 
Epoch [9/10] Batch 2400/7168 Train_loss 2.173666243178802 
Epoch [9/10] Batch 2500/7168 Train_loss 2.172864797722097 
Epoch [9/10] Batch 2600/7168 Train_loss 2.171460568435098 
Epoch [9/10] Batch 2700/7168 Train_loss 2.1708106462799237 
Epoch [9/10] Batch 2800/7168 Train_loss 2.171154806669037 
Epoch [9/10] Batch 2900/7168 Train_loss 2.169041562824157 
Epoch [9/10] Batch 3000/7168 Train_loss 2.169857904220652 
Epoch [9/10] Batch 3100/7168 Train_loss 2.1700095968673554 
Epoch [9/10] Batch 3200/7168 Train_loss 2.171084943487286 
Epoch [9/10] Batch 3300/7168 Train_loss 2.1710069799415996 
Epoch [9/10] Batch 3400/7168 Train_loss 2.1728890059065096 
Epoch [9/10] Batch 3500/7168 Train_loss 2.1750558773787216 
Epoch [9/10] Batch 3600/7168 Train_loss 2.1750665041008976 
Epoch [9/10] Batch 3700/7168 Train_loss 2.175662542188274 
Epoch [9/10] Batch 3800/7168 Train_loss 2.175817645104174 
Epoch [9/10] Batch 3900/7168 Train_loss 2.1762631213976706 
Epoch [9/10] Batch 4000/7168 Train_loss 2.1771092911573744 
Epoch [9/10] Batch 4100/7168 Train_loss 2.175966275483915 
Epoch [9/10] Batch 4200/7168 Train_loss 2.1785559145907962 
Epoch [9/10] Batch 4300/7168 Train_loss 2.1794447238030643 
Epoch [9/10] Batch 4400/7168 Train_loss 2.1782238642801293 
Epoch [9/10] Batch 4500/7168 Train_loss 2.1792360235123973 
Epoch [9/10] Batch 4600/7168 Train_loss 2.1790133790720083 
Epoch [9/10] Batch 4700/7168 Train_loss 2.1784945173076915 
Epoch [9/10] Batch 4800/7168 Train_loss 2.1790888553403414 
Epoch [9/10] Batch 4900/7168 Train_loss 2.1799844016477397 
Epoch [9/10] Batch 5000/7168 Train_loss 2.181230339675492 
Epoch [9/10] Batch 5100/7168 Train_loss 2.1805772037803837 
Epoch [9/10] Batch 5200/7168 Train_loss 2.17953840605492 
Epoch [9/10] Batch 5300/7168 Train_loss 2.177852079598347 
Epoch [9/10] Batch 5400/7168 Train_loss 2.1792307114644176 
Epoch [9/10] Batch 5500/7168 Train_loss 2.180222410670065 
Epoch [9/10] Batch 5600/7168 Train_loss 2.1794996389723784 
Epoch [9/10] Batch 5700/7168 Train_loss 2.181116254224921 
Epoch [9/10] Batch 5800/7168 Train_loss 2.179515748982326 
Epoch [9/10] Batch 5900/7168 Train_loss 2.178310585997202 
Epoch [9/10] Batch 6000/7168 Train_loss 2.1781644151744555 
Epoch [9/10] Batch 6100/7168 Train_loss 2.1782298319785443 
Epoch [9/10] Batch 6200/7168 Train_loss 2.1780668640738434 
Epoch [9/10] Batch 6300/7168 Train_loss 2.1785688266652365 
Epoch [9/10] Batch 6400/7168 Train_loss 2.1796066921458657 
Epoch [9/10] Batch 6500/7168 Train_loss 2.178301401556666 
Epoch [9/10] Batch 6600/7168 Train_loss 2.177373350497322 
Epoch [9/10] Batch 6700/7168 Train_loss 2.177127432303275 
Epoch [9/10] Batch 6800/7168 Train_loss 2.178024357238722 
Epoch [9/10] Batch 6900/7168 Train_loss 2.1787327763252957 
Epoch [9/10] Batch 7000/7168 Train_loss 2.1767310417604318 
Epoch [9/10] Batch 7100/7168 Train_loss 2.1763932542924462 
Epoch: 9/10 	Training Loss: 2.176947 	Validation Loss: 2.181689 Duration seconds: 955.7785940170288 
best_valid_loss_fold [2.1800200242183303] Best_Epoch [9]Fold: 5/5 
Epoch [0/10] Batch 0/7168 Train_loss 1.6058212518692017 
Epoch [0/10] Batch 100/7168 Train_loss 2.2282322525683016 
Epoch [0/10] Batch 200/7168 Train_loss 2.185363758781656 
Epoch [0/10] Batch 300/7168 Train_loss 2.174137504740411 
Epoch [0/10] Batch 400/7168 Train_loss 2.16631630542421 
Epoch [0/10] Batch 500/7168 Train_loss 2.1688637210282975 
Epoch [0/10] Batch 600/7168 Train_loss 2.164809207576087 
Epoch [0/10] Batch 700/7168 Train_loss 2.1568319293937397 
Epoch [0/10] Batch 800/7168 Train_loss 2.1599907581502578 
Epoch [0/10] Batch 900/7168 Train_loss 2.15980236185973 
Epoch [0/10] Batch 1000/7168 Train_loss 2.166592847931754 
Epoch [0/10] Batch 1100/7168 Train_loss 2.16648600102283 
Epoch [0/10] Batch 1200/7168 Train_loss 2.167593043834145 
Epoch [0/10] Batch 1300/7168 Train_loss 2.178233670017794 
Epoch [0/10] Batch 1400/7168 Train_loss 2.166978522263026 
Epoch [0/10] Batch 1500/7168 Train_loss 2.1660854469868123 
Epoch [0/10] Batch 1600/7168 Train_loss 2.1711978723506196 
Epoch [0/10] Batch 1700/7168 Train_loss 2.171201256094506 
Epoch [0/10] Batch 1800/7168 Train_loss 2.173605389491774 
Epoch [0/10] Batch 1900/7168 Train_loss 2.1742863648192374 
Epoch [0/10] Batch 2000/7168 Train_loss 2.173991905360565 
Epoch [0/10] Batch 2100/7168 Train_loss 2.1711690167141553 
Epoch [0/10] Batch 2200/7168 Train_loss 2.1741730426656067 
Epoch [0/10] Batch 2300/7168 Train_loss 2.1695154096796845 
Epoch [0/10] Batch 2400/7168 Train_loss 2.1708190972695296 
Epoch [0/10] Batch 2500/7168 Train_loss 2.172707832131277 
Epoch [0/10] Batch 2600/7168 Train_loss 2.1735805419432297 
Epoch [0/10] Batch 2700/7168 Train_loss 2.174183719442183 
Epoch [0/10] Batch 2800/7168 Train_loss 2.1700215717642615 
Epoch [0/10] Batch 2900/7168 Train_loss 2.1708039914491217 
Epoch [0/10] Batch 3000/7168 Train_loss 2.1755822721132634 
Epoch [0/10] Batch 3100/7168 Train_loss 2.1738588141553135 
Epoch [0/10] Batch 3200/7168 Train_loss 2.173343511116166 
Epoch [0/10] Batch 3300/7168 Train_loss 2.168687601810475 
Epoch [0/10] Batch 3400/7168 Train_loss 2.1670676471643047 
Epoch [0/10] Batch 3500/7168 Train_loss 2.1682952795368164 
Epoch [0/10] Batch 3600/7168 Train_loss 2.17046542960599 
Epoch [0/10] Batch 3700/7168 Train_loss 2.172709892779967 
Epoch [0/10] Batch 3800/7168 Train_loss 2.172619588506501 
Epoch [0/10] Batch 3900/7168 Train_loss 2.1715556102680935 
Epoch [0/10] Batch 4000/7168 Train_loss 2.171396980305607 
Epoch [0/10] Batch 4100/7168 Train_loss 2.1705704111532302 
Epoch [0/10] Batch 4200/7168 Train_loss 2.171659107787556 
Epoch [0/10] Batch 4300/7168 Train_loss 2.172761587549492 
Epoch [0/10] Batch 4400/7168 Train_loss 2.171794743622278 
Epoch [0/10] Batch 4500/7168 Train_loss 2.1729376863616223 
Epoch [0/10] Batch 4600/7168 Train_loss 2.1728570884471354 
Epoch [0/10] Batch 4700/7168 Train_loss 2.169161717994014 
Epoch [0/10] Batch 4800/7168 Train_loss 2.168469563688345 
Epoch [0/10] Batch 4900/7168 Train_loss 2.170142997873362 
Epoch [0/10] Batch 5000/7168 Train_loss 2.1720128357142263 
Epoch [0/10] Batch 5100/7168 Train_loss 2.1722010782485475 
Epoch [0/10] Batch 5200/7168 Train_loss 2.172938167914201 
Epoch [0/10] Batch 5300/7168 Train_loss 2.1731598152684346 
Epoch [0/10] Batch 5400/7168 Train_loss 2.1741042297055513 
Epoch [0/10] Batch 5500/7168 Train_loss 2.1760011486975936 
Epoch [0/10] Batch 5600/7168 Train_loss 2.175803780263076 
Epoch [0/10] Batch 5700/7168 Train_loss 2.1745308655627964 
Epoch [0/10] Batch 5800/7168 Train_loss 2.1747896631505648 
Epoch [0/10] Batch 5900/7168 Train_loss 2.175707080554728 
Epoch [0/10] Batch 6000/7168 Train_loss 2.1764469576679533 
Epoch [0/10] Batch 6100/7168 Train_loss 2.176028604408089 
Epoch [0/10] Batch 6200/7168 Train_loss 2.176737713404787 
Epoch [0/10] Batch 6300/7168 Train_loss 2.1772431101667147 
Epoch [0/10] Batch 6400/7168 Train_loss 2.1774561087929856 
Epoch [0/10] Batch 6500/7168 Train_loss 2.178243218126471 
Epoch [0/10] Batch 6600/7168 Train_loss 2.1787367698379114 
Epoch [0/10] Batch 6700/7168 Train_loss 2.179182330539753 
Epoch [0/10] Batch 6800/7168 Train_loss 2.1782037604820936 
Epoch [0/10] Batch 6900/7168 Train_loss 2.1780122851650883 
Epoch [0/10] Batch 7000/7168 Train_loss 2.1785761853245016 
Epoch [0/10] Batch 7100/7168 Train_loss 2.177092350653201 
Epoch: 0/10 	Training Loss: 2.178251 	Validation Loss: 2.176845 Duration seconds: 949.5058071613312 
Validation loss decreased (inf --> 2.176845).  Saving model ... 
best_valid_loss_fold [2.176844690577127] Best_Epoch [0]Epoch [1/10] Batch 0/7168 Train_loss 3.289596736431122 
Epoch [1/10] Batch 100/7168 Train_loss 2.1631950663753075 
Epoch [1/10] Batch 200/7168 Train_loss 2.171274388755732 
Epoch [1/10] Batch 300/7168 Train_loss 2.183014675232263 
Epoch [1/10] Batch 400/7168 Train_loss 2.17956830122673 
Epoch [1/10] Batch 500/7168 Train_loss 2.2082657893439253 
Epoch [1/10] Batch 600/7168 Train_loss 2.1977503157643827 
Epoch [1/10] Batch 700/7168 Train_loss 2.198362866158832 
Epoch [1/10] Batch 800/7168 Train_loss 2.191488788499219 
Epoch [1/10] Batch 900/7168 Train_loss 2.1776252424346487 
Epoch [1/10] Batch 1000/7168 Train_loss 2.174408193398546 
Epoch [1/10] Batch 1100/7168 Train_loss 2.1781107310297054 
Epoch [1/10] Batch 1200/7168 Train_loss 2.1759465793388073 
Epoch [1/10] Batch 1300/7168 Train_loss 2.1787329758019927 
Epoch [1/10] Batch 1400/7168 Train_loss 2.1741433373567975 
Epoch [1/10] Batch 1500/7168 Train_loss 2.1713955121624875 
Epoch [1/10] Batch 1600/7168 Train_loss 2.1712634505367814 
Epoch [1/10] Batch 1700/7168 Train_loss 2.1733961596883513 
Epoch [1/10] Batch 1800/7168 Train_loss 2.1742467461841626 
Epoch [1/10] Batch 1900/7168 Train_loss 2.1781571739711367 
Epoch [1/10] Batch 2000/7168 Train_loss 2.1764872091329437 
Epoch [1/10] Batch 2100/7168 Train_loss 2.179514918368184 
Epoch [1/10] Batch 2200/7168 Train_loss 2.1808304886474548 
Epoch [1/10] Batch 2300/7168 Train_loss 2.1780700152371564 
Epoch [1/10] Batch 2400/7168 Train_loss 2.1812812107049244 
Epoch [1/10] Batch 2500/7168 Train_loss 2.1823981218114468 
Epoch [1/10] Batch 2600/7168 Train_loss 2.1828075106764517 
Epoch [1/10] Batch 2700/7168 Train_loss 2.1824451302299233 
Epoch [1/10] Batch 2800/7168 Train_loss 2.1812629487175297 
Epoch [1/10] Batch 2900/7168 Train_loss 2.1785455290838094 
Epoch [1/10] Batch 3000/7168 Train_loss 2.1754411759673657 
Epoch [1/10] Batch 3100/7168 Train_loss 2.1766107311963037 
Epoch [1/10] Batch 3200/7168 Train_loss 2.180258047693374 
Epoch [1/10] Batch 3300/7168 Train_loss 2.181562131388698 
Epoch [1/10] Batch 3400/7168 Train_loss 2.181538508170424 
Epoch [1/10] Batch 3500/7168 Train_loss 2.1814362870322332 
Epoch [1/10] Batch 3600/7168 Train_loss 2.1830556747756775 
Epoch [1/10] Batch 3700/7168 Train_loss 2.1841379994477688 
Epoch [1/10] Batch 3800/7168 Train_loss 2.18418789581072 
Epoch [1/10] Batch 3900/7168 Train_loss 2.183991220193134 
Epoch [1/10] Batch 4000/7168 Train_loss 2.185949709744073 
Epoch [1/10] Batch 4100/7168 Train_loss 2.184482770750535 
Epoch [1/10] Batch 4200/7168 Train_loss 2.1827873789080834 
Epoch [1/10] Batch 4300/7168 Train_loss 2.1829864020175473 
Epoch [1/10] Batch 4400/7168 Train_loss 2.1851722090095587 
Epoch [1/10] Batch 4500/7168 Train_loss 2.184460813208994 
Epoch [1/10] Batch 4600/7168 Train_loss 2.187042310137615 
Epoch [1/10] Batch 4700/7168 Train_loss 2.185934675000394 
Epoch [1/10] Batch 4800/7168 Train_loss 2.1857549903002216 
Epoch [1/10] Batch 4900/7168 Train_loss 2.1853170396448083 
Epoch [1/10] Batch 5000/7168 Train_loss 2.1821470151774003 
Epoch [1/10] Batch 5100/7168 Train_loss 2.1822587296971383 
Epoch [1/10] Batch 5200/7168 Train_loss 2.1821246187581713 
Epoch [1/10] Batch 5300/7168 Train_loss 2.181850461068974 
Epoch [1/10] Batch 5400/7168 Train_loss 2.1822301960669055 
Epoch [1/10] Batch 5500/7168 Train_loss 2.18328859321834 
Epoch [1/10] Batch 5600/7168 Train_loss 2.182309407899793 
Epoch [1/10] Batch 5700/7168 Train_loss 2.181746674156925 
Epoch [1/10] Batch 5800/7168 Train_loss 2.180946664517038 
Epoch [1/10] Batch 5900/7168 Train_loss 2.1807250826717572 
Epoch [1/10] Batch 6000/7168 Train_loss 2.180123257735852 
Epoch [1/10] Batch 6100/7168 Train_loss 2.1810406588971705 
Epoch [1/10] Batch 6200/7168 Train_loss 2.178899686897095 
Epoch [1/10] Batch 6300/7168 Train_loss 2.180235292978314 
Epoch [1/10] Batch 6400/7168 Train_loss 2.1797629884142853 
Epoch [1/10] Batch 6500/7168 Train_loss 2.1788036205542674 
Epoch [1/10] Batch 6600/7168 Train_loss 2.1777104992132226 
Epoch [1/10] Batch 6700/7168 Train_loss 2.1774359969701576 
Epoch [1/10] Batch 6800/7168 Train_loss 2.1778838340082936 
Epoch [1/10] Batch 6900/7168 Train_loss 2.1776941344692577 
Epoch [1/10] Batch 7000/7168 Train_loss 2.176514361157466 
Epoch [1/10] Batch 7100/7168 Train_loss 2.1771626019898376 
Epoch: 1/10 	Training Loss: 2.178304 	Validation Loss: 2.176947 Duration seconds: 948.9364900588989 
best_valid_loss_fold [2.176844690577127] Best_Epoch [1]Epoch [2/10] Batch 0/7168 Train_loss 1.8775556981563568 
Epoch [2/10] Batch 100/7168 Train_loss 2.1681260509361135 
Epoch [2/10] Batch 200/7168 Train_loss 2.159455251412012 
Epoch [2/10] Batch 300/7168 Train_loss 2.192610124268405 
Epoch [2/10] Batch 400/7168 Train_loss 2.200678686734447 
Epoch [2/10] Batch 500/7168 Train_loss 2.18519155017868 
Epoch [2/10] Batch 600/7168 Train_loss 2.1824496843156322 
Epoch [2/10] Batch 700/7168 Train_loss 2.1678555094651593 
Epoch [2/10] Batch 800/7168 Train_loss 2.1638742302278158 
Epoch [2/10] Batch 900/7168 Train_loss 2.1663967663883237 
Epoch [2/10] Batch 1000/7168 Train_loss 2.165033782978396 
Epoch [2/10] Batch 1100/7168 Train_loss 2.160654655851417 
Epoch [2/10] Batch 1200/7168 Train_loss 2.158383210433909 
Epoch [2/10] Batch 1300/7168 Train_loss 2.1638988547720057 
Epoch [2/10] Batch 1400/7168 Train_loss 2.172681102203438 
Epoch [2/10] Batch 1500/7168 Train_loss 2.1650738503379396 
Epoch [2/10] Batch 1600/7168 Train_loss 2.168607016993939 
Epoch [2/10] Batch 1700/7168 Train_loss 2.17291133481569 
Epoch [2/10] Batch 1800/7168 Train_loss 2.1730464128058595 
Epoch [2/10] Batch 1900/7168 Train_loss 2.173221796194482 
Epoch [2/10] Batch 2000/7168 Train_loss 2.171515405386582 
Epoch [2/10] Batch 2100/7168 Train_loss 2.1705771845786246 
Epoch [2/10] Batch 2200/7168 Train_loss 2.1697189348806094 
Epoch [2/10] Batch 2300/7168 Train_loss 2.1716073037540844 
Epoch [2/10] Batch 2400/7168 Train_loss 2.1712696803008353 
Epoch [2/10] Batch 2500/7168 Train_loss 2.172408013588569 
Epoch [2/10] Batch 2600/7168 Train_loss 2.1713435339359357 
Epoch [2/10] Batch 2700/7168 Train_loss 2.173409317818362 
Epoch [2/10] Batch 2800/7168 Train_loss 2.1707039204336667 
Epoch [2/10] Batch 2900/7168 Train_loss 2.1697416398462366 
Epoch [2/10] Batch 3000/7168 Train_loss 2.171557942322793 
Epoch [2/10] Batch 3100/7168 Train_loss 2.174454640521422 
Epoch [2/10] Batch 3200/7168 Train_loss 2.1719036094660464 
Epoch [2/10] Batch 3300/7168 Train_loss 2.1733712688228355 
Epoch [2/10] Batch 3400/7168 Train_loss 2.1747818279006976 
Epoch [2/10] Batch 3500/7168 Train_loss 2.17657708636065 
Epoch [2/10] Batch 3600/7168 Train_loss 2.1741243368244874 
Epoch [2/10] Batch 3700/7168 Train_loss 2.176784007626107 
Epoch [2/10] Batch 3800/7168 Train_loss 2.1740515455427185 
Epoch [2/10] Batch 3900/7168 Train_loss 2.1745279181717114 
Epoch [2/10] Batch 4000/7168 Train_loss 2.1744332992525526 
Epoch [2/10] Batch 4100/7168 Train_loss 2.1736685478943087 
Epoch [2/10] Batch 4200/7168 Train_loss 2.1731573860437012 
Epoch [2/10] Batch 4300/7168 Train_loss 2.172798831669765 
Epoch [2/10] Batch 4400/7168 Train_loss 2.173713892461278 
Epoch [2/10] Batch 4500/7168 Train_loss 2.172546791708541 
Epoch [2/10] Batch 4600/7168 Train_loss 2.17118021501546 
Epoch [2/10] Batch 4700/7168 Train_loss 2.172541869806244 
Epoch [2/10] Batch 4800/7168 Train_loss 2.1732066348707195 
Epoch [2/10] Batch 4900/7168 Train_loss 2.173723880185616 
Epoch [2/10] Batch 5000/7168 Train_loss 2.1725570690426865 
Epoch [2/10] Batch 5100/7168 Train_loss 2.1720954096625773 
Epoch [2/10] Batch 5200/7168 Train_loss 2.173139048789332 
Epoch [2/10] Batch 5300/7168 Train_loss 2.1712218206590177 
Epoch [2/10] Batch 5400/7168 Train_loss 2.172085611583171 
Epoch [2/10] Batch 5500/7168 Train_loss 2.172498710777213 
Epoch [2/10] Batch 5600/7168 Train_loss 2.1744011286922147 
Epoch [2/10] Batch 5700/7168 Train_loss 2.1749630796111608 
Epoch [2/10] Batch 5800/7168 Train_loss 2.1754553624831936 
Epoch [2/10] Batch 5900/7168 Train_loss 2.1751435234236163 
Epoch [2/10] Batch 6000/7168 Train_loss 2.175890014228613 
Epoch [2/10] Batch 6100/7168 Train_loss 2.1768201699053296 
Epoch [2/10] Batch 6200/7168 Train_loss 2.176410131753596 
Epoch [2/10] Batch 6300/7168 Train_loss 2.177224215430469 
Epoch [2/10] Batch 6400/7168 Train_loss 2.1768387710511767 
Epoch [2/10] Batch 6500/7168 Train_loss 2.178062852337404 
Epoch [2/10] Batch 6600/7168 Train_loss 2.177807742303979 
Epoch [2/10] Batch 6700/7168 Train_loss 2.178700617837685 
Epoch [2/10] Batch 6800/7168 Train_loss 2.179148718539432 
Epoch [2/10] Batch 6900/7168 Train_loss 2.1788824291341355 
Epoch [2/10] Batch 7000/7168 Train_loss 2.178211684234056 
Epoch [2/10] Batch 7100/7168 Train_loss 2.1778465053776155 
Epoch: 2/10 	Training Loss: 2.178410 	Validation Loss: 2.177375 Duration seconds: 947.6182134151459 
best_valid_loss_fold [2.176844690577127] Best_Epoch [2]Epoch [3/10] Batch 0/7168 Train_loss 2.01670303940773 
Epoch [3/10] Batch 100/7168 Train_loss 2.1526813698877203 
Epoch [3/10] Batch 200/7168 Train_loss 2.180986313974086 
Epoch [3/10] Batch 300/7168 Train_loss 2.1525691679842844 
Epoch [3/10] Batch 400/7168 Train_loss 2.158709644825381 
Epoch [3/10] Batch 500/7168 Train_loss 2.1781261745624674 
Epoch [3/10] Batch 600/7168 Train_loss 2.188841798861888 
Epoch [3/10] Batch 700/7168 Train_loss 2.1901745767931797 
Epoch [3/10] Batch 800/7168 Train_loss 2.1845788797003203 
Epoch [3/10] Batch 900/7168 Train_loss 2.1769743854310217 
Epoch [3/10] Batch 1000/7168 Train_loss 2.1799453968679035 
Epoch [3/10] Batch 1100/7168 Train_loss 2.173170146822496 
Epoch [3/10] Batch 1200/7168 Train_loss 2.1745977829909147 
Epoch [3/10] Batch 1300/7168 Train_loss 2.175252838353759 
Epoch [3/10] Batch 1400/7168 Train_loss 2.1732473081984236 
Epoch [3/10] Batch 1500/7168 Train_loss 2.1790258350132468 
Epoch [3/10] Batch 1600/7168 Train_loss 2.182157460057981 
Epoch [3/10] Batch 1700/7168 Train_loss 2.182931592408732 
Epoch [3/10] Batch 1800/7168 Train_loss 2.183976850075764 
Epoch [3/10] Batch 1900/7168 Train_loss 2.1792814212369644 
Epoch [3/10] Batch 2000/7168 Train_loss 2.1839508998102097 
Epoch [3/10] Batch 2100/7168 Train_loss 2.1837432814433084 
Epoch [3/10] Batch 2200/7168 Train_loss 2.184896761892547 
Epoch [3/10] Batch 2300/7168 Train_loss 2.1828836171860697 
Epoch [3/10] Batch 2400/7168 Train_loss 2.185747956314816 
Epoch [3/10] Batch 2500/7168 Train_loss 2.185526085419447 
Epoch [3/10] Batch 2600/7168 Train_loss 2.184667587538079 
Epoch [3/10] Batch 2700/7168 Train_loss 2.1857778815175375 
Epoch [3/10] Batch 2800/7168 Train_loss 2.1853792282491016 
Epoch [3/10] Batch 2900/7168 Train_loss 2.18326957723635 
Epoch [3/10] Batch 3000/7168 Train_loss 2.180826407657072 
Epoch [3/10] Batch 3100/7168 Train_loss 2.1840197211194523 
Epoch [3/10] Batch 3200/7168 Train_loss 2.1826287453266207 
Epoch [3/10] Batch 3300/7168 Train_loss 2.185534377647544 
Epoch [3/10] Batch 3400/7168 Train_loss 2.1852490880302877 
Epoch [3/10] Batch 3500/7168 Train_loss 2.1825220457564694 
Epoch [3/10] Batch 3600/7168 Train_loss 2.185112911917143 
Epoch [3/10] Batch 3700/7168 Train_loss 2.185677074799825 
Epoch [3/10] Batch 3800/7168 Train_loss 2.1855615973629408 
Epoch [3/10] Batch 3900/7168 Train_loss 2.186824485327274 
Epoch [3/10] Batch 4000/7168 Train_loss 2.1867621060245725 
Epoch [3/10] Batch 4100/7168 Train_loss 2.1857658015521846 
Epoch [3/10] Batch 4200/7168 Train_loss 2.1851147278359435 
Epoch [3/10] Batch 4300/7168 Train_loss 2.1818939543406484 
Epoch [3/10] Batch 4400/7168 Train_loss 2.18213341322657 
Epoch [3/10] Batch 4500/7168 Train_loss 2.1826037331352866 
Epoch [3/10] Batch 4600/7168 Train_loss 2.1834921826165385 
Epoch [3/10] Batch 4700/7168 Train_loss 2.182636190379358 
Epoch [3/10] Batch 4800/7168 Train_loss 2.1815351605216704 
Epoch [3/10] Batch 4900/7168 Train_loss 2.181636268427678 
Epoch [3/10] Batch 5000/7168 Train_loss 2.1798795444670738 
Epoch [3/10] Batch 5100/7168 Train_loss 2.1791320396968836 
Epoch [3/10] Batch 5200/7168 Train_loss 2.177361690168357 
Epoch [3/10] Batch 5300/7168 Train_loss 2.176649508060678 
Epoch [3/10] Batch 5400/7168 Train_loss 2.1768510665684535 
Epoch [3/10] Batch 5500/7168 Train_loss 2.1774167617062745 
Epoch [3/10] Batch 5600/7168 Train_loss 2.17921039715098 
Epoch [3/10] Batch 5700/7168 Train_loss 2.1798373314580926 
Epoch [3/10] Batch 5800/7168 Train_loss 2.179493612268324 
Epoch [3/10] Batch 5900/7168 Train_loss 2.1794510606870148 
Epoch [3/10] Batch 6000/7168 Train_loss 2.1802684609476994 
Epoch [3/10] Batch 6100/7168 Train_loss 2.180502527200908 
Epoch [3/10] Batch 6200/7168 Train_loss 2.1812682571074324 
Epoch [3/10] Batch 6300/7168 Train_loss 2.179954149121047 
Epoch [3/10] Batch 6400/7168 Train_loss 2.1795862777412656 
Epoch [3/10] Batch 6500/7168 Train_loss 2.1790515607599112 
Epoch [3/10] Batch 6600/7168 Train_loss 2.1783445386444327 
Epoch [3/10] Batch 6700/7168 Train_loss 2.1779706333109985 
Epoch [3/10] Batch 6800/7168 Train_loss 2.1780226418414093 
Epoch [3/10] Batch 6900/7168 Train_loss 2.178259992958791 
Epoch [3/10] Batch 7000/7168 Train_loss 2.177264212321135 
Epoch [3/10] Batch 7100/7168 Train_loss 2.1784477710824603 
Epoch: 3/10 	Training Loss: 2.178407 	Validation Loss: 2.178043 Duration seconds: 948.1030602455139 
best_valid_loss_fold [2.176844690577127] Best_Epoch [3]Epoch [4/10] Batch 0/7168 Train_loss 2.036398947238922 
Epoch [4/10] Batch 100/7168 Train_loss 2.181562073897607 
Epoch [4/10] Batch 200/7168 Train_loss 2.1802451895244084 
Epoch [4/10] Batch 300/7168 Train_loss 2.187941140956261 
Epoch [4/10] Batch 400/7168 Train_loss 2.1815972734344866 
Epoch [4/10] Batch 500/7168 Train_loss 2.191248310807936 
Epoch [4/10] Batch 600/7168 Train_loss 2.1688738160343615 
Epoch [4/10] Batch 700/7168 Train_loss 2.1771655728065338 
Epoch [4/10] Batch 800/7168 Train_loss 2.187767628482665 
Epoch [4/10] Batch 900/7168 Train_loss 2.192001704552462 
Epoch [4/10] Batch 1000/7168 Train_loss 2.1837827301287387 
Epoch [4/10] Batch 1100/7168 Train_loss 2.1915709724028907 
Epoch [4/10] Batch 1200/7168 Train_loss 2.1874529148120865 
Epoch [4/10] Batch 1300/7168 Train_loss 2.184650242775335 
Epoch [4/10] Batch 1400/7168 Train_loss 2.184202174345652 
Epoch [4/10] Batch 1500/7168 Train_loss 2.185642306980175 
Epoch [4/10] Batch 1600/7168 Train_loss 2.1891998369793533 
Epoch [4/10] Batch 1700/7168 Train_loss 2.1866273843649906 
Epoch [4/10] Batch 1800/7168 Train_loss 2.181541299502231 
Epoch [4/10] Batch 1900/7168 Train_loss 2.177322594344083 
Epoch [4/10] Batch 2000/7168 Train_loss 2.1719413803986227 
Epoch [4/10] Batch 2100/7168 Train_loss 2.1781933364032393 
Epoch [4/10] Batch 2200/7168 Train_loss 2.178298733935525 
Epoch [4/10] Batch 2300/7168 Train_loss 2.1799208739952958 
Epoch [4/10] Batch 2400/7168 Train_loss 2.1803638636631453 
Epoch [4/10] Batch 2500/7168 Train_loss 2.1801257294649985 
Epoch [4/10] Batch 2600/7168 Train_loss 2.1817970378141776 
Epoch [4/10] Batch 2700/7168 Train_loss 2.1750958830460227 
Epoch [4/10] Batch 2800/7168 Train_loss 2.175750243199889 
Epoch [4/10] Batch 2900/7168 Train_loss 2.1774023978546215 
Epoch [4/10] Batch 3000/7168 Train_loss 2.1782056947314157 
Epoch [4/10] Batch 3100/7168 Train_loss 2.1783131514295997 
Epoch [4/10] Batch 3200/7168 Train_loss 2.1803752954622166 
Epoch [4/10] Batch 3300/7168 Train_loss 2.1804585707788937 
Epoch [4/10] Batch 3400/7168 Train_loss 2.1798023850084998 
Epoch [4/10] Batch 3500/7168 Train_loss 2.179051688937452 
Epoch [4/10] Batch 3600/7168 Train_loss 2.1779826829885978 
Epoch [4/10] Batch 3700/7168 Train_loss 2.1808672966916185 
Epoch [4/10] Batch 3800/7168 Train_loss 2.1807151831935436 
Epoch [4/10] Batch 3900/7168 Train_loss 2.178615045066951 
Epoch [4/10] Batch 4000/7168 Train_loss 2.1776493055891675 
Epoch [4/10] Batch 4100/7168 Train_loss 2.1763410628699464 
Epoch [4/10] Batch 4200/7168 Train_loss 2.176383889482663 
Epoch [4/10] Batch 4300/7168 Train_loss 2.17736707151047 
Epoch [4/10] Batch 4400/7168 Train_loss 2.1792543796457178 
Epoch [4/10] Batch 4500/7168 Train_loss 2.1822171949613254 
Epoch [4/10] Batch 4600/7168 Train_loss 2.1814702559239034 
Epoch [4/10] Batch 4700/7168 Train_loss 2.181110512643539 
Epoch [4/10] Batch 4800/7168 Train_loss 2.1806467913434147 
Epoch [4/10] Batch 4900/7168 Train_loss 2.180536433984445 
Epoch [4/10] Batch 5000/7168 Train_loss 2.179413683708013 
Epoch [4/10] Batch 5100/7168 Train_loss 2.1805369175957408 
Epoch [4/10] Batch 5200/7168 Train_loss 2.1825039220289457 
Epoch [4/10] Batch 5300/7168 Train_loss 2.180418053610643 
Epoch [4/10] Batch 5400/7168 Train_loss 2.181227219971633 
Epoch [4/10] Batch 5500/7168 Train_loss 2.1813323195983747 
Epoch [4/10] Batch 5600/7168 Train_loss 2.1830352184533477 
Epoch [4/10] Batch 5700/7168 Train_loss 2.1825448442443256 
Epoch [4/10] Batch 5800/7168 Train_loss 2.182561356655854 
Epoch [4/10] Batch 5900/7168 Train_loss 2.181028299269747 
Epoch [4/10] Batch 6000/7168 Train_loss 2.1796655108870673 
Epoch [4/10] Batch 6100/7168 Train_loss 2.1801906463489673 
Epoch [4/10] Batch 6200/7168 Train_loss 2.18084728644752 
Epoch [4/10] Batch 6300/7168 Train_loss 2.1809910104764185 
Epoch [4/10] Batch 6400/7168 Train_loss 2.1801736822721085 
Epoch [4/10] Batch 6500/7168 Train_loss 2.1797328331510464 
Epoch [4/10] Batch 6600/7168 Train_loss 2.1792942134855626 
Epoch [4/10] Batch 6700/7168 Train_loss 2.1792321856422863 
Epoch [4/10] Batch 6800/7168 Train_loss 2.1782994073132453 
Epoch [4/10] Batch 6900/7168 Train_loss 2.1789164810513366 
Epoch [4/10] Batch 7000/7168 Train_loss 2.1794322505859354 
Epoch [4/10] Batch 7100/7168 Train_loss 2.1793873094829133 
Epoch: 4/10 	Training Loss: 2.178457 	Validation Loss: 2.175485 Duration seconds: 962.1929061412811 
Validation loss decreased (2.176845 --> 2.175485).  Saving model ... 
best_valid_loss_fold [2.175485256435682] Best_Epoch [4]Epoch [5/10] Batch 0/7168 Train_loss 2.66241192817688 
Epoch [5/10] Batch 100/7168 Train_loss 2.186152934290395 
Epoch [5/10] Batch 200/7168 Train_loss 2.18968493799072 
Epoch [5/10] Batch 300/7168 Train_loss 2.212020160005338 
Epoch [5/10] Batch 400/7168 Train_loss 2.211796846658809 
Epoch [5/10] Batch 500/7168 Train_loss 2.2076354990759772 
Epoch [5/10] Batch 600/7168 Train_loss 2.1999073695521583 
Epoch [5/10] Batch 700/7168 Train_loss 2.1950655895020925 
Epoch [5/10] Batch 800/7168 Train_loss 2.2070799204294156 
Epoch [5/10] Batch 900/7168 Train_loss 2.209855059821254 
Epoch [5/10] Batch 1000/7168 Train_loss 2.2021198959706667 
Epoch [5/10] Batch 1100/7168 Train_loss 2.1889958705932417 
Epoch [5/10] Batch 1200/7168 Train_loss 2.18353339637646 
Epoch [5/10] Batch 1300/7168 Train_loss 2.176593187265814 
Epoch [5/10] Batch 1400/7168 Train_loss 2.175894586913165 
Epoch [5/10] Batch 1500/7168 Train_loss 2.1711859295143436 
Epoch [5/10] Batch 1600/7168 Train_loss 2.167890680852642 
Epoch [5/10] Batch 1700/7168 Train_loss 2.1709722888462686 
Epoch [5/10] Batch 1800/7168 Train_loss 2.16923893991442 
Epoch [5/10] Batch 1900/7168 Train_loss 2.1698421393489284 
Epoch [5/10] Batch 2000/7168 Train_loss 2.170788264874814 
Epoch [5/10] Batch 2100/7168 Train_loss 2.173757834775796 
Epoch [5/10] Batch 2200/7168 Train_loss 2.1742180906158315 
Epoch [5/10] Batch 2300/7168 Train_loss 2.178759385598115 
Epoch [5/10] Batch 2400/7168 Train_loss 2.177275951264402 
Epoch [5/10] Batch 2500/7168 Train_loss 2.1775428408601196 
Epoch [5/10] Batch 2600/7168 Train_loss 2.1767778883114177 
Epoch [5/10] Batch 2700/7168 Train_loss 2.1771779669374944 
Epoch [5/10] Batch 2800/7168 Train_loss 2.176053053069353 
Epoch [5/10] Batch 2900/7168 Train_loss 2.176070783013765 
Epoch [5/10] Batch 3000/7168 Train_loss 2.1760926018532656 
Epoch [5/10] Batch 3100/7168 Train_loss 2.177070196704763 
Epoch [5/10] Batch 3200/7168 Train_loss 2.17481132437292 
Epoch [5/10] Batch 3300/7168 Train_loss 2.1753187404233953 
Epoch [5/10] Batch 3400/7168 Train_loss 2.1726996480971708 
Epoch [5/10] Batch 3500/7168 Train_loss 2.1721457469739156 
Epoch [5/10] Batch 3600/7168 Train_loss 2.1730774847733185 
Epoch [5/10] Batch 3700/7168 Train_loss 2.172855963677337 
Epoch [5/10] Batch 3800/7168 Train_loss 2.170081991830709 
Epoch [5/10] Batch 3900/7168 Train_loss 2.16870382615341 
Epoch [5/10] Batch 4000/7168 Train_loss 2.166737502152638 
Epoch [5/10] Batch 4100/7168 Train_loss 2.1677535120284257 
Epoch [5/10] Batch 4200/7168 Train_loss 2.16768000908932 
Epoch [5/10] Batch 4300/7168 Train_loss 2.1684013074447988 
Epoch [5/10] Batch 4400/7168 Train_loss 2.1672674032659103 
Epoch [5/10] Batch 4500/7168 Train_loss 2.167057831488406 
Epoch [5/10] Batch 4600/7168 Train_loss 2.168017465380761 
Epoch [5/10] Batch 4700/7168 Train_loss 2.170037066136439 
Epoch [5/10] Batch 4800/7168 Train_loss 2.169566958618149 
Epoch [5/10] Batch 4900/7168 Train_loss 2.1697088914747216 
Epoch [5/10] Batch 5000/7168 Train_loss 2.1724786770758544 
Epoch [5/10] Batch 5100/7168 Train_loss 2.1739234209317737 
Epoch [5/10] Batch 5200/7168 Train_loss 2.1724980183259452 
Epoch [5/10] Batch 5300/7168 Train_loss 2.173412004575327 
Epoch [5/10] Batch 5400/7168 Train_loss 2.1742741788560385 
Epoch [5/10] Batch 5500/7168 Train_loss 2.1751317191023194 
Epoch [5/10] Batch 5600/7168 Train_loss 2.1772064143681056 
Epoch [5/10] Batch 5700/7168 Train_loss 2.177516807277795 
Epoch [5/10] Batch 5800/7168 Train_loss 2.17695747649047 
Epoch [5/10] Batch 5900/7168 Train_loss 2.17615167091578 
Epoch [5/10] Batch 6000/7168 Train_loss 2.175633215200025 
Epoch [5/10] Batch 6100/7168 Train_loss 2.1735763778292414 
Epoch [5/10] Batch 6200/7168 Train_loss 2.1747203766722927 
Epoch [5/10] Batch 6300/7168 Train_loss 2.174772342161193 
Epoch [5/10] Batch 6400/7168 Train_loss 2.1751443378265307 
Epoch [5/10] Batch 6500/7168 Train_loss 2.175721713401798 
Epoch [5/10] Batch 6600/7168 Train_loss 2.1760929858746842 
Epoch [5/10] Batch 6700/7168 Train_loss 2.1764083513416046 
Epoch [5/10] Batch 6800/7168 Train_loss 2.177400583730798 
Epoch [5/10] Batch 6900/7168 Train_loss 2.1770883828764322 
Epoch [5/10] Batch 7000/7168 Train_loss 2.176818214415431 
Epoch [5/10] Batch 7100/7168 Train_loss 2.1777522934312437 
Epoch: 5/10 	Training Loss: 2.178334 	Validation Loss: 2.177010 Duration seconds: 920.4451529979706 
best_valid_loss_fold [2.175485256435682] Best_Epoch [5]Epoch [6/10] Batch 0/7168 Train_loss 2.929154634475708 
Epoch [6/10] Batch 100/7168 Train_loss 2.1330041327688956 
Epoch [6/10] Batch 200/7168 Train_loss 2.125226982419764 
Epoch [6/10] Batch 300/7168 Train_loss 2.1612908864437146 
Epoch [6/10] Batch 400/7168 Train_loss 2.156315307107352 
Epoch [6/10] Batch 500/7168 Train_loss 2.162708402007164 
Epoch [6/10] Batch 600/7168 Train_loss 2.1533280179623557 
Epoch [6/10] Batch 700/7168 Train_loss 2.155728752534842 
Epoch [6/10] Batch 800/7168 Train_loss 2.149814015414831 
Epoch [6/10] Batch 900/7168 Train_loss 2.154714189031545 
Epoch [6/10] Batch 1000/7168 Train_loss 2.154891573778399 
Epoch [6/10] Batch 1100/7168 Train_loss 2.1618567493750116 
Epoch [6/10] Batch 1200/7168 Train_loss 2.158914033593385 
Epoch [6/10] Batch 1300/7168 Train_loss 2.1694684376815574 
Epoch [6/10] Batch 1400/7168 Train_loss 2.167243397582607 
Epoch [6/10] Batch 1500/7168 Train_loss 2.1655394671004586 
Epoch [6/10] Batch 1600/7168 Train_loss 2.1637757053641806 
Epoch [6/10] Batch 1700/7168 Train_loss 2.1660442603888894 
Epoch [6/10] Batch 1800/7168 Train_loss 2.1641554750878567 
Epoch [6/10] Batch 1900/7168 Train_loss 2.1684641939348324 
Epoch [6/10] Batch 2000/7168 Train_loss 2.166752920061097 
Epoch [6/10] Batch 2100/7168 Train_loss 2.1674291067878615 
Epoch [6/10] Batch 2200/7168 Train_loss 2.1674173490831063 
Epoch [6/10] Batch 2300/7168 Train_loss 2.166093988736678 
Epoch [6/10] Batch 2400/7168 Train_loss 2.1678754487158307 
Epoch [6/10] Batch 2500/7168 Train_loss 2.168344634591413 
Epoch [6/10] Batch 2600/7168 Train_loss 2.169075571673735 
Epoch [6/10] Batch 2700/7168 Train_loss 2.170005253082026 
Epoch [6/10] Batch 2800/7168 Train_loss 2.1682738634315144 
Epoch [6/10] Batch 2900/7168 Train_loss 2.1666287301542924 
Epoch [6/10] Batch 3000/7168 Train_loss 2.16895088426811 
Epoch [6/10] Batch 3100/7168 Train_loss 2.1686700066465674 
Epoch [6/10] Batch 3200/7168 Train_loss 2.171328062100248 
Epoch [6/10] Batch 3300/7168 Train_loss 2.1713696635291275 
Epoch [6/10] Batch 3400/7168 Train_loss 2.1727375246713665 
Epoch [6/10] Batch 3500/7168 Train_loss 2.1715675631282125 
Epoch [6/10] Batch 3600/7168 Train_loss 2.174363624955243 
Epoch [6/10] Batch 3700/7168 Train_loss 2.1747224000064915 
Epoch [6/10] Batch 3800/7168 Train_loss 2.1739319982818164 
Epoch [6/10] Batch 3900/7168 Train_loss 2.173773431985753 
Epoch [6/10] Batch 4000/7168 Train_loss 2.1748021571070395 
Epoch [6/10] Batch 4100/7168 Train_loss 2.175181643922455 
Epoch [6/10] Batch 4200/7168 Train_loss 2.1742400570983857 
Epoch [6/10] Batch 4300/7168 Train_loss 2.176014306679822 
Epoch [6/10] Batch 4400/7168 Train_loss 2.176148682456888 
Epoch [6/10] Batch 4500/7168 Train_loss 2.1779955397497623 
Epoch [6/10] Batch 4600/7168 Train_loss 2.1769031322156573 
Epoch [6/10] Batch 4700/7168 Train_loss 2.178540897988127 
Epoch [6/10] Batch 4800/7168 Train_loss 2.1779651103405078 
Epoch [6/10] Batch 4900/7168 Train_loss 2.1784401609748754 
Epoch [6/10] Batch 5000/7168 Train_loss 2.1809392664235157 
Epoch [6/10] Batch 5100/7168 Train_loss 2.180622531441795 
Epoch [6/10] Batch 5200/7168 Train_loss 2.180785572665216 
Epoch [6/10] Batch 5300/7168 Train_loss 2.181116617531557 
Epoch [6/10] Batch 5400/7168 Train_loss 2.181024761017639 
Epoch [6/10] Batch 5500/7168 Train_loss 2.1804911376990703 
Epoch [6/10] Batch 5600/7168 Train_loss 2.180953923785097 
Epoch [6/10] Batch 5700/7168 Train_loss 2.1789984568271317 
Epoch [6/10] Batch 5800/7168 Train_loss 2.1780764803590906 
Epoch [6/10] Batch 5900/7168 Train_loss 2.176591169150315 
Epoch [6/10] Batch 6000/7168 Train_loss 2.1769605649250625 
Epoch [6/10] Batch 6100/7168 Train_loss 2.177401157961558 
Epoch [6/10] Batch 6200/7168 Train_loss 2.1768401033420597 
Epoch [6/10] Batch 6300/7168 Train_loss 2.177209121448107 
Epoch [6/10] Batch 6400/7168 Train_loss 2.1773052676974416 
Epoch [6/10] Batch 6500/7168 Train_loss 2.176561604424855 
Epoch [6/10] Batch 6600/7168 Train_loss 2.1775536788487826 
Epoch [6/10] Batch 6700/7168 Train_loss 2.177033888946325 
Epoch [6/10] Batch 6800/7168 Train_loss 2.1787034274747032 
Epoch [6/10] Batch 6900/7168 Train_loss 2.1785551654098 
Epoch [6/10] Batch 7000/7168 Train_loss 2.178672139011797 
Epoch [6/10] Batch 7100/7168 Train_loss 2.1787030366298565 
Epoch: 6/10 	Training Loss: 2.178311 	Validation Loss: 2.177244 Duration seconds: 892.6094923019409 
best_valid_loss_fold [2.175485256435682] Best_Epoch [6]Epoch [7/10] Batch 0/7168 Train_loss 1.8335115909576416 
Epoch [7/10] Batch 100/7168 Train_loss 2.090694129024402 
Epoch [7/10] Batch 200/7168 Train_loss 2.1400919549204223 
Epoch [7/10] Batch 300/7168 Train_loss 2.1491784872208717 
Epoch [7/10] Batch 400/7168 Train_loss 2.171620562523975 
Epoch [7/10] Batch 500/7168 Train_loss 2.169581665369327 
Epoch [7/10] Batch 600/7168 Train_loss 2.1768501776575846 
Epoch [7/10] Batch 700/7168 Train_loss 2.188804812208562 
Epoch [7/10] Batch 800/7168 Train_loss 2.187486598256524 
Epoch [7/10] Batch 900/7168 Train_loss 2.1837772524085612 
Epoch [7/10] Batch 1000/7168 Train_loss 2.175784490920685 
Epoch [7/10] Batch 1100/7168 Train_loss 2.1774727838971635 
Epoch [7/10] Batch 1200/7168 Train_loss 2.1794335496341257 
Epoch [7/10] Batch 1300/7168 Train_loss 2.18071748249857 
Epoch [7/10] Batch 1400/7168 Train_loss 2.1859418628451484 
Epoch [7/10] Batch 1500/7168 Train_loss 2.1816156131477853 
Epoch [7/10] Batch 1600/7168 Train_loss 2.1744725523257094 
Epoch [7/10] Batch 1700/7168 Train_loss 2.1760869780878402 
Epoch [7/10] Batch 1800/7168 Train_loss 2.175618801752042 
Epoch [7/10] Batch 1900/7168 Train_loss 2.1736765206095296 
Epoch [7/10] Batch 2000/7168 Train_loss 2.175416265954559 
Epoch [7/10] Batch 2100/7168 Train_loss 2.178881199572099 
Epoch [7/10] Batch 2200/7168 Train_loss 2.1748737462105288 
Epoch [7/10] Batch 2300/7168 Train_loss 2.1740596430916104 
Epoch [7/10] Batch 2400/7168 Train_loss 2.1767951415906794 
Epoch [7/10] Batch 2500/7168 Train_loss 2.1757511780267715 
Epoch [7/10] Batch 2600/7168 Train_loss 2.177721030239599 
Epoch [7/10] Batch 2700/7168 Train_loss 2.178171807303556 
Epoch [7/10] Batch 2800/7168 Train_loss 2.1767135381283653 
Epoch [7/10] Batch 2900/7168 Train_loss 2.176354970321989 
Epoch [7/10] Batch 3000/7168 Train_loss 2.1795267528571594 
Epoch [7/10] Batch 3100/7168 Train_loss 2.184203185263621 
Epoch [7/10] Batch 3200/7168 Train_loss 2.1849193633701307 
Epoch [7/10] Batch 3300/7168 Train_loss 2.18442509807951 
Epoch [7/10] Batch 3400/7168 Train_loss 2.184614480172701 
Epoch [7/10] Batch 3500/7168 Train_loss 2.183940401037432 
Epoch [7/10] Batch 3600/7168 Train_loss 2.1843153054980364 
Epoch [7/10] Batch 3700/7168 Train_loss 2.1839313608863744 
Epoch [7/10] Batch 3800/7168 Train_loss 2.18280728221316 
Epoch [7/10] Batch 3900/7168 Train_loss 2.180128049939084 
Epoch [7/10] Batch 4000/7168 Train_loss 2.1798244225765817 
Epoch [7/10] Batch 4100/7168 Train_loss 2.17849662576949 
Epoch [7/10] Batch 4200/7168 Train_loss 2.180139158693446 
Epoch [7/10] Batch 4300/7168 Train_loss 2.1801228657056497 
Epoch [7/10] Batch 4400/7168 Train_loss 2.17855226372933 
Epoch [7/10] Batch 4500/7168 Train_loss 2.178735421766469 
Epoch [7/10] Batch 4600/7168 Train_loss 2.1786879967526493 
Epoch [7/10] Batch 4700/7168 Train_loss 2.177752990198754 
Epoch [7/10] Batch 4800/7168 Train_loss 2.1781834356146934 
Epoch [7/10] Batch 4900/7168 Train_loss 2.1777726446085137 
Epoch [7/10] Batch 5000/7168 Train_loss 2.1774811117536093 
Epoch [7/10] Batch 5100/7168 Train_loss 2.1775411918300716 
Epoch [7/10] Batch 5200/7168 Train_loss 2.1766343283334666 
Epoch [7/10] Batch 5300/7168 Train_loss 2.1763553111547793 
Epoch [7/10] Batch 5400/7168 Train_loss 2.1760652188354457 
Epoch [7/10] Batch 5500/7168 Train_loss 2.17637949096812 
Epoch [7/10] Batch 5600/7168 Train_loss 2.17541478908162 
Epoch [7/10] Batch 5700/7168 Train_loss 2.1768542577918844 
Epoch [7/10] Batch 5800/7168 Train_loss 2.176799297247996 
Epoch [7/10] Batch 5900/7168 Train_loss 2.177481030702207 
Epoch [7/10] Batch 6000/7168 Train_loss 2.1771378062666704 
Epoch [7/10] Batch 6100/7168 Train_loss 2.1791157634570704 
Epoch [7/10] Batch 6200/7168 Train_loss 2.17891108736677 
Epoch [7/10] Batch 6300/7168 Train_loss 2.1792425497848447 
Epoch [7/10] Batch 6400/7168 Train_loss 2.179725572113003 
Epoch [7/10] Batch 6500/7168 Train_loss 2.1800046881846766 
Epoch [7/10] Batch 6600/7168 Train_loss 2.1801050601154905 
Epoch [7/10] Batch 6700/7168 Train_loss 2.179218340760166 
Epoch [7/10] Batch 6800/7168 Train_loss 2.178407974312225 
Epoch [7/10] Batch 6900/7168 Train_loss 2.1782238946636765 
Epoch [7/10] Batch 7000/7168 Train_loss 2.178690519075856 
Epoch [7/10] Batch 7100/7168 Train_loss 2.1782684473483735 
Epoch: 7/10 	Training Loss: 2.178322 	Validation Loss: 2.175970 Duration seconds: 895.390908241272 
best_valid_loss_fold [2.175485256435682] Best_Epoch [7]Epoch [8/10] Batch 0/7168 Train_loss 2.696933150291443 
Epoch [8/10] Batch 100/7168 Train_loss 2.1738556647064664 
Epoch [8/10] Batch 200/7168 Train_loss 2.1886302123022316 
Epoch [8/10] Batch 300/7168 Train_loss 2.1700451278013246 
Epoch [8/10] Batch 400/7168 Train_loss 2.175888642668724 
Epoch [8/10] Batch 500/7168 Train_loss 2.1891776188286003 
Epoch [8/10] Batch 600/7168 Train_loss 2.2017883374270504 
Epoch [8/10] Batch 700/7168 Train_loss 2.197428052439839 
Epoch [8/10] Batch 800/7168 Train_loss 2.198830871155646 
Epoch [8/10] Batch 900/7168 Train_loss 2.189854625757209 
Epoch [8/10] Batch 1000/7168 Train_loss 2.1802511101181095 
Epoch [8/10] Batch 1100/7168 Train_loss 2.1868831939744906 
Epoch [8/10] Batch 1200/7168 Train_loss 2.186420269056125 
Epoch [8/10] Batch 1300/7168 Train_loss 2.180485917594596 
Epoch [8/10] Batch 1400/7168 Train_loss 2.181169352122582 
Epoch [8/10] Batch 1500/7168 Train_loss 2.1805107108578534 
Epoch [8/10] Batch 1600/7168 Train_loss 2.1828064254527537 
Epoch [8/10] Batch 1700/7168 Train_loss 2.187318205833435 
Epoch [8/10] Batch 1800/7168 Train_loss 2.183436743919218 
Epoch [8/10] Batch 1900/7168 Train_loss 2.1789993776468024 
Epoch [8/10] Batch 2000/7168 Train_loss 2.1726445801209833 
Epoch [8/10] Batch 2100/7168 Train_loss 2.174729284737747 
Epoch [8/10] Batch 2200/7168 Train_loss 2.1767185978770853 
Epoch [8/10] Batch 2300/7168 Train_loss 2.1759280251231417 
Epoch [8/10] Batch 2400/7168 Train_loss 2.1757549707502783 
Epoch [8/10] Batch 2500/7168 Train_loss 2.176977324782491 
Epoch [8/10] Batch 2600/7168 Train_loss 2.1771975773107726 
Epoch [8/10] Batch 2700/7168 Train_loss 2.1812233403849186 
Epoch [8/10] Batch 2800/7168 Train_loss 2.1819400132468494 
Epoch [8/10] Batch 2900/7168 Train_loss 2.179226756085613 
Epoch [8/10] Batch 3000/7168 Train_loss 2.1767215950862044 
Epoch [8/10] Batch 3100/7168 Train_loss 2.1752181503608203 
Epoch [8/10] Batch 3200/7168 Train_loss 2.1764738954573413 
Epoch [8/10] Batch 3300/7168 Train_loss 2.1779245138845456 
Epoch [8/10] Batch 3400/7168 Train_loss 2.178923735848002 
Epoch [8/10] Batch 3500/7168 Train_loss 2.1793442800262253 
Epoch [8/10] Batch 3600/7168 Train_loss 2.1774403625513177 
Epoch [8/10] Batch 3700/7168 Train_loss 2.1772265572365606 
Epoch [8/10] Batch 3800/7168 Train_loss 2.1775812647062613 
Epoch [8/10] Batch 3900/7168 Train_loss 2.177825217765284 
Epoch [8/10] Batch 4000/7168 Train_loss 2.1772233766783655 
Epoch [8/10] Batch 4100/7168 Train_loss 2.1768641126676176 
Epoch [8/10] Batch 4200/7168 Train_loss 2.175729064250015 
Epoch [8/10] Batch 4300/7168 Train_loss 2.1766412465503344 
Epoch [8/10] Batch 4400/7168 Train_loss 2.1777123195038577 
Epoch [8/10] Batch 4500/7168 Train_loss 2.1786069098948584 
Epoch [8/10] Batch 4600/7168 Train_loss 2.179700833245532 
Epoch [8/10] Batch 4700/7168 Train_loss 2.1805413715244986 
Epoch [8/10] Batch 4800/7168 Train_loss 2.1791131487490607 
Epoch [8/10] Batch 4900/7168 Train_loss 2.1791704377424375 
Epoch [8/10] Batch 5000/7168 Train_loss 2.1768386047605373 
Epoch [8/10] Batch 5100/7168 Train_loss 2.1793008020662183 
Epoch [8/10] Batch 5200/7168 Train_loss 2.1792029028436444 
Epoch [8/10] Batch 5300/7168 Train_loss 2.179948520677829 
Epoch [8/10] Batch 5400/7168 Train_loss 2.1790903281379777 
Epoch [8/10] Batch 5500/7168 Train_loss 2.1792110063405152 
Epoch [8/10] Batch 5600/7168 Train_loss 2.179891386695726 
Epoch [8/10] Batch 5700/7168 Train_loss 2.1776852416293786 
Epoch [8/10] Batch 5800/7168 Train_loss 2.1772154721283252 
Epoch [8/10] Batch 5900/7168 Train_loss 2.1761110994426343 
Epoch [8/10] Batch 6000/7168 Train_loss 2.1769087605328585 
Epoch [8/10] Batch 6100/7168 Train_loss 2.1769932763969444 
Epoch [8/10] Batch 6200/7168 Train_loss 2.1768169188558084 
Epoch [8/10] Batch 6300/7168 Train_loss 2.1772441191344845 
Epoch [8/10] Batch 6400/7168 Train_loss 2.1776006856857286 
Epoch [8/10] Batch 6500/7168 Train_loss 2.178725687985493 
Epoch [8/10] Batch 6600/7168 Train_loss 2.177450527473573 
Epoch [8/10] Batch 6700/7168 Train_loss 2.17750410618417 
Epoch [8/10] Batch 6800/7168 Train_loss 2.177505137527836 
Epoch [8/10] Batch 6900/7168 Train_loss 2.1770790257634087 
Epoch [8/10] Batch 7000/7168 Train_loss 2.177584760966956 
Epoch [8/10] Batch 7100/7168 Train_loss 2.178302910884507 
Epoch: 8/10 	Training Loss: 2.178278 	Validation Loss: 2.175204 Duration seconds: 877.6897122859955 
Validation loss decreased (2.175485 --> 2.175204).  Saving model ... 
best_valid_loss_fold [2.1752041932611195] Best_Epoch [8]Epoch [9/10] Batch 0/7168 Train_loss 2.6447099447250366 
Epoch [9/10] Batch 100/7168 Train_loss 2.173270920273101 
Epoch [9/10] Batch 200/7168 Train_loss 2.2125574523833262 
Epoch [9/10] Batch 300/7168 Train_loss 2.2071268409490585 
Epoch [9/10] Batch 400/7168 Train_loss 2.2084913417659795 
Epoch [9/10] Batch 500/7168 Train_loss 2.2016392892111325 
Epoch [9/10] Batch 600/7168 Train_loss 2.1905187285581165 
Epoch [9/10] Batch 700/7168 Train_loss 2.1923932215244046 
Epoch [9/10] Batch 800/7168 Train_loss 2.189693250571297 
Epoch [9/10] Batch 900/7168 Train_loss 2.181710639387601 
Epoch [9/10] Batch 1000/7168 Train_loss 2.1773691490277662 
Epoch [9/10] Batch 1100/7168 Train_loss 2.1755923229954006 
Epoch [9/10] Batch 1200/7168 Train_loss 2.1780226199727175 
Epoch [9/10] Batch 1300/7168 Train_loss 2.1797696356128307 
Epoch [9/10] Batch 1400/7168 Train_loss 2.1801032614678166 
Epoch [9/10] Batch 1500/7168 Train_loss 2.1769971407448585 
Epoch [9/10] Batch 1600/7168 Train_loss 2.177971733279112 
Epoch [9/10] Batch 1700/7168 Train_loss 2.179138870457563 
Epoch [9/10] Batch 1800/7168 Train_loss 2.1798357775828894 
Epoch [9/10] Batch 1900/7168 Train_loss 2.182528314732803 
Epoch [9/10] Batch 2000/7168 Train_loss 2.181229612109126 
Epoch [9/10] Batch 2100/7168 Train_loss 2.1777618681034094 
Epoch [9/10] Batch 2200/7168 Train_loss 2.176358986549462 
Epoch [9/10] Batch 2300/7168 Train_loss 2.177572720455429 
Epoch [9/10] Batch 2400/7168 Train_loss 2.179059997256822 
Epoch [9/10] Batch 2500/7168 Train_loss 2.1814977573078185 
Epoch [9/10] Batch 2600/7168 Train_loss 2.1809702321331303 
Epoch [9/10] Batch 2700/7168 Train_loss 2.1809617287359693 
Epoch [9/10] Batch 2800/7168 Train_loss 2.180316881068857 
Epoch [9/10] Batch 2900/7168 Train_loss 2.1831091948467383 
Epoch [9/10] Batch 3000/7168 Train_loss 2.1833031415760575 
Epoch [9/10] Batch 3100/7168 Train_loss 2.18541006091832 
Epoch [9/10] Batch 3200/7168 Train_loss 2.1845422716103133 
Epoch [9/10] Batch 3300/7168 Train_loss 2.1826191130270565 
Epoch [9/10] Batch 3400/7168 Train_loss 2.1830079161228766 
Epoch [9/10] Batch 3500/7168 Train_loss 2.1829053306921793 
Epoch [9/10] Batch 3600/7168 Train_loss 2.185714492319326 
Epoch [9/10] Batch 3700/7168 Train_loss 2.1853096669419525 
Epoch [9/10] Batch 3800/7168 Train_loss 2.184996858878187 
Epoch [9/10] Batch 3900/7168 Train_loss 2.1876392851950603 
Epoch [9/10] Batch 4000/7168 Train_loss 2.188305612322957 
Epoch [9/10] Batch 4100/7168 Train_loss 2.1879394066657882 
Epoch [9/10] Batch 4200/7168 Train_loss 2.1873982797291016 
Epoch [9/10] Batch 4300/7168 Train_loss 2.186788075145137 
Epoch [9/10] Batch 4400/7168 Train_loss 2.184123163079977 
Epoch [9/10] Batch 4500/7168 Train_loss 2.1821247268997017 
Epoch [9/10] Batch 4600/7168 Train_loss 2.1817405121892826 
Epoch [9/10] Batch 4700/7168 Train_loss 2.181138900480887 
Epoch [9/10] Batch 4800/7168 Train_loss 2.182691850821889 
Epoch [9/10] Batch 4900/7168 Train_loss 2.18087713012365 
Epoch [9/10] Batch 5000/7168 Train_loss 2.1793899284383342 
Epoch [9/10] Batch 5100/7168 Train_loss 2.179311876570657 
Epoch [9/10] Batch 5200/7168 Train_loss 2.1797988608900662 
Epoch [9/10] Batch 5300/7168 Train_loss 2.179165999282983 
Epoch [9/10] Batch 5400/7168 Train_loss 2.1791843561480784 
Epoch [9/10] Batch 5500/7168 Train_loss 2.1814674180873936 
Epoch [9/10] Batch 5600/7168 Train_loss 2.1824957647854206 
Epoch [9/10] Batch 5700/7168 Train_loss 2.1818224947335656 
Epoch [9/10] Batch 5800/7168 Train_loss 2.183481211526972 
Epoch [9/10] Batch 5900/7168 Train_loss 2.184133840956197 
Epoch [9/10] Batch 6000/7168 Train_loss 2.183225634018196 
Epoch [9/10] Batch 6100/7168 Train_loss 2.1845104229672976 
Epoch [9/10] Batch 6200/7168 Train_loss 2.1828708476889345 
Epoch [9/10] Batch 6300/7168 Train_loss 2.1812406228167958 
Epoch [9/10] Batch 6400/7168 Train_loss 2.1815412327672816 
Epoch [9/10] Batch 6500/7168 Train_loss 2.181016631061124 
Epoch [9/10] Batch 6600/7168 Train_loss 2.1805640454410407 
Epoch [9/10] Batch 6700/7168 Train_loss 2.1795998113984547 
Epoch [9/10] Batch 6800/7168 Train_loss 2.179070359081963 
Epoch [9/10] Batch 6900/7168 Train_loss 2.179033921872545 
Epoch [9/10] Batch 7000/7168 Train_loss 2.1790758984895318 
Epoch [9/10] Batch 7100/7168 Train_loss 2.1796359913548256 
Epoch: 9/10 	Training Loss: 2.178297 	Validation Loss: 2.177276 Duration seconds: 954.8455741405487 
best_valid_loss_fold [2.1752041932611195] Best_Epoch [9]
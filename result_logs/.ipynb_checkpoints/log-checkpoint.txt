Fold: 1/5 
Epoch [0/10] Batch 0/7568 Train_loss 7.484314441680908 
Epoch [0/10] Batch 100/7568 Train_loss 2.4979103157425873 
Epoch [0/10] Batch 200/7568 Train_loss 2.3015196677019345 
Epoch [0/10] Batch 300/7568 Train_loss 2.2624092123338153 
Epoch [0/10] Batch 400/7568 Train_loss 2.2222697918774776 
Epoch [0/10] Batch 500/7568 Train_loss 2.207026704253551 
Epoch [0/10] Batch 600/7568 Train_loss 2.1758395375160133 
Epoch [0/10] Batch 700/7568 Train_loss 2.1575284540950825 
Epoch [0/10] Batch 800/7568 Train_loss 2.141421970524145 
Epoch [0/10] Batch 900/7568 Train_loss 2.1437653710654256 
Epoch [0/10] Batch 1000/7568 Train_loss 2.1345809395318027 
Epoch [0/10] Batch 1100/7568 Train_loss 2.131298351677627 
Epoch [0/10] Batch 1200/7568 Train_loss 2.133918977299896 
Epoch [0/10] Batch 1300/7568 Train_loss 2.1355595842267254 
Epoch [0/10] Batch 1400/7568 Train_loss 2.1387538130393122 
Epoch [0/10] Batch 1500/7568 Train_loss 2.1369453011791677 
Epoch [0/10] Batch 1600/7568 Train_loss 2.1407137363032205 
Epoch [0/10] Batch 1700/7568 Train_loss 2.1361946777551752 
Epoch [0/10] Batch 1800/7568 Train_loss 2.132046047447126 
Epoch [0/10] Batch 1900/7568 Train_loss 2.1304157632611287 
Epoch [0/10] Batch 2000/7568 Train_loss 2.1338164347639563 
Epoch [0/10] Batch 2100/7568 Train_loss 2.128147926821645 
Epoch [0/10] Batch 2200/7568 Train_loss 2.128663962941341 
Epoch [0/10] Batch 2300/7568 Train_loss 2.126810712750349 
Epoch [0/10] Batch 2400/7568 Train_loss 2.123711770249128 
Epoch [0/10] Batch 2500/7568 Train_loss 2.124835977026197 
Epoch [0/10] Batch 2600/7568 Train_loss 2.1214270435906704 
Epoch [0/10] Batch 2700/7568 Train_loss 2.117084234384685 
Epoch [0/10] Batch 2800/7568 Train_loss 2.1118593524765092 
Epoch [0/10] Batch 2900/7568 Train_loss 2.1117647694631185 
Epoch [0/10] Batch 3000/7568 Train_loss 2.1110464336076844 
Epoch [0/10] Batch 3200/7568 Train_loss 2.1089216051009325 
Epoch [0/10] Batch 3300/7568 Train_loss 2.1055431461531047 
Epoch [0/10] Batch 3400/7568 Train_loss 2.1044037594046183 
Epoch [0/10] Batch 3500/7568 Train_loss 2.1022174929813193 
Epoch [0/10] Batch 3600/7568 Train_loss 2.0989574793435506 
Epoch [0/10] Batch 3700/7568 Train_loss 2.0974740273676766 
Epoch [0/10] Batch 3800/7568 Train_loss 2.0961681755161137 
Epoch [0/10] Batch 3900/7568 Train_loss 2.0941589749973266 
Epoch [0/10] Batch 4000/7568 Train_loss 2.0938140865423716 
Epoch [0/10] Batch 4100/7568 Train_loss 2.092149762649764 
Epoch [0/10] Batch 4200/7568 Train_loss 2.0898605287926335 
Epoch [0/10] Batch 4300/7568 Train_loss 2.088918649500344 
Epoch [0/10] Batch 4400/7568 Train_loss 2.0882191409125594 
Epoch [0/10] Batch 4500/7568 Train_loss 2.0876880697914295 
Epoch [0/10] Batch 4600/7568 Train_loss 2.0873262927865652 
Epoch [0/10] Batch 4700/7568 Train_loss 2.0847683215751163 
Epoch [0/10] Batch 4800/7568 Train_loss 2.0816076453018626 
Epoch [0/10] Batch 4900/7568 Train_loss 2.0800344698424484 
Epoch [0/10] Batch 5000/7568 Train_loss 2.0760135974908587 
Epoch [0/10] Batch 5100/7568 Train_loss 2.0756532280901046 
Epoch [0/10] Batch 5200/7568 Train_loss 2.075598669814802 
Epoch [0/10] Batch 5300/7568 Train_loss 2.074867706001648 
Epoch [0/10] Batch 5400/7568 Train_loss 2.074554996374271 
Epoch [0/10] Batch 5500/7568 Train_loss 2.073276837228905 
Epoch [0/10] Batch 5600/7568 Train_loss 2.0737121534217704 
Epoch [0/10] Batch 5700/7568 Train_loss 2.0735061360522122 
Epoch [0/10] Batch 5800/7568 Train_loss 2.073598464161358 
Epoch [0/10] Batch 5900/7568 Train_loss 2.073349469855688 
Epoch [0/10] Batch 6000/7568 Train_loss 2.0705458589880252 
Epoch [0/10] Batch 6100/7568 Train_loss 2.069041839858935 
Epoch [0/10] Batch 6200/7568 Train_loss 2.0680859397302 
Epoch [0/10] Batch 6300/7568 Train_loss 2.0661852338393136 
Epoch [0/10] Batch 6400/7568 Train_loss 2.0663272585254817 
Epoch [0/10] Batch 6500/7568 Train_loss 2.0660881248873357 
Epoch [0/10] Batch 6600/7568 Train_loss 2.0649782799487943 
Epoch [0/10] Batch 6700/7568 Train_loss 2.0645989420151216 
Epoch [0/10] Batch 6800/7568 Train_loss 2.0637254506117313 
Epoch [0/10] Batch 6900/7568 Train_loss 2.0634699786569635 
Epoch [0/10] Batch 7000/7568 Train_loss 2.062437978664001 
Epoch [0/10] Batch 7100/7568 Train_loss 2.0604266924449144 
Epoch [0/10] Batch 7200/7568 Train_loss 2.0596205587064933 
Epoch [0/10] Batch 7300/7568 Train_loss 2.0588619510060613 
Epoch [0/10] Batch 7400/7568 Train_loss 2.056979115636119 
Epoch [0/10] Batch 7500/7568 Train_loss 2.056115907206565 
Epoch: 0/10 	Training Loss: 2.055798 	Validation Loss: 1.967044 Duration seconds: 994.5787484645844 
Validation loss decreased (inf --> 1.967044).  Saving model ... 
best_valid_loss_fold [1.9670444365795583] Best_Epoch [0]Epoch [1/10] Batch 0/7568 Train_loss 1.9221429526805878 
Epoch [1/10] Batch 100/7568 Train_loss 2.039114627802726 
Epoch [1/10] Batch 200/7568 Train_loss 1.9967949520147854 
Epoch [1/10] Batch 300/7568 Train_loss 2.0247454983352426 
Epoch [1/10] Batch 400/7568 Train_loss 1.9963760924829808 
Epoch [1/10] Batch 500/7568 Train_loss 1.9925726996270006 
Epoch [1/10] Batch 600/7568 Train_loss 1.9852651550299714 
Epoch [1/10] Batch 700/7568 Train_loss 1.9743458730050398 
Epoch [1/10] Batch 800/7568 Train_loss 1.9744626103380647 
Epoch [1/10] Batch 900/7568 Train_loss 1.9659755668384782 
Epoch [1/10] Batch 1000/7568 Train_loss 1.9685145335776226 
Epoch [1/10] Batch 1100/7568 Train_loss 1.9690259550778462 
Epoch [1/10] Batch 1200/7568 Train_loss 1.9635192028787314 
Epoch [1/10] Batch 1300/7568 Train_loss 1.9633091107686604 
Epoch [1/10] Batch 1400/7568 Train_loss 1.971836652152033 
Epoch [1/10] Batch 1500/7568 Train_loss 1.9674011749735998 
Epoch [1/10] Batch 1600/7568 Train_loss 1.9717479547454297 
Epoch [1/10] Batch 1700/7568 Train_loss 1.9742402114162438 
Epoch [1/10] Batch 1800/7568 Train_loss 1.9739258936764068 
Epoch [1/10] Batch 1900/7568 Train_loss 1.974669673971475 
Epoch [1/10] Batch 2000/7568 Train_loss 1.9757148272347176 
Epoch [1/10] Batch 2100/7568 Train_loss 1.9733690670817765 
Epoch [1/10] Batch 2200/7568 Train_loss 1.9730265459205714 
Epoch [1/10] Batch 2300/7568 Train_loss 1.9743946604576383 
Epoch [1/10] Batch 2400/7568 Train_loss 1.9781866585560413 
Epoch [1/10] Batch 2500/7568 Train_loss 1.9787609633482823 
Epoch [1/10] Batch 2600/7568 Train_loss 1.9793943795262736 
Epoch [1/10] Batch 2700/7568 Train_loss 1.9805217040540641 
Epoch [1/10] Batch 2800/7568 Train_loss 1.9815524296701401 
Epoch [1/10] Batch 2900/7568 Train_loss 1.983306217053889 
Epoch [1/10] Batch 3000/7568 Train_loss 1.9835712553878022 
Epoch [1/10] Batch 3100/7568 Train_loss 1.9811978437407713 
Epoch [1/10] Batch 3200/7568 Train_loss 1.982422480263661 
Epoch [1/10] Batch 3300/7568 Train_loss 1.9820809824760666 
Epoch [1/10] Batch 3400/7568 Train_loss 1.9807306181346973 
Epoch [1/10] Batch 3500/7568 Train_loss 1.9813088849463554 
Epoch [1/10] Batch 3600/7568 Train_loss 1.9835688514954115 
Epoch [1/10] Batch 3700/7568 Train_loss 1.9824561538107812 
Epoch [1/10] Batch 3800/7568 Train_loss 1.9837583755543595 
Epoch [1/10] Batch 3900/7568 Train_loss 1.9815564330144895 
Epoch [1/10] Batch 4000/7568 Train_loss 1.9816588582664214 
Epoch [1/10] Batch 4100/7568 Train_loss 1.9809510714722622 
Epoch [1/10] Batch 4200/7568 Train_loss 1.9795196892867002 
Epoch [1/10] Batch 4300/7568 Train_loss 1.9801388242665459 
Epoch [1/10] Batch 4400/7568 Train_loss 1.9797011177205575 
Epoch [1/10] Batch 4500/7568 Train_loss 1.9783058851771926 
Epoch [1/10] Batch 4600/7568 Train_loss 1.9785177250532606 
Epoch [1/10] Batch 4700/7568 Train_loss 1.9793288725579046 
Epoch [1/10] Batch 4800/7568 Train_loss 1.9787456280047684 
Epoch [1/10] Batch 4900/7568 Train_loss 1.976109567958952 
Epoch [1/10] Batch 5000/7568 Train_loss 1.9766362239374826 
Epoch [1/10] Batch 5100/7568 Train_loss 1.975909988261676 
Epoch [1/10] Batch 5200/7568 Train_loss 1.9768346606011895 
Epoch [1/10] Batch 5300/7568 Train_loss 1.976521663104919 
Epoch [1/10] Batch 5400/7568 Train_loss 1.9769643670352515 
Epoch [1/10] Batch 5500/7568 Train_loss 1.9753468227487243 
Epoch [1/10] Batch 5600/7568 Train_loss 1.975932861586018 
Epoch [1/10] Batch 5700/7568 Train_loss 1.9736436900575418 
Epoch [1/10] Batch 5800/7568 Train_loss 1.9735178642053273 
Epoch [1/10] Batch 5900/7568 Train_loss 1.9742854432703336 
Epoch [1/10] Batch 6000/7568 Train_loss 1.9747428844455202 
Epoch [1/10] Batch 6100/7568 Train_loss 1.9731494194512444 
Epoch [1/10] Batch 6200/7568 Train_loss 1.9727758619645357 
Epoch [1/10] Batch 6300/7568 Train_loss 1.9721751237503027 
Epoch [1/10] Batch 6400/7568 Train_loss 1.9696414602201369 
Epoch [1/10] Batch 6500/7568 Train_loss 1.9683002863452355 
Epoch [1/10] Batch 6600/7568 Train_loss 1.9675327751798497 
Epoch [1/10] Batch 6700/7568 Train_loss 1.9691716888501134 
Epoch [1/10] Batch 6800/7568 Train_loss 1.9682476286996973 
Epoch [1/10] Batch 6900/7568 Train_loss 1.9671674302813282 
Epoch [1/10] Batch 7000/7568 Train_loss 1.967508938547169 
Epoch [1/10] Batch 7100/7568 Train_loss 1.9667473171730476 
Epoch [1/10] Batch 7200/7568 Train_loss 1.9670875855240288 
Epoch [1/10] Batch 7300/7568 Train_loss 1.9667285939464503 
Epoch [1/10] Batch 7400/7568 Train_loss 1.9671999712442807 
Epoch [1/10] Batch 7500/7568 Train_loss 1.967333061840356 
Epoch: 1/10 	Training Loss: 1.966936 	Validation Loss: 2.086887 Duration seconds: 1119.8188059329987 
best_valid_loss_fold [1.9670444365795583] Best_Epoch [1]Epoch [2/10] Batch 0/7568 Train_loss 1.7916177213191986 
Epoch [2/10] Batch 100/7568 Train_loss 1.882854828445038 
Epoch [2/10] Batch 200/7568 Train_loss 1.8774399772953632 
Epoch [2/10] Batch 300/7568 Train_loss 1.9055261547086246 
Epoch [2/10] Batch 400/7568 Train_loss 1.9076981350891014 
Epoch [2/10] Batch 500/7568 Train_loss 1.917155622365232 
Epoch [2/10] Batch 600/7568 Train_loss 1.9236478299993445 
Epoch [2/10] Batch 700/7568 Train_loss 1.93442315054518 
Epoch [2/10] Batch 800/7568 Train_loss 1.9215281299325857 
Epoch [2/10] Batch 900/7568 Train_loss 1.914368309121947 
Epoch [2/10] Batch 1000/7568 Train_loss 1.914504435170185 
Epoch [2/10] Batch 1100/7568 Train_loss 1.9172706754774964 
Epoch [2/10] Batch 1200/7568 Train_loss 1.9217998078472907 
Epoch [2/10] Batch 1300/7568 Train_loss 1.9140530878149666 
Epoch [2/10] Batch 1400/7568 Train_loss 1.9136821592411426 
Epoch [2/10] Batch 1500/7568 Train_loss 1.9240281217639086 
Epoch [2/10] Batch 1600/7568 Train_loss 1.9223005108204876 
Epoch [2/10] Batch 1700/7568 Train_loss 1.9232701175549953 
Epoch [2/10] Batch 1800/7568 Train_loss 1.9293334574616001 
Epoch [2/10] Batch 1900/7568 Train_loss 1.9328675459068991 
Epoch [2/10] Batch 2000/7568 Train_loss 1.935154136339168 
Epoch [2/10] Batch 2100/7568 Train_loss 1.934813224009705 
Epoch [2/10] Batch 2200/7568 Train_loss 1.9330752580031 
Epoch [2/10] Batch 2300/7568 Train_loss 1.9351221734689454 
Epoch [2/10] Batch 2400/7568 Train_loss 1.9353474254895626 
Epoch [2/10] Batch 2500/7568 Train_loss 1.9362862516002435 
Epoch [2/10] Batch 2600/7568 Train_loss 1.9313210957339524 
Epoch [2/10] Batch 2700/7568 Train_loss 1.9312902328946864 
Epoch [2/10] Batch 2800/7568 Train_loss 1.9313667748909429 
Epoch [2/10] Batch 2900/7568 Train_loss 1.9321616062292843 
Epoch [2/10] Batch 3000/7568 Train_loss 1.9327544730550168 
Epoch [2/10] Batch 3100/7568 Train_loss 1.9313252669203784 
Epoch [2/10] Batch 3200/7568 Train_loss 1.9318371610375578 
Epoch [2/10] Batch 3300/7568 Train_loss 1.9322325061511285 
Epoch [2/10] Batch 3400/7568 Train_loss 1.9337102820315244 
Epoch [2/10] Batch 3500/7568 Train_loss 1.9346825360519755 
Epoch [2/10] Batch 3600/7568 Train_loss 1.9345078182010245 
Epoch [2/10] Batch 3700/7568 Train_loss 1.9321849648693517 
Epoch [2/10] Batch 3800/7568 Train_loss 1.9309434379183659 
Epoch [2/10] Batch 3900/7568 Train_loss 1.9321000747720696 
Epoch [2/10] Batch 4000/7568 Train_loss 1.9317043352927068 
Epoch [2/10] Batch 4100/7568 Train_loss 1.93216004324189 
Epoch [2/10] Batch 4200/7568 Train_loss 1.9332324982335016 
Epoch [2/10] Batch 4300/7568 Train_loss 1.9337543035504208 
Epoch [2/10] Batch 4400/7568 Train_loss 1.9353817961475195 
Epoch [2/10] Batch 4500/7568 Train_loss 1.9353213396947189 
Epoch [2/10] Batch 4600/7568 Train_loss 1.9365905070144231 
Epoch [2/10] Batch 4700/7568 Train_loss 1.9374622731063245 
Epoch [2/10] Batch 4800/7568 Train_loss 1.9372487907588694 
Epoch [2/10] Batch 4900/7568 Train_loss 1.9395143622933444 
Epoch [2/10] Batch 5000/7568 Train_loss 1.9382728342025715 
Epoch [2/10] Batch 5100/7568 Train_loss 1.939790090336049 
Epoch [2/10] Batch 5200/7568 Train_loss 1.9385088035302125 
Epoch [2/10] Batch 5300/7568 Train_loss 1.9394374083340382 
Epoch [2/10] Batch 5400/7568 Train_loss 1.938804951205869 
Epoch [2/10] Batch 5500/7568 Train_loss 1.9384091954635634 
Epoch [2/10] Batch 5600/7568 Train_loss 1.9391308356807437 
Epoch [2/10] Batch 5700/7568 Train_loss 1.9387458017302739 
Epoch [2/10] Batch 5800/7568 Train_loss 1.939882638549069 
Epoch [2/10] Batch 5900/7568 Train_loss 1.9398154716681795 
Epoch [2/10] Batch 6000/7568 Train_loss 1.9386578504218994 
Epoch [2/10] Batch 6100/7568 Train_loss 1.9365880528818757 
Epoch [2/10] Batch 6200/7568 Train_loss 1.9366637119803192 
Epoch [2/10] Batch 6300/7568 Train_loss 1.935789101671234 
Epoch [2/10] Batch 6400/7568 Train_loss 1.935524592579687 
Epoch [2/10] Batch 6500/7568 Train_loss 1.9356642590359787 
Epoch [2/10] Batch 6600/7568 Train_loss 1.936951767647019 
Epoch [2/10] Batch 6700/7568 Train_loss 1.9373400336574251 
Epoch [2/10] Batch 6800/7568 Train_loss 1.9375715104468938 
Epoch [2/10] Batch 6900/7568 Train_loss 1.9372200159931714 
Epoch [2/10] Batch 7000/7568 Train_loss 1.9389381134740762 
Epoch [2/10] Batch 7100/7568 Train_loss 1.938884552956937 
Epoch [2/10] Batch 7200/7568 Train_loss 1.938979741076747 
Epoch [2/10] Batch 7300/7568 Train_loss 1.9388968185857784 
Epoch [2/10] Batch 7400/7568 Train_loss 1.938520156882966 
Epoch [2/10] Batch 7500/7568 Train_loss 1.937389434220282 
Epoch: 2/10 	Training Loss: 1.936987 	Validation Loss: 2.087413 Duration seconds: 1137.6568925380707 
best_valid_loss_fold [1.9670444365795583] Best_Epoch [2]Epoch [3/10] Batch 0/7568 Train_loss 1.9621023535728455 
Epoch [3/10] Batch 100/7568 Train_loss 1.9791388275599715 
Epoch [3/10] Batch 200/7568 Train_loss 1.93951847504324 
Epoch [3/10] Batch 300/7568 Train_loss 1.90436977336177 
Epoch [3/10] Batch 400/7568 Train_loss 1.9135222600731172 
Epoch [3/10] Batch 500/7568 Train_loss 1.9140531452829965 
Epoch [3/10] Batch 600/7568 Train_loss 1.8988067293524147 
Epoch [3/10] Batch 700/7568 Train_loss 1.900704552269356 
Epoch [3/10] Batch 800/7568 Train_loss 1.9033679647020036 
Epoch [3/10] Batch 900/7568 Train_loss 1.9082335726700137 
Epoch [3/10] Batch 1000/7568 Train_loss 1.9172045063752157 
Epoch [3/10] Batch 1100/7568 Train_loss 1.915815367712745 
Epoch [3/10] Batch 1200/7568 Train_loss 1.9175867808922245 
Epoch [3/10] Batch 1300/7568 Train_loss 1.9165272586946758 
Epoch [3/10] Batch 1400/7568 Train_loss 1.9207928804607581 
Epoch [3/10] Batch 1500/7568 Train_loss 1.9213247195452074 
Epoch [3/10] Batch 1600/7568 Train_loss 1.9239101208034417 
Epoch [3/10] Batch 1700/7568 Train_loss 1.9216069406547804 
Epoch [3/10] Batch 1800/7568 Train_loss 1.9210847290537876 
Epoch [3/10] Batch 1900/7568 Train_loss 1.9217430566217573 
Epoch [3/10] Batch 2000/7568 Train_loss 1.9257539458971271 
Epoch [3/10] Batch 2100/7568 Train_loss 1.921406092883745 
Epoch [3/10] Batch 2200/7568 Train_loss 1.9236219147967513 
Epoch [3/10] Batch 2300/7568 Train_loss 1.9230421755571356 
Epoch [3/10] Batch 2400/7568 Train_loss 1.920092631533264 
Epoch [3/10] Batch 2500/7568 Train_loss 1.9165603089623335 
Epoch [3/10] Batch 2600/7568 Train_loss 1.9176009442314923 
Epoch [3/10] Batch 2700/7568 Train_loss 1.9192043270933676 
Epoch [3/10] Batch 2800/7568 Train_loss 1.9156412915677439 
Epoch [3/10] Batch 2900/7568 Train_loss 1.915260779111807 
Epoch [3/10] Batch 3000/7568 Train_loss 1.9143199016206942 
Epoch [3/10] Batch 3100/7568 Train_loss 1.9131507126747198 
Epoch [3/10] Batch 3200/7568 Train_loss 1.9136807608561603 
Epoch [3/10] Batch 3300/7568 Train_loss 1.9138075299594879 
Epoch [3/10] Batch 3400/7568 Train_loss 1.9122857727730356 
Epoch [3/10] Batch 3500/7568 Train_loss 1.9097692547517653 
Epoch [3/10] Batch 3600/7568 Train_loss 1.9097711530924506 
Epoch [3/10] Batch 3700/7568 Train_loss 1.9101216433274362 
Epoch [3/10] Batch 3800/7568 Train_loss 1.9092285577908563 
Epoch [3/10] Batch 3900/7568 Train_loss 1.9088373136016932 
Epoch [3/10] Batch 4000/7568 Train_loss 1.9095354114110485 
Epoch [3/10] Batch 4100/7568 Train_loss 1.9080449949329552 
Epoch [3/10] Batch 4200/7568 Train_loss 1.909793088506571 
Epoch [3/10] Batch 4300/7568 Train_loss 1.907939793586634 
Epoch [3/10] Batch 4400/7568 Train_loss 1.9097068329303406 
Epoch [3/10] Batch 4500/7568 Train_loss 1.910376699740252 
Epoch [3/10] Batch 4600/7568 Train_loss 1.9107840273110692 
Epoch [3/10] Batch 4700/7568 Train_loss 1.9105261342290538 
Epoch [3/10] Batch 4800/7568 Train_loss 1.9120558557290506 
Epoch [3/10] Batch 4900/7568 Train_loss 1.9110008784381862 
Epoch [3/10] Batch 5000/7568 Train_loss 1.9115271251309731 
Epoch [3/10] Batch 5100/7568 Train_loss 1.9107910794335776 
Epoch [3/10] Batch 5200/7568 Train_loss 1.9116152537683297 
Epoch [3/10] Batch 5300/7568 Train_loss 1.9084224010079993 
Epoch [3/10] Batch 5400/7568 Train_loss 1.908890529770395 
Epoch [3/10] Batch 5500/7568 Train_loss 1.9090684602662469 
Epoch [3/10] Batch 5600/7568 Train_loss 1.9087875438423438 
Epoch [3/10] Batch 5700/7568 Train_loss 1.9081986539942761 
Epoch [3/10] Batch 5800/7568 Train_loss 1.9069101713524566 
Epoch [3/10] Batch 5900/7568 Train_loss 1.9070426017548487 
Epoch [3/10] Batch 6000/7568 Train_loss 1.9067201858556801 
Epoch [3/10] Batch 6100/7568 Train_loss 1.9046997101182157 
Epoch [3/10] Batch 6200/7568 Train_loss 1.906896218365526 
Epoch [3/10] Batch 6300/7568 Train_loss 1.9066748048987792 
Epoch [3/10] Batch 6400/7568 Train_loss 1.9058957252546114 
Epoch [3/10] Batch 6500/7568 Train_loss 1.9048772271223442 
Epoch [3/10] Batch 6600/7568 Train_loss 1.9047191710131057 
Epoch [3/10] Batch 6700/7568 Train_loss 1.9059770287513074 
Epoch [3/10] Batch 6800/7568 Train_loss 1.9060217967365634 
Epoch [3/10] Batch 6900/7568 Train_loss 1.905199347606147 
Epoch [3/10] Batch 7000/7568 Train_loss 1.9061746308067886 
Epoch [3/10] Batch 7100/7568 Train_loss 1.905791387208506 
Epoch [3/10] Batch 7200/7568 Train_loss 1.9055457934095716 
Epoch [3/10] Batch 7300/7568 Train_loss 1.9042395845814823 
Epoch [3/10] Batch 7400/7568 Train_loss 1.9047897947161136 
Epoch [3/10] Batch 7500/7568 Train_loss 1.9052441011870436 
Epoch: 3/10 	Training Loss: 1.905295 	Validation Loss: 2.113310 Duration seconds: 1066.3284850120544 
best_valid_loss_fold [1.9670444365795583] Best_Epoch [3]Epoch [4/10] Batch 0/7568 Train_loss 1.8378464877605438 
Epoch [4/10] Batch 100/7568 Train_loss 1.9044678894304994 
Epoch [4/10] Batch 200/7568 Train_loss 1.8905943880478542 
Epoch [4/10] Batch 300/7568 Train_loss 1.8862441048471634 
Epoch [4/10] Batch 400/7568 Train_loss 1.8956163350929345 
Epoch [4/10] Batch 500/7568 Train_loss 1.9089263552677131 
Epoch [4/10] Batch 600/7568 Train_loss 1.9250759887764735 
Epoch [4/10] Batch 700/7568 Train_loss 1.9334190143285566 
Epoch [4/10] Batch 800/7568 Train_loss 1.9253753491220402 
Epoch [4/10] Batch 900/7568 Train_loss 1.9280562436540967 
Epoch [4/10] Batch 1000/7568 Train_loss 1.9169724329963669 
Epoch [4/10] Batch 1100/7568 Train_loss 1.9307807284820957 
Epoch [4/10] Batch 1200/7568 Train_loss 1.9240925521849592 
Epoch [4/10] Batch 1300/7568 Train_loss 1.9209458234560297 
Epoch [4/10] Batch 1400/7568 Train_loss 1.9182423638348747 
Epoch [4/10] Batch 1500/7568 Train_loss 1.9237394913842407 
Epoch [4/10] Batch 1600/7568 Train_loss 1.9221334524224656 
Epoch [4/10] Batch 1700/7568 Train_loss 1.9187034368471214 
Epoch [4/10] Batch 1800/7568 Train_loss 1.9142897579941003 
Epoch [4/10] Batch 1900/7568 Train_loss 1.9141952349936129 
Epoch [4/10] Batch 2000/7568 Train_loss 1.911984957858868 
Epoch [4/10] Batch 2100/7568 Train_loss 1.9111479390261856 
Epoch [4/10] Batch 2200/7568 Train_loss 1.912051863419988 
Epoch [4/10] Batch 2300/7568 Train_loss 1.9139190769225345 
Epoch [4/10] Batch 2400/7568 Train_loss 1.912704429595806 
Epoch [4/10] Batch 2500/7568 Train_loss 1.9143257926001114 
Epoch [4/10] Batch 2600/7568 Train_loss 1.9156664473128888 
Epoch [4/10] Batch 2700/7568 Train_loss 1.9136210630162696 
Epoch [4/10] Batch 2800/7568 Train_loss 1.9110420750974757 
Epoch [4/10] Batch 2900/7568 Train_loss 1.9112510103006808 
Epoch [4/10] Batch 3000/7568 Train_loss 1.9094953061475908 
Epoch [4/10] Batch 3100/7568 Train_loss 1.9089491131377005 
Epoch [4/10] Batch 3200/7568 Train_loss 1.9095545576205109 
Epoch [4/10] Batch 3300/7568 Train_loss 1.9101960045082105 
Epoch [4/10] Batch 3400/7568 Train_loss 1.9063061682446605 
Epoch [4/10] Batch 3500/7568 Train_loss 1.9043217072999161 
Epoch [4/10] Batch 3600/7568 Train_loss 1.9024767462341066 
Epoch [4/10] Batch 3700/7568 Train_loss 1.9024461483136443 
Epoch [4/10] Batch 3800/7568 Train_loss 1.8989612851992654 
Epoch [4/10] Batch 3900/7568 Train_loss 1.898342301150876 
Epoch [4/10] Batch 4000/7568 Train_loss 1.8986967444639598 
Epoch [4/10] Batch 4100/7568 Train_loss 1.8990134067108533 
Epoch [4/10] Batch 4200/7568 Train_loss 1.8984040733487997 
Epoch [4/10] Batch 4300/7568 Train_loss 1.8960178851080034 
Epoch [4/10] Batch 4400/7568 Train_loss 1.8964591913282451 
Epoch [4/10] Batch 4500/7568 Train_loss 1.8956961198628175 
Epoch [4/10] Batch 4600/7568 Train_loss 1.8952926487083592 
Epoch [4/10] Batch 4700/7568 Train_loss 1.895314978070714 
Epoch [4/10] Batch 4800/7568 Train_loss 1.8942480892722022 
Epoch [4/10] Batch 4900/7568 Train_loss 1.8915756613736712 
Epoch [4/10] Batch 5000/7568 Train_loss 1.8936281221222457 
Epoch [4/10] Batch 5100/7568 Train_loss 1.893328182790717 
Epoch [4/10] Batch 5200/7568 Train_loss 1.8946627672805372 
Epoch [4/10] Batch 5300/7568 Train_loss 1.89437987406908 
Epoch [4/10] Batch 5400/7568 Train_loss 1.8942534957971535 
Epoch [4/10] Batch 5500/7568 Train_loss 1.894706432605858 
Epoch [4/10] Batch 5600/7568 Train_loss 1.8961844386636328 
Epoch [4/10] Batch 5700/7568 Train_loss 1.8967971755193318 
Epoch [4/10] Batch 5800/7568 Train_loss 1.8975058104537865 
Epoch [4/10] Batch 5900/7568 Train_loss 1.8984868034084073 
Epoch [4/10] Batch 6000/7568 Train_loss 1.8976107137853495 
Epoch [4/10] Batch 6100/7568 Train_loss 1.8969335384365282 
Epoch [4/10] Batch 6200/7568 Train_loss 1.8971397883984624 
Epoch [4/10] Batch 6300/7568 Train_loss 1.8962751945530663 
Epoch [4/10] Batch 6400/7568 Train_loss 1.8957596638008145 
Epoch [4/10] Batch 6500/7568 Train_loss 1.894145118766859 
Epoch [4/10] Batch 6600/7568 Train_loss 1.893689517011346 
Epoch [4/10] Batch 6700/7568 Train_loss 1.8944789798297361 
Epoch [4/10] Batch 6800/7568 Train_loss 1.8950464329499477 
Epoch [4/10] Batch 6900/7568 Train_loss 1.8958249181636415 
Epoch [4/10] Batch 7000/7568 Train_loss 1.8969212977888499 
Epoch [4/10] Batch 7100/7568 Train_loss 1.8976832960845467 
Epoch [4/10] Batch 7200/7568 Train_loss 1.8967314102629906 
Epoch [4/10] Batch 7300/7568 Train_loss 1.896004269853461 
Epoch [4/10] Batch 7400/7568 Train_loss 1.8953571974609031 
Epoch [4/10] Batch 7500/7568 Train_loss 1.8953410090714466 
Epoch: 4/10 	Training Loss: 1.894151 	Validation Loss: 2.016284 Duration seconds: 1086.3100912570953 
best_valid_loss_fold [1.9670444365795583] Best_Epoch [4]Epoch [5/10] Batch 0/7568 Train_loss 2.5541210174560547 
Epoch [5/10] Batch 100/7568 Train_loss 1.9498398199234859 
Epoch [5/10] Batch 200/7568 Train_loss 1.9532943535503464 
Epoch [5/10] Batch 300/7568 Train_loss 1.9249948896343922 
Epoch [5/10] Batch 400/7568 Train_loss 1.909739299679635 
Epoch [5/10] Batch 500/7568 Train_loss 1.908703590789717 
Epoch [5/10] Batch 600/7568 Train_loss 1.924585693712639 
Epoch [5/10] Batch 700/7568 Train_loss 1.915560430088499 
Epoch [5/10] Batch 800/7568 Train_loss 1.914099483342653 
Epoch [5/10] Batch 900/7568 Train_loss 1.9091993763000932 
Epoch [5/10] Batch 1000/7568 Train_loss 1.9082691678247015 
Epoch [5/10] Batch 1100/7568 Train_loss 1.9105489545253924 
Epoch [5/10] Batch 1200/7568 Train_loss 1.9107301353316422 
Epoch [5/10] Batch 1300/7568 Train_loss 1.9071679797702161 
Epoch [5/10] Batch 1400/7568 Train_loss 1.9059239849075942 
Epoch [5/10] Batch 1500/7568 Train_loss 1.900763092469486 
Epoch [5/10] Batch 1600/7568 Train_loss 1.8987625541815827 
Epoch [5/10] Batch 1700/7568 Train_loss 1.9027739132084893 
Epoch [5/10] Batch 1800/7568 Train_loss 1.9027363137858633 
Epoch [5/10] Batch 1900/7568 Train_loss 1.9008785692376253 
Epoch [5/10] Batch 2000/7568 Train_loss 1.8973716911764398 
Epoch [5/10] Batch 2100/7568 Train_loss 1.898791111619174 
Epoch [5/10] Batch 2200/7568 Train_loss 1.8965598097623668 
Epoch [5/10] Batch 2300/7568 Train_loss 1.9003375558128361 
Epoch [5/10] Batch 2400/7568 Train_loss 1.8968897259505975 
Epoch [5/10] Batch 2500/7568 Train_loss 1.8959940756346787 
Epoch [5/10] Batch 2600/7568 Train_loss 1.8966073630487217 
Epoch [5/10] Batch 2700/7568 Train_loss 1.894608793827791 
Epoch [5/10] Batch 2800/7568 Train_loss 1.8910755393431469 
Epoch [5/10] Batch 2900/7568 Train_loss 1.8916196035788166 
Epoch [5/10] Batch 3000/7568 Train_loss 1.8913465254666686 
Epoch [5/10] Batch 3100/7568 Train_loss 1.8915051244192067 
Epoch [5/10] Batch 3200/7568 Train_loss 1.890460885118522 
Epoch [5/10] Batch 3300/7568 Train_loss 1.8911637553380352 
Epoch [5/10] Batch 3400/7568 Train_loss 1.888846742343601 
Epoch [5/10] Batch 3500/7568 Train_loss 1.8871127272658401 
Epoch [5/10] Batch 3600/7568 Train_loss 1.8884837358023054 
Epoch [5/10] Batch 3700/7568 Train_loss 1.887215508827001 
Epoch [5/10] Batch 3800/7568 Train_loss 1.888235870300861 
Epoch [5/10] Batch 3900/7568 Train_loss 1.88708550649342 
Epoch [5/10] Batch 4000/7568 Train_loss 1.8874155257387895 
Epoch [5/10] Batch 4100/7568 Train_loss 1.8861758739748138 
Epoch [5/10] Batch 4200/7568 Train_loss 1.8851212232017398 
Epoch [5/10] Batch 4300/7568 Train_loss 1.8862884899030117 
Epoch [5/10] Batch 4400/7568 Train_loss 1.885303653932395 
Epoch [5/10] Batch 4500/7568 Train_loss 1.885440364236687 
Epoch [5/10] Batch 4600/7568 Train_loss 1.8846789294790247 
Epoch [5/10] Batch 4700/7568 Train_loss 1.8846843936463742 
Epoch [5/10] Batch 4800/7568 Train_loss 1.8843763887065943 
Epoch [5/10] Batch 4900/7568 Train_loss 1.8837342457114203 
Epoch [5/10] Batch 5000/7568 Train_loss 1.8842206957253353 
Epoch [5/10] Batch 5100/7568 Train_loss 1.8837748198512372 
Epoch [5/10] Batch 5200/7568 Train_loss 1.8836202355768905 
Epoch [5/10] Batch 5300/7568 Train_loss 1.883765213736845 
Epoch [5/10] Batch 5400/7568 Train_loss 1.8841322529298843 
Epoch [5/10] Batch 5500/7568 Train_loss 1.8836956860122085 
Epoch [5/10] Batch 5600/7568 Train_loss 1.8806902432062331 
Epoch [5/10] Batch 5700/7568 Train_loss 1.881927944630923 
Epoch [5/10] Batch 5800/7568 Train_loss 1.8821910789875076 
Epoch [5/10] Batch 5900/7568 Train_loss 1.882122136422677 
Epoch [5/10] Batch 6000/7568 Train_loss 1.8807440425985218 
Epoch [5/10] Batch 6100/7568 Train_loss 1.8813128930413807 
Epoch [5/10] Batch 6200/7568 Train_loss 1.8803642393107702 
Epoch [5/10] Batch 6300/7568 Train_loss 1.8805120064815044 
Epoch [5/10] Batch 6400/7568 Train_loss 1.8831779496385481 
Epoch [5/10] Batch 6500/7568 Train_loss 1.882813664448617 
Epoch [5/10] Batch 6600/7568 Train_loss 1.8819326584191687 
Epoch [5/10] Batch 6700/7568 Train_loss 1.8815976167991346 
Epoch [5/10] Batch 6800/7568 Train_loss 1.8820957372417224 
Epoch [5/10] Batch 6900/7568 Train_loss 1.8823936777403951 
Epoch [5/10] Batch 7000/7568 Train_loss 1.883857997330942 
Epoch [5/10] Batch 7100/7568 Train_loss 1.8838499922873504 
Epoch [5/10] Batch 7200/7568 Train_loss 1.884163070106412 
Epoch [5/10] Batch 7300/7568 Train_loss 1.883736366778306 
Epoch [5/10] Batch 7400/7568 Train_loss 1.8833618054946377 
Epoch [5/10] Batch 7500/7568 Train_loss 1.8843170946049002 
Epoch: 5/10 	Training Loss: 1.883436 	Validation Loss: 1.932857 Duration seconds: 1053.4829950332642 
Validation loss decreased (1.967044 --> 1.932857).  Saving model ... 
best_valid_loss_fold [1.9328566585248041] Best_Epoch [5]Epoch [6/10] Batch 0/7568 Train_loss 1.5074516832828522 
Epoch [6/10] Batch 100/7568 Train_loss 1.8900005534143731 
Epoch [6/10] Batch 200/7568 Train_loss 1.8897617801060131 
Epoch [6/10] Batch 300/7568 Train_loss 1.8909324945702506 
Epoch [6/10] Batch 400/7568 Train_loss 1.8757617328678284 
Epoch [6/10] Batch 500/7568 Train_loss 1.8630088691761393 
Epoch [6/10] Batch 600/7568 Train_loss 1.8611935579439567 
Epoch [6/10] Batch 700/7568 Train_loss 1.8488928033689969 
Epoch [6/10] Batch 800/7568 Train_loss 1.8610496232088436 
Epoch [6/10] Batch 900/7568 Train_loss 1.8666716389664004 
Epoch [6/10] Batch 1000/7568 Train_loss 1.8588727444678277 
Epoch [6/10] Batch 1100/7568 Train_loss 1.8581213859619605 
Epoch [6/10] Batch 1200/7568 Train_loss 1.8576359645549603 
Epoch [6/10] Batch 1300/7568 Train_loss 1.8528270612339806 
Epoch [6/10] Batch 1400/7568 Train_loss 1.858691543229004 
Epoch [6/10] Batch 1500/7568 Train_loss 1.861780941769292 
Epoch [6/10] Batch 1600/7568 Train_loss 1.861852921806113 
Epoch [6/10] Batch 1700/7568 Train_loss 1.8606983609869858 
Epoch [6/10] Batch 1800/7568 Train_loss 1.8624611082034332 
Epoch [6/10] Batch 1900/7568 Train_loss 1.8609503288799871 
Epoch [6/10] Batch 2000/7568 Train_loss 1.8542178919997767 
Epoch [6/10] Batch 2100/7568 Train_loss 1.8560972932667632 
Epoch [6/10] Batch 2200/7568 Train_loss 1.8580647165811317 
Epoch [6/10] Batch 2300/7568 Train_loss 1.859016405521321 
Epoch [6/10] Batch 2400/7568 Train_loss 1.8586222353123318 
Epoch [6/10] Batch 2500/7568 Train_loss 1.857504923476047 
Epoch [6/10] Batch 2600/7568 Train_loss 1.8578306932871684 
Epoch [6/10] Batch 2700/7568 Train_loss 1.8573415469307892 
Epoch [6/10] Batch 2800/7568 Train_loss 1.8586534203849108 
Epoch [6/10] Batch 2900/7568 Train_loss 1.860772780046735 
Epoch [6/10] Batch 3000/7568 Train_loss 1.8583649190211646 
Epoch [6/10] Batch 3100/7568 Train_loss 1.8591386144539304 
Epoch [6/10] Batch 3200/7568 Train_loss 1.8610844395354142 
Epoch [6/10] Batch 3300/7568 Train_loss 1.8624388070489846 
Epoch [6/10] Batch 3400/7568 Train_loss 1.8612141530304682 
Epoch [6/10] Batch 3500/7568 Train_loss 1.8625310488030318 
Epoch [6/10] Batch 3600/7568 Train_loss 1.8631187421175315 
Epoch [6/10] Batch 3700/7568 Train_loss 1.8616291905671027 
Epoch [6/10] Batch 3800/7568 Train_loss 1.8645461075942913 
Epoch [6/10] Batch 3900/7568 Train_loss 1.8640958314956808 
Epoch [6/10] Batch 4000/7568 Train_loss 1.866061507471202 
Epoch [6/10] Batch 4100/7568 Train_loss 1.8684221228225382 
Epoch [6/10] Batch 4200/7568 Train_loss 1.8664824099263757 
Epoch [6/10] Batch 4300/7568 Train_loss 1.8684996797390745 
Epoch [6/10] Batch 4400/7568 Train_loss 1.871425506422477 
Epoch [6/10] Batch 4500/7568 Train_loss 1.87064467986619 
Epoch [6/10] Batch 4600/7568 Train_loss 1.870638654348488 
Epoch [6/10] Batch 4700/7568 Train_loss 1.8725462709239533 
Epoch [6/10] Batch 4800/7568 Train_loss 1.8750470212643853 
Epoch [6/10] Batch 4900/7568 Train_loss 1.8766894000564345 
Epoch [6/10] Batch 5000/7568 Train_loss 1.875961365964479 
Epoch [6/10] Batch 5100/7568 Train_loss 1.8765138173934128 
Epoch [6/10] Batch 5200/7568 Train_loss 1.8756294744137594 
Epoch [6/10] Batch 5300/7568 Train_loss 1.8747263076372405 
Epoch [6/10] Batch 5400/7568 Train_loss 1.8736080210217942 
Epoch [6/10] Batch 5500/7568 Train_loss 1.8732554987113577 
Epoch [6/10] Batch 5600/7568 Train_loss 1.8722508499461015 
Epoch [6/10] Batch 5700/7568 Train_loss 1.8724801333729288 
Epoch [6/10] Batch 5800/7568 Train_loss 1.8732504269862418 
Epoch [6/10] Batch 5900/7568 Train_loss 1.8724320817930056 
Epoch [6/10] Batch 6000/7568 Train_loss 1.8733984917908824 
Epoch [6/10] Batch 6100/7568 Train_loss 1.873467892953029 
Epoch [6/10] Batch 6200/7568 Train_loss 1.8732935628392509 
Epoch [6/10] Batch 6300/7568 Train_loss 1.8730846817527305 
Epoch [6/10] Batch 6400/7568 Train_loss 1.8730573228114733 
Epoch [6/10] Batch 6500/7568 Train_loss 1.8715054640846425 
Epoch [6/10] Batch 6600/7568 Train_loss 1.8706307713822188 
Epoch [6/10] Batch 6700/7568 Train_loss 1.8705664323263427 
Epoch [6/10] Batch 6800/7568 Train_loss 1.871119023964451 
Epoch [6/10] Batch 6900/7568 Train_loss 1.8713912653708835 
Epoch [6/10] Batch 7000/7568 Train_loss 1.8712754454689866 
Epoch [6/10] Batch 7100/7568 Train_loss 1.8711317652502524 
Epoch [6/10] Batch 7200/7568 Train_loss 1.8712465668793206 
Epoch [6/10] Batch 7300/7568 Train_loss 1.8707919277433434 
Epoch [6/10] Batch 7400/7568 Train_loss 1.8717945868434045 
Epoch [6/10] Batch 7500/7568 Train_loss 1.872132433273684 
Epoch: 6/10 	Training Loss: 1.871768 	Validation Loss: 1.885214 Duration seconds: 1077.2046415805817 
Validation loss decreased (1.932857 --> 1.885214).  Saving model ... 
best_valid_loss_fold [1.8852142867244324] Best_Epoch [6]Epoch [7/10] Batch 0/7568 Train_loss 2.0662572979927063 
Epoch [7/10] Batch 100/7568 Train_loss 1.8131113613005911 
Epoch [7/10] Batch 200/7568 Train_loss 1.824538069800358 
Epoch [7/10] Batch 300/7568 Train_loss 1.8604344471150458 
Epoch [7/10] Batch 400/7568 Train_loss 1.8493698975837736 
Epoch [7/10] Batch 500/7568 Train_loss 1.8648532965404545 
Epoch [7/10] Batch 600/7568 Train_loss 1.8796473663430444 
Epoch [7/10] Batch 700/7568 Train_loss 1.882528359025249 
Epoch [7/10] Batch 800/7568 Train_loss 1.868589020493474 
Epoch [7/10] Batch 900/7568 Train_loss 1.8617269331454966 
Epoch [7/10] Batch 1000/7568 Train_loss 1.8669837496646278 
Epoch [7/10] Batch 1100/7568 Train_loss 1.8569470274784043 
Epoch [7/10] Batch 1200/7568 Train_loss 1.8562536862842447 
Epoch [7/10] Batch 1300/7568 Train_loss 1.8552114276772367 
Epoch [7/10] Batch 1400/7568 Train_loss 1.8550810975678729 
Epoch [7/10] Batch 1500/7568 Train_loss 1.8536457487895122 
Epoch [7/10] Batch 1600/7568 Train_loss 1.8557732761706656 
Epoch [7/10] Batch 1700/7568 Train_loss 1.8551658553841672 
Epoch [7/10] Batch 1800/7568 Train_loss 1.851413760712781 
Epoch [7/10] Batch 1900/7568 Train_loss 1.8518079175314484 
Epoch [7/10] Batch 2000/7568 Train_loss 1.8496204279679647 
Epoch [7/10] Batch 2100/7568 Train_loss 1.8495468776150013 
Epoch [7/10] Batch 2200/7568 Train_loss 1.8483382169290978 
Epoch [7/10] Batch 2300/7568 Train_loss 1.8478124271082599 
Epoch [7/10] Batch 2400/7568 Train_loss 1.8520136701500898 
Epoch [7/10] Batch 2500/7568 Train_loss 1.8511699274128697 
Epoch [7/10] Batch 2600/7568 Train_loss 1.8534335769320487 
Epoch [7/10] Batch 2700/7568 Train_loss 1.8524860327815798 
Epoch [7/10] Batch 2800/7568 Train_loss 1.8517716908052708 
Epoch [7/10] Batch 2900/7568 Train_loss 1.8529405545023383 
Epoch [7/10] Batch 3000/7568 Train_loss 1.8556737357807016 
Epoch [7/10] Batch 3100/7568 Train_loss 1.8562908115280479 
Epoch [7/10] Batch 3200/7568 Train_loss 1.8572063818783433 
Epoch [7/10] Batch 3300/7568 Train_loss 1.857218921220653 
Epoch [7/10] Batch 3400/7568 Train_loss 1.8579385513962805 
Epoch [7/10] Batch 3500/7568 Train_loss 1.8581450782062938 
Epoch [7/10] Batch 3600/7568 Train_loss 1.857139852060374 
Epoch [7/10] Batch 3700/7568 Train_loss 1.857228155346762 
Epoch [7/10] Batch 3800/7568 Train_loss 1.8560017294489815 
Epoch [7/10] Batch 3900/7568 Train_loss 1.8588926887411363 
Epoch [7/10] Batch 4000/7568 Train_loss 1.855927456661005 
Epoch [7/10] Batch 4100/7568 Train_loss 1.8570476528407016 
Epoch [7/10] Batch 4200/7568 Train_loss 1.8563276429285296 
Epoch [7/10] Batch 4300/7568 Train_loss 1.855804598822036 
Epoch [7/10] Batch 4400/7568 Train_loss 1.8600199879624306 
Epoch [7/10] Batch 4500/7568 Train_loss 1.8594171537694282 
Epoch [7/10] Batch 4600/7568 Train_loss 1.8602936709705733 
Epoch [7/10] Batch 4700/7568 Train_loss 1.8621001709581462 
Epoch [7/10] Batch 4800/7568 Train_loss 1.8610162029096768 
Epoch [7/10] Batch 4900/7568 Train_loss 1.8596340950226522 
Epoch [7/10] Batch 5000/7568 Train_loss 1.8598400408331692 
Epoch [7/10] Batch 5100/7568 Train_loss 1.8610484144341664 
Epoch [7/10] Batch 5200/7568 Train_loss 1.8621711738134 
Epoch [7/10] Batch 5300/7568 Train_loss 1.863837290075259 
Epoch [7/10] Batch 5400/7568 Train_loss 1.8652496911437726 
Epoch [7/10] Batch 5500/7568 Train_loss 1.8658860224923683 
Epoch [7/10] Batch 5600/7568 Train_loss 1.8643649118584813 
Epoch [7/10] Batch 5700/7568 Train_loss 1.8630006492929885 
Epoch [7/10] Batch 5800/7568 Train_loss 1.8624022263000288 
Epoch [7/10] Batch 5900/7568 Train_loss 1.864043271582251 
Epoch [7/10] Batch 6000/7568 Train_loss 1.863475060135429 
Epoch [7/10] Batch 6100/7568 Train_loss 1.863852627282308 
Epoch [7/10] Batch 6200/7568 Train_loss 1.8647630225252458 
Epoch [7/10] Batch 6300/7568 Train_loss 1.8655487105907798 
Epoch [7/10] Batch 6400/7568 Train_loss 1.8662243771357028 
Epoch [7/10] Batch 6500/7568 Train_loss 1.8650756507408268 
Epoch [7/10] Batch 6600/7568 Train_loss 1.8651289942916587 
Epoch [7/10] Batch 6700/7568 Train_loss 1.8661202745223877 
Epoch [7/10] Batch 6800/7568 Train_loss 1.8663340943735798 
Epoch [7/10] Batch 6900/7568 Train_loss 1.864861529452734 
Epoch [7/10] Batch 7000/7568 Train_loss 1.8648205853337783 
Epoch [7/10] Batch 7100/7568 Train_loss 1.8647127441034705 
Epoch [7/10] Batch 7200/7568 Train_loss 1.8633158551212186 
Epoch [7/10] Batch 7300/7568 Train_loss 1.862618897909776 
Epoch [7/10] Batch 7400/7568 Train_loss 1.8633735934522826 
Epoch [7/10] Batch 7500/7568 Train_loss 1.8631016370765752 
Epoch: 7/10 	Training Loss: 1.862775 	Validation Loss: 1.917827 Duration seconds: 1097.1753211021423 
best_valid_loss_fold [1.8852142867244324] Best_Epoch [7]Epoch [8/10] Batch 0/7568 Train_loss 1.2690676748752594 
Epoch [8/10] Batch 100/7568 Train_loss 1.951959913614953 
Epoch [8/10] Batch 200/7568 Train_loss 1.8891799883611167 
Epoch [8/10] Batch 300/7568 Train_loss 1.8814071050018963 
Epoch [8/10] Batch 400/7568 Train_loss 1.8693570627313005 
Epoch [8/10] Batch 500/7568 Train_loss 1.872253452767869 
Epoch [8/10] Batch 600/7568 Train_loss 1.863584549796958 
Epoch [8/10] Batch 700/7568 Train_loss 1.8726590037685997 
Epoch [8/10] Batch 800/7568 Train_loss 1.8789350847886892 
Epoch [8/10] Batch 900/7568 Train_loss 1.8832117891338107 
Epoch [8/10] Batch 1000/7568 Train_loss 1.8788881502398005 
Epoch [8/10] Batch 1100/7568 Train_loss 1.8695206515200022 
Epoch [8/10] Batch 1200/7568 Train_loss 1.8638685959810817 
Epoch [8/10] Batch 1300/7568 Train_loss 1.8658477849084354 
Epoch [8/10] Batch 1400/7568 Train_loss 1.8580310536464737 
Epoch [8/10] Batch 1500/7568 Train_loss 1.8576843145090607 
Epoch [8/10] Batch 1600/7568 Train_loss 1.8581261970265666 
Epoch [8/10] Batch 1700/7568 Train_loss 1.863365583362683 
Epoch [8/10] Batch 1800/7568 Train_loss 1.8617959593581466 
Epoch [8/10] Batch 1900/7568 Train_loss 1.8618664328892691 
Epoch [8/10] Batch 2000/7568 Train_loss 1.860421951087578 
Epoch [8/10] Batch 2100/7568 Train_loss 1.8610804806020589 
Epoch [8/10] Batch 2200/7568 Train_loss 1.8592167068535173 
Epoch [8/10] Batch 2300/7568 Train_loss 1.853745211302742 
Epoch [8/10] Batch 2400/7568 Train_loss 1.8504938085418798 
Epoch [8/10] Batch 2500/7568 Train_loss 1.8518414748746936 
Epoch [8/10] Batch 2600/7568 Train_loss 1.8524291158948483 
Epoch [8/10] Batch 2700/7568 Train_loss 1.8546563850888849 
Epoch [8/10] Batch 2800/7568 Train_loss 1.8563454141205271 
Epoch [8/10] Batch 2900/7568 Train_loss 1.854990575255833 
Epoch [8/10] Batch 3000/7568 Train_loss 1.8570023823265074 
Epoch [8/10] Batch 3100/7568 Train_loss 1.8558179880728147 
Epoch [8/10] Batch 3200/7568 Train_loss 1.8559255704670092 
Epoch [8/10] Batch 3300/7568 Train_loss 1.855972301089666 
Epoch [8/10] Batch 3400/7568 Train_loss 1.8583885883275497 
Epoch [8/10] Batch 3500/7568 Train_loss 1.8575078452001195 
Epoch [8/10] Batch 3600/7568 Train_loss 1.8585015605577526 
Epoch [8/10] Batch 3700/7568 Train_loss 1.8576289997837474 
Epoch [8/10] Batch 3800/7568 Train_loss 1.8597082387174753 
Epoch [8/10] Batch 3900/7568 Train_loss 1.8615529519858895 
Epoch [8/10] Batch 4000/7568 Train_loss 1.8608033931000594 
Epoch [8/10] Batch 4100/7568 Train_loss 1.8612525598853427 
Epoch [8/10] Batch 4200/7568 Train_loss 1.8609037970863789 
Epoch [8/10] Batch 4300/7568 Train_loss 1.861678470178167 
Epoch [8/10] Batch 4400/7568 Train_loss 1.8601349523509803 
Epoch [8/10] Batch 4500/7568 Train_loss 1.861701047991917 
Epoch [8/10] Batch 4600/7568 Train_loss 1.861930911669496 
Epoch [8/10] Batch 4700/7568 Train_loss 1.863099244636973 
Epoch [8/10] Batch 4800/7568 Train_loss 1.8625063698686826 
Epoch [8/10] Batch 4900/7568 Train_loss 1.8616809887506358 
Epoch [8/10] Batch 5000/7568 Train_loss 1.862216945175194 
Epoch [8/10] Batch 5100/7568 Train_loss 1.8597817216035641 
Epoch [8/10] Batch 5200/7568 Train_loss 1.8603729661944461 
Epoch [8/10] Batch 5300/7568 Train_loss 1.8616217433692284 
Epoch [8/10] Batch 5400/7568 Train_loss 1.8627856546415265 
Epoch [8/10] Batch 5500/7568 Train_loss 1.8601920192804233 
Epoch [8/10] Batch 5600/7568 Train_loss 1.8607693057925632 
Epoch [8/10] Batch 5700/7568 Train_loss 1.858623966501483 
Epoch [8/10] Batch 5800/7568 Train_loss 1.8593915290404879 
Epoch [8/10] Batch 5900/7568 Train_loss 1.8594541991439801 
Epoch [8/10] Batch 6000/7568 Train_loss 1.8597633726838985 
Epoch [8/10] Batch 6100/7568 Train_loss 1.8602387228035142 
Epoch [8/10] Batch 6200/7568 Train_loss 1.8583944070903433 
Epoch [8/10] Batch 6300/7568 Train_loss 1.8592116880882281 
Epoch [8/10] Batch 6400/7568 Train_loss 1.8592991262739513 
Epoch [8/10] Batch 6500/7568 Train_loss 1.859514974017562 
Epoch [8/10] Batch 6600/7568 Train_loss 1.8590952763795274 
Epoch [8/10] Batch 6700/7568 Train_loss 1.8595406683816462 
Epoch [8/10] Batch 6800/7568 Train_loss 1.8585180014332434 
Epoch [8/10] Batch 6900/7568 Train_loss 1.8586444894682927 
Epoch [8/10] Batch 7000/7568 Train_loss 1.8589748751327935 
Epoch [8/10] Batch 7100/7568 Train_loss 1.8595738513606348 
Epoch [8/10] Batch 7200/7568 Train_loss 1.859417424188432 
Epoch [8/10] Batch 7300/7568 Train_loss 1.858221859187847 
Epoch [8/10] Batch 7400/7568 Train_loss 1.858498443951439 
Epoch [8/10] Batch 7500/7568 Train_loss 1.8591509160926272 
Epoch: 8/10 	Training Loss: 1.858875 	Validation Loss: 1.914525 Duration seconds: 1116.825432062149 
best_valid_loss_fold [1.8852142867244324] Best_Epoch [8]Epoch [9/10] Batch 0/7568 Train_loss 1.8461482226848602 
Epoch [9/10] Batch 100/7568 Train_loss 1.9340698716073934 
Epoch [9/10] Batch 200/7568 Train_loss 1.9504710005142203 
Epoch [9/10] Batch 300/7568 Train_loss 1.919163828127804 
Epoch [9/10] Batch 400/7568 Train_loss 1.8619817722022087 
Epoch [9/10] Batch 500/7568 Train_loss 1.850405149384887 
Epoch [9/10] Batch 600/7568 Train_loss 1.8546612888029133 
Epoch [9/10] Batch 700/7568 Train_loss 1.8520028863670823 
Epoch [9/10] Batch 800/7568 Train_loss 1.8547159167339144 
Epoch [9/10] Batch 900/7568 Train_loss 1.8481095685942985 
Epoch [9/10] Batch 1000/7568 Train_loss 1.8513391660405445 
Epoch [9/10] Batch 1100/7568 Train_loss 1.8501439436823535 
Epoch [9/10] Batch 1200/7568 Train_loss 1.851019063634142 
Epoch [9/10] Batch 1300/7568 Train_loss 1.8475842608180437 
Epoch [9/10] Batch 1400/7568 Train_loss 1.8486505259708539 
Epoch [9/10] Batch 1500/7568 Train_loss 1.847525168654523 
Epoch [9/10] Batch 1600/7568 Train_loss 1.8479139190271896 
Epoch [9/10] Batch 1700/7568 Train_loss 1.84448664641114 
Epoch [9/10] Batch 1800/7568 Train_loss 1.847114776080678 
Epoch [9/10] Batch 1900/7568 Train_loss 1.8482925079719823 
Epoch [9/10] Batch 2000/7568 Train_loss 1.852051462942752 
Epoch [9/10] Batch 2100/7568 Train_loss 1.856548931593273 
Epoch [9/10] Batch 2200/7568 Train_loss 1.8543524683022814 
Epoch [9/10] Batch 2300/7568 Train_loss 1.8560557786548515 
Epoch [9/10] Batch 2400/7568 Train_loss 1.8523981534828995 
Epoch [9/10] Batch 2500/7568 Train_loss 1.8547852640030908 
Epoch [9/10] Batch 2600/7568 Train_loss 1.8564293343360403 
Epoch [9/10] Batch 2700/7568 Train_loss 1.8559026075420182 
Epoch [9/10] Batch 2800/7568 Train_loss 1.8534373150728125 
Epoch [9/10] Batch 2900/7568 Train_loss 1.8521670263385164 
Epoch [9/10] Batch 3000/7568 Train_loss 1.853685564262515 
Epoch [9/10] Batch 3100/7568 Train_loss 1.8569692876099848 
Epoch [9/10] Batch 3200/7568 Train_loss 1.8562664536852942 
Epoch [9/10] Batch 3300/7568 Train_loss 1.8565020243693973 
Epoch [9/10] Batch 3400/7568 Train_loss 1.8573228227644414 
Epoch [9/10] Batch 3500/7568 Train_loss 1.8554801424399712 
Epoch [9/10] Batch 3600/7568 Train_loss 1.8529993211356544 
Epoch [9/10] Batch 3700/7568 Train_loss 1.85262022762927 
Epoch [9/10] Batch 3800/7568 Train_loss 1.851465529493911 
Epoch [9/10] Batch 3900/7568 Train_loss 1.85180759874369 
Epoch [9/10] Batch 4000/7568 Train_loss 1.8511991191937518 
Epoch [9/10] Batch 4100/7568 Train_loss 1.8514780694302249 
Epoch [9/10] Batch 4200/7568 Train_loss 1.853858181128003 
Epoch [9/10] Batch 4300/7568 Train_loss 1.8532222515880243 
Epoch [9/10] Batch 4400/7568 Train_loss 1.854997892106251 
Epoch [9/10] Batch 4500/7568 Train_loss 1.8537135037451393 
Epoch [9/10] Batch 4600/7568 Train_loss 1.852925202305862 
Epoch [9/10] Batch 4700/7568 Train_loss 1.853289952617748 
Epoch [9/10] Batch 4800/7568 Train_loss 1.8556285574659783 
Epoch [9/10] Batch 4900/7568 Train_loss 1.8563248313259553 
Epoch [9/10] Batch 5000/7568 Train_loss 1.8554007499245042 
Epoch [9/10] Batch 5100/7568 Train_loss 1.8551282932365836 
Epoch [9/10] Batch 5200/7568 Train_loss 1.856820125070484 
Epoch [9/10] Batch 5300/7568 Train_loss 1.8572525649917078 
Epoch [9/10] Batch 5400/7568 Train_loss 1.8568397450200984 
Epoch [9/10] Batch 5500/7568 Train_loss 1.8562894802384107 
Epoch [9/10] Batch 5600/7568 Train_loss 1.855064159755706 
Epoch [9/10] Batch 5700/7568 Train_loss 1.8538800018897204 
Epoch [9/10] Batch 5800/7568 Train_loss 1.851648921890313 
Epoch [9/10] Batch 5900/7568 Train_loss 1.851143685779194 
Epoch [9/10] Batch 6000/7568 Train_loss 1.8503142787409592 
Epoch [9/10] Batch 6100/7568 Train_loss 1.850888129238847 
Epoch [9/10] Batch 6200/7568 Train_loss 1.8515490577131908 
Epoch [9/10] Batch 6300/7568 Train_loss 1.8528654691795001 
Epoch [9/10] Batch 6400/7568 Train_loss 1.8515045203965748 
Epoch [9/10] Batch 6500/7568 Train_loss 1.8524427443669405 
Epoch [9/10] Batch 6600/7568 Train_loss 1.8519405910141125 
Epoch [9/10] Batch 6700/7568 Train_loss 1.8508918306650892 
Epoch [9/10] Batch 6800/7568 Train_loss 1.8500981889714425 
Epoch [9/10] Batch 6900/7568 Train_loss 1.8497514423944348 
Epoch [9/10] Batch 7000/7568 Train_loss 1.8489657608780745 
Epoch [9/10] Batch 7100/7568 Train_loss 1.8478841979936376 
Epoch [9/10] Batch 7200/7568 Train_loss 1.8484344937108015 
Epoch [9/10] Batch 7300/7568 Train_loss 1.8478377569691846 
Epoch [9/10] Batch 7400/7568 Train_loss 1.8473441864217501 
Epoch [9/10] Batch 7500/7568 Train_loss 1.8486051627472908 
Epoch: 9/10 	Training Loss: 1.848942 	Validation Loss: 1.957446 Duration seconds: 1262.796942949295 
best_valid_loss_fold [1.8852142867244324] Best_Epoch [9]Fold: 2/5 
Epoch [0/10] Batch 0/7568 Train_loss 1.2596593797206879 
Epoch [0/10] Batch 100/7568 Train_loss 1.9608988553875744 
Epoch [0/10] Batch 200/7568 Train_loss 1.9910118591726123 
Epoch [0/10] Batch 300/7568 Train_loss 1.9593462409866214 
Epoch [0/10] Batch 400/7568 Train_loss 1.9402039242429923 
Epoch [0/10] Batch 500/7568 Train_loss 1.944128918552589 
Epoch [0/10] Batch 600/7568 Train_loss 1.9637391967900382 
Epoch [0/10] Batch 700/7568 Train_loss 1.9432748178187518 
Epoch [0/10] Batch 800/7568 Train_loss 1.9219813714461975 
Epoch [0/10] Batch 900/7568 Train_loss 1.9177604848715362 
Epoch [0/10] Batch 1000/7568 Train_loss 1.908611946410828 
Epoch [0/10] Batch 1100/7568 Train_loss 1.9147194084202994 
Epoch [0/10] Batch 1200/7568 Train_loss 1.9070811190647845 
Epoch [0/10] Batch 1300/7568 Train_loss 1.9082954312910216 
Epoch [0/10] Batch 1400/7568 Train_loss 1.9112063394985737 
Epoch [0/10] Batch 1500/7568 Train_loss 1.9092732335848461 
Epoch [0/10] Batch 1600/7568 Train_loss 1.9101311963547176 
Epoch [0/10] Batch 1700/7568 Train_loss 1.905808026416732 
Epoch [0/10] Batch 1800/7568 Train_loss 1.9111837587344653 
Epoch [0/10] Batch 1900/7568 Train_loss 1.9105206976308124 
Epoch [0/10] Batch 2000/7568 Train_loss 1.9073687468854146 
Epoch [0/10] Batch 2100/7568 Train_loss 1.9081740329899373 
Epoch [0/10] Batch 2200/7568 Train_loss 1.9063722431280568 
Epoch [0/10] Batch 2300/7568 Train_loss 1.904619934991721 
Epoch [0/10] Batch 2400/7568 Train_loss 1.9026023878412215 
Epoch [0/10] Batch 2500/7568 Train_loss 1.902995092726526 
Epoch [0/10] Batch 2600/7568 Train_loss 1.906198154002004 
Epoch [0/10] Batch 2700/7568 Train_loss 1.901068719519769 
Epoch [0/10] Batch 2800/7568 Train_loss 1.897965926744818 
Epoch [0/10] Batch 2900/7568 Train_loss 1.8946052834478915 
Epoch [0/10] Batch 3000/7568 Train_loss 1.892343427128134 
Epoch [0/10] Batch 3100/7568 Train_loss 1.8899810191643927 
Epoch [0/10] Batch 3200/7568 Train_loss 1.8914712768575244 
Epoch [0/10] Batch 3300/7568 Train_loss 1.892310414498266 
Epoch [0/10] Batch 3400/7568 Train_loss 1.8920708536856388 
Epoch [0/10] Batch 3500/7568 Train_loss 1.8901938658608808 
Epoch [0/10] Batch 3600/7568 Train_loss 1.8874537941897058 
Epoch [0/10] Batch 3700/7568 Train_loss 1.888705469895395 
Epoch [0/10] Batch 3800/7568 Train_loss 1.8899148065764413 
Epoch [0/10] Batch 3900/7568 Train_loss 1.8925718040656994 
Epoch [0/10] Batch 4000/7568 Train_loss 1.8907481604771328 
Epoch [0/10] Batch 4100/7568 Train_loss 1.8883641513272977 
Epoch [0/10] Batch 4200/7568 Train_loss 1.8892189497303264 
Epoch [0/10] Batch 4300/7568 Train_loss 1.8888675415153504 
Epoch [0/10] Batch 4400/7568 Train_loss 1.8882503231931913 
Epoch [0/10] Batch 4500/7568 Train_loss 1.8880551384620814 
Epoch [0/10] Batch 4600/7568 Train_loss 1.8889680545687209 
Epoch [0/10] Batch 4700/7568 Train_loss 1.8877328822373 
Epoch [0/10] Batch 4800/7568 Train_loss 1.8876417664435847 
Epoch [0/10] Batch 4900/7568 Train_loss 1.8882983034923961 
Epoch [0/10] Batch 5000/7568 Train_loss 1.88700146416686 
Epoch [0/10] Batch 5100/7568 Train_loss 1.8861397712403267 
Epoch [0/10] Batch 5200/7568 Train_loss 1.8860164042966936 
Epoch [0/10] Batch 5300/7568 Train_loss 1.886457885230595 
Epoch [0/10] Batch 5400/7568 Train_loss 1.88425572865281 
Epoch [0/10] Batch 5500/7568 Train_loss 1.8835899200161095 
Epoch [0/10] Batch 5600/7568 Train_loss 1.8823984059342378 
Epoch [0/10] Batch 5700/7568 Train_loss 1.8822319579072697 
Epoch [0/10] Batch 5800/7568 Train_loss 1.8827251523894968 
Epoch [0/10] Batch 5900/7568 Train_loss 1.8831147978054206 
Epoch [0/10] Batch 6000/7568 Train_loss 1.8818171595674398 
Epoch [0/10] Batch 6100/7568 Train_loss 1.8808668481155197 
Epoch [0/10] Batch 6200/7568 Train_loss 1.8807160447207003 
Epoch [0/10] Batch 6300/7568 Train_loss 1.8808037109721503 
Epoch [0/10] Batch 6400/7568 Train_loss 1.880785700517499 
Epoch [0/10] Batch 6500/7568 Train_loss 1.8808885042407644 
Epoch [0/10] Batch 6600/7568 Train_loss 1.8790985779816174 
Epoch [0/10] Batch 6700/7568 Train_loss 1.8784303218404261 
Epoch [0/10] Batch 6800/7568 Train_loss 1.8784050274420672 
Epoch [0/10] Batch 6900/7568 Train_loss 1.8799741320509598 
Epoch [0/10] Batch 7000/7568 Train_loss 1.8797440863342272 
Epoch [0/10] Batch 7100/7568 Train_loss 1.8791460799054098 
Epoch [0/10] Batch 7200/7568 Train_loss 1.8781066179087205 
Epoch [0/10] Batch 7300/7568 Train_loss 1.8780231927855098 
Epoch [0/10] Batch 7400/7568 Train_loss 1.8784672336207662 
Epoch [0/10] Batch 7500/7568 Train_loss 1.8786935280045498 
Epoch: 0/10 	Training Loss: 1.878258 	Validation Loss: 2.422300 Duration seconds: 1051.1822984218597 
Validation loss decreased (inf --> 2.422300).  Saving model ... 
best_valid_loss_fold [2.422300474013322] Best_Epoch [0]Epoch [1/10] Batch 0/7568 Train_loss 1.9623363018035889 
Epoch [1/10] Batch 100/7568 Train_loss 1.8889371025385242 
Epoch [1/10] Batch 200/7568 Train_loss 1.8834087200722291 
Epoch [1/10] Batch 300/7568 Train_loss 1.8610692462354799 
Epoch [1/10] Batch 400/7568 Train_loss 1.8725951614225298 
Epoch [1/10] Batch 500/7568 Train_loss 1.8660997526493377 
Epoch [1/10] Batch 600/7568 Train_loss 1.86410884430226 
Epoch [1/10] Batch 700/7568 Train_loss 1.8664862267598616 
Epoch [1/10] Batch 800/7568 Train_loss 1.855411964204725 
Epoch [1/10] Batch 900/7568 Train_loss 1.8549066084901182 
Epoch [1/10] Batch 1000/7568 Train_loss 1.8530211371409666 
Epoch [1/10] Batch 1100/7568 Train_loss 1.8522669705177415 
Epoch [1/10] Batch 1200/7568 Train_loss 1.8517076413299915 
Epoch [1/10] Batch 1300/7568 Train_loss 1.8527108862663946 
Epoch [1/10] Batch 1400/7568 Train_loss 1.8563232742659455 
Epoch [1/10] Batch 1500/7568 Train_loss 1.8607548723552005 
Epoch [1/10] Batch 1600/7568 Train_loss 1.8658999335963156 
Epoch [1/10] Batch 1700/7568 Train_loss 1.8665445057225185 
Epoch [1/10] Batch 1800/7568 Train_loss 1.8633372276847657 
Epoch [1/10] Batch 1900/7568 Train_loss 1.8624042721366267 
Epoch [1/10] Batch 2000/7568 Train_loss 1.8668169735454727 
Epoch [1/10] Batch 2100/7568 Train_loss 1.8656457808650657 
Epoch [1/10] Batch 2200/7568 Train_loss 1.8605981792057984 
Epoch [1/10] Batch 2300/7568 Train_loss 1.8595686666133195 
Epoch [1/10] Batch 2400/7568 Train_loss 1.8614371852799536 
Epoch [1/10] Batch 2500/7568 Train_loss 1.8618634218474428 
Epoch [1/10] Batch 2600/7568 Train_loss 1.8642567385593913 
Epoch [1/10] Batch 2700/7568 Train_loss 1.8634612957461814 
Epoch [1/10] Batch 2800/7568 Train_loss 1.86236607358365 
Epoch [1/10] Batch 2900/7568 Train_loss 1.8623793828732924 
Epoch [1/10] Batch 3000/7568 Train_loss 1.8643519336587984 
Epoch [1/10] Batch 3100/7568 Train_loss 1.8641469628459713 
Epoch [1/10] Batch 3200/7568 Train_loss 1.863549466849174 
Epoch [1/10] Batch 3300/7568 Train_loss 1.8628276560666597 
Epoch [1/10] Batch 3400/7568 Train_loss 1.8662354204408282 
Epoch [1/10] Batch 3500/7568 Train_loss 1.8655505495917897 
Epoch [1/10] Batch 3600/7568 Train_loss 1.8659369861440704 
Epoch [1/10] Batch 3700/7568 Train_loss 1.865466450751472 
Epoch [1/10] Batch 3800/7568 Train_loss 1.8621355997507776 
Epoch [1/10] Batch 3900/7568 Train_loss 1.8603443652315037 
Epoch [1/10] Batch 4000/7568 Train_loss 1.8587855739024126 
Epoch [1/10] Batch 4100/7568 Train_loss 1.8586493579462313 
Epoch [1/10] Batch 4200/7568 Train_loss 1.858276335047671 
Epoch [1/10] Batch 4300/7568 Train_loss 1.859644766627592 
Epoch [1/10] Batch 4400/7568 Train_loss 1.8578065433398725 
Epoch [1/10] Batch 4500/7568 Train_loss 1.8573824814679065 
Epoch [1/10] Batch 4600/7568 Train_loss 1.8565112123396623 
Epoch [1/10] Batch 4700/7568 Train_loss 1.8574450033170267 
Epoch [1/10] Batch 4800/7568 Train_loss 1.8592883948949792 
Epoch [1/10] Batch 4900/7568 Train_loss 1.861365078827771 
Epoch [1/10] Batch 5000/7568 Train_loss 1.8617415466300966 
Epoch [1/10] Batch 5100/7568 Train_loss 1.8608271311461002 
Epoch [1/10] Batch 5200/7568 Train_loss 1.8605855995761593 
Epoch [1/10] Batch 5300/7568 Train_loss 1.86016503765749 
Epoch [1/10] Batch 5400/7568 Train_loss 1.8585389944949078 
Epoch [1/10] Batch 5500/7568 Train_loss 1.857193863756482 
Epoch [1/10] Batch 5600/7568 Train_loss 1.8558816634114566 
Epoch [1/10] Batch 5700/7568 Train_loss 1.856912384428825 
Epoch [1/10] Batch 5800/7568 Train_loss 1.8555146698765952 
Epoch [1/10] Batch 5900/7568 Train_loss 1.8552891592154401 
Epoch [1/10] Batch 6000/7568 Train_loss 1.8560781601522787 
Epoch [1/10] Batch 6100/7568 Train_loss 1.857673547791958 
Epoch [1/10] Batch 6200/7568 Train_loss 1.8568526973764352 
Epoch [1/10] Batch 6300/7568 Train_loss 1.855515053697076 
Epoch [1/10] Batch 6400/7568 Train_loss 1.8550851645576378 
Epoch [1/10] Batch 6500/7568 Train_loss 1.8567646174380603 
Epoch [1/10] Batch 6600/7568 Train_loss 1.8560465491733629 
Epoch [1/10] Batch 6700/7568 Train_loss 1.8566189792494758 
Epoch [1/10] Batch 6800/7568 Train_loss 1.8564504517661071 
Epoch [1/10] Batch 6900/7568 Train_loss 1.8558839669915632 
Epoch [1/10] Batch 7000/7568 Train_loss 1.8570229722408547 
Epoch [1/10] Batch 7100/7568 Train_loss 1.8569541113877897 
Epoch [1/10] Batch 7200/7568 Train_loss 1.8552808915822974 
Epoch [1/10] Batch 7300/7568 Train_loss 1.8548171584176913 
Epoch [1/10] Batch 7400/7568 Train_loss 1.8547859329188165 
Epoch [1/10] Batch 7500/7568 Train_loss 1.8550368923176894 
Epoch: 1/10 	Training Loss: 1.854375 	Validation Loss: 2.062361 Duration seconds: 1291.9548106193542 
Validation loss decreased (2.422300 --> 2.062361).  Saving model ... 
best_valid_loss_fold [2.0623608231623254] Best_Epoch [1]Epoch [2/10] Batch 0/7568 Train_loss 1.8142278492450714 
Epoch [2/10] Batch 100/7568 Train_loss 1.9326422016809481 
Epoch [2/10] Batch 200/7568 Train_loss 1.918796669710335 
Epoch [2/10] Batch 300/7568 Train_loss 1.8668502467712296 
Epoch [2/10] Batch 400/7568 Train_loss 1.8569602891616988 
Epoch [2/10] Batch 500/7568 Train_loss 1.8297811921782599 
Epoch [2/10] Batch 600/7568 Train_loss 1.8307968555095786 
Epoch [2/10] Batch 700/7568 Train_loss 1.8326746387548012 
Epoch [2/10] Batch 800/7568 Train_loss 1.8416200742851037 
Epoch [2/10] Batch 900/7568 Train_loss 1.8445131437363027 
Epoch [2/10] Batch 1000/7568 Train_loss 1.8458870368136036 
Epoch [2/10] Batch 1100/7568 Train_loss 1.837629176791314 
Epoch [2/10] Batch 1200/7568 Train_loss 1.8441577691221913 
Epoch [2/10] Batch 1300/7568 Train_loss 1.8428858683418072 
Epoch [2/10] Batch 1400/7568 Train_loss 1.847496319357452 
Epoch [2/10] Batch 1500/7568 Train_loss 1.8498723859651178 
Epoch [2/10] Batch 1600/7568 Train_loss 1.8552705364477478 
Epoch [2/10] Batch 1700/7568 Train_loss 1.847163177128752 
Epoch [2/10] Batch 1800/7568 Train_loss 1.8443700762621764 
Epoch [2/10] Batch 1900/7568 Train_loss 1.8417795957984704 
Epoch [2/10] Batch 2000/7568 Train_loss 1.8447291395296042 
Epoch [2/10] Batch 2100/7568 Train_loss 1.8527601950441526 
Epoch [2/10] Batch 2200/7568 Train_loss 1.8524953920642986 
Epoch [2/10] Batch 2300/7568 Train_loss 1.8571543388201432 
Epoch [2/10] Batch 2400/7568 Train_loss 1.8517259341734045 
Epoch [2/10] Batch 2500/7568 Train_loss 1.847301561196224 
Epoch [2/10] Batch 2600/7568 Train_loss 1.8483815988563703 
Epoch [2/10] Batch 2700/7568 Train_loss 1.844981428813665 
Epoch [2/10] Batch 2800/7568 Train_loss 1.8445015193143728 
Epoch [2/10] Batch 2900/7568 Train_loss 1.8430647394089854 
Epoch [2/10] Batch 3000/7568 Train_loss 1.8444119162562091 
Epoch [2/10] Batch 3100/7568 Train_loss 1.8429538798486944 
Epoch [2/10] Batch 3200/7568 Train_loss 1.843709022712145 
Epoch [2/10] Batch 3300/7568 Train_loss 1.840405834726459 
Epoch [2/10] Batch 3400/7568 Train_loss 1.838488683194334 
Epoch [2/10] Batch 3500/7568 Train_loss 1.8384830324339445 
Epoch [2/10] Batch 3600/7568 Train_loss 1.837574258287378 
Epoch [2/10] Batch 3700/7568 Train_loss 1.8365591485531418 
Epoch [2/10] Batch 3800/7568 Train_loss 1.8380644539666628 
Epoch [2/10] Batch 3900/7568 Train_loss 1.8360934448823565 
Epoch [2/10] Batch 4000/7568 Train_loss 1.8366175651516774 
Epoch [2/10] Batch 4100/7568 Train_loss 1.8368459553692842 
Epoch [2/10] Batch 4200/7568 Train_loss 1.836599624398506 
Epoch [2/10] Batch 4300/7568 Train_loss 1.8348985378342682 
Epoch [2/10] Batch 4400/7568 Train_loss 1.8350734031033147 
Epoch [2/10] Batch 4500/7568 Train_loss 1.835378615305215 
Epoch [2/10] Batch 4600/7568 Train_loss 1.8358025537677056 
Epoch [2/10] Batch 4700/7568 Train_loss 1.8358259107715473 
Epoch [2/10] Batch 4800/7568 Train_loss 1.8350227842201692 
Epoch [2/10] Batch 4900/7568 Train_loss 1.8361776876017235 
Epoch [2/10] Batch 5000/7568 Train_loss 1.8365773987311813 
Epoch [2/10] Batch 5100/7568 Train_loss 1.834963839132338 
Epoch [2/10] Batch 5200/7568 Train_loss 1.8354379784167558 
Epoch [2/10] Batch 5300/7568 Train_loss 1.8350318383086044 
Epoch [2/10] Batch 5400/7568 Train_loss 1.8348755115749018 
Epoch [2/10] Batch 5500/7568 Train_loss 1.833883761144718 
Epoch [2/10] Batch 5600/7568 Train_loss 1.8339147611420255 
Epoch [2/10] Batch 5700/7568 Train_loss 1.8341301650742003 
Epoch [2/10] Batch 5800/7568 Train_loss 1.8353831729183956 
Epoch [2/10] Batch 5900/7568 Train_loss 1.836489720777917 
Epoch [2/10] Batch 6000/7568 Train_loss 1.8384153683994378 
Epoch [2/10] Batch 6100/7568 Train_loss 1.8378970738505853 
Epoch [2/10] Batch 6200/7568 Train_loss 1.8391307365342102 
Epoch [2/10] Batch 6300/7568 Train_loss 1.8409438076234113 
Epoch [2/10] Batch 6400/7568 Train_loss 1.842013314998377 
Epoch [2/10] Batch 6500/7568 Train_loss 1.8427872018098665 
Epoch [2/10] Batch 6600/7568 Train_loss 1.8424639672914558 
Epoch [2/10] Batch 6700/7568 Train_loss 1.8429100235326221 
Epoch [2/10] Batch 6800/7568 Train_loss 1.8424407501579418 
Epoch [2/10] Batch 6900/7568 Train_loss 1.8426438448487754 
Epoch [2/10] Batch 7000/7568 Train_loss 1.8430933840663293 
Epoch [2/10] Batch 7100/7568 Train_loss 1.8428805458138164 
Epoch [2/10] Batch 7200/7568 Train_loss 1.8430302513991192 
Epoch [2/10] Batch 7300/7568 Train_loss 1.843540469038333 
Epoch [2/10] Batch 7400/7568 Train_loss 1.843441849391479 
Epoch [2/10] Batch 7500/7568 Train_loss 1.8423126441018214 
Epoch: 2/10 	Training Loss: 1.841725 	Validation Loss: 2.012548 Duration seconds: 1229.5089137554169 
Validation loss decreased (2.062361 --> 2.012548).  Saving model ... 
best_valid_loss_fold [2.0125476384523617] Best_Epoch [2]Epoch [3/10] Batch 0/7568 Train_loss 1.4105120301246643 
Epoch [3/10] Batch 100/7568 Train_loss 1.904836792019334 
Epoch [3/10] Batch 200/7568 Train_loss 1.852601318439441 
Epoch [3/10] Batch 300/7568 Train_loss 1.8292894046072945 
Epoch [3/10] Batch 400/7568 Train_loss 1.8273021598483559 
Epoch [3/10] Batch 500/7568 Train_loss 1.823345877958867 
Epoch [3/10] Batch 600/7568 Train_loss 1.8079975934175405 
Epoch [3/10] Batch 700/7568 Train_loss 1.8180180033841589 
Epoch [3/10] Batch 800/7568 Train_loss 1.8123042222563546 
Epoch [3/10] Batch 900/7568 Train_loss 1.8119535934217232 
Epoch [3/10] Batch 1000/7568 Train_loss 1.8133701103461253 
Epoch [3/10] Batch 1100/7568 Train_loss 1.8133140816188313 
Epoch [3/10] Batch 1200/7568 Train_loss 1.8232583740321324 
Epoch [3/10] Batch 1300/7568 Train_loss 1.8205898221321604 
Epoch [3/10] Batch 1400/7568 Train_loss 1.819660652968647 
Epoch [3/10] Batch 1500/7568 Train_loss 1.8252502728270976 
Epoch [3/10] Batch 1600/7568 Train_loss 1.8296324257866283 
Epoch [3/10] Batch 1700/7568 Train_loss 1.8268754961366587 
Epoch [3/10] Batch 1800/7568 Train_loss 1.8221841087313508 
Epoch [3/10] Batch 1900/7568 Train_loss 1.8272702955591371 
Epoch [3/10] Batch 2000/7568 Train_loss 1.82789311732637 
Epoch [3/10] Batch 2100/7568 Train_loss 1.8256218922011969 
Epoch [3/10] Batch 2200/7568 Train_loss 1.8261744424375768 
Epoch [3/10] Batch 2300/7568 Train_loss 1.8220401985464745 
Epoch [3/10] Batch 2400/7568 Train_loss 1.819583589117237 
Epoch [3/10] Batch 2500/7568 Train_loss 1.822890083013964 
Epoch [3/10] Batch 2600/7568 Train_loss 1.825087361885625 
Epoch [3/10] Batch 2700/7568 Train_loss 1.8246821220171139 
Epoch [3/10] Batch 2800/7568 Train_loss 1.8262325457215138 
Epoch [3/10] Batch 2900/7568 Train_loss 1.8236808156513091 
Epoch [3/10] Batch 3000/7568 Train_loss 1.8202622863192512 
Epoch [3/10] Batch 3100/7568 Train_loss 1.8195681372445385 
Epoch [3/10] Batch 3200/7568 Train_loss 1.8181878023876021 
Epoch [3/10] Batch 3300/7568 Train_loss 1.8187248386765422 
Epoch [3/10] Batch 3400/7568 Train_loss 1.8184857024441254 
Epoch [3/10] Batch 3500/7568 Train_loss 1.817982664549395 
Epoch [3/10] Batch 3600/7568 Train_loss 1.819871766381514 
Epoch [3/10] Batch 3700/7568 Train_loss 1.8198300740547968 
Epoch [3/10] Batch 3800/7568 Train_loss 1.8191969340485323 
Epoch [3/10] Batch 3900/7568 Train_loss 1.819075644216807 
Epoch [3/10] Batch 4000/7568 Train_loss 1.8197931296112269 
Epoch [3/10] Batch 4100/7568 Train_loss 1.8205939435237242 
Epoch [3/10] Batch 4200/7568 Train_loss 1.819167891151492 
Epoch [3/10] Batch 4300/7568 Train_loss 1.8176562991967675 
Epoch [3/10] Batch 4400/7568 Train_loss 1.8177272581703363 
Epoch [3/10] Batch 4500/7568 Train_loss 1.819206113283355 
Epoch [3/10] Batch 4600/7568 Train_loss 1.8210378011667943 
Epoch [3/10] Batch 4700/7568 Train_loss 1.8213497473917928 
Epoch [3/10] Batch 4800/7568 Train_loss 1.8214862339138067 
Epoch [3/10] Batch 4900/7568 Train_loss 1.8231584571516288 
Epoch [3/10] Batch 5000/7568 Train_loss 1.8234418778330344 
Epoch [3/10] Batch 5100/7568 Train_loss 1.8244487060048817 
Epoch [3/10] Batch 5200/7568 Train_loss 1.8255192921972347 
Epoch [3/10] Batch 5300/7568 Train_loss 1.825032595659602 
Epoch [3/10] Batch 5400/7568 Train_loss 1.825461764770776 
Epoch [3/10] Batch 5500/7568 Train_loss 1.8250693072163826 
Epoch [3/10] Batch 5600/7568 Train_loss 1.8256898435593247 
Epoch [3/10] Batch 5700/7568 Train_loss 1.8254669006979898 
Epoch [3/10] Batch 5800/7568 Train_loss 1.8257919329314412 
Epoch [3/10] Batch 5900/7568 Train_loss 1.8259895661865688 
Epoch [3/10] Batch 6000/7568 Train_loss 1.8265852682109953 
Epoch [3/10] Batch 6100/7568 Train_loss 1.8275050359013785 
Epoch [3/10] Batch 6200/7568 Train_loss 1.8277067266232812 
Epoch [3/10] Batch 6300/7568 Train_loss 1.8282014675233842 
Epoch [3/10] Batch 6400/7568 Train_loss 1.8285119263497465 
Epoch [3/10] Batch 6500/7568 Train_loss 1.8290286530895135 
Epoch [3/10] Batch 6600/7568 Train_loss 1.828667466226644 
Epoch [3/10] Batch 6700/7568 Train_loss 1.829354042300465 
Epoch [3/10] Batch 6800/7568 Train_loss 1.8277908733078723 
Epoch [3/10] Batch 6900/7568 Train_loss 1.8284981178599948 
Epoch [3/10] Batch 7000/7568 Train_loss 1.8279933337260017 
Epoch [3/10] Batch 7100/7568 Train_loss 1.8284063005483902 
Epoch [3/10] Batch 7200/7568 Train_loss 1.8294326320017416 
Epoch [3/10] Batch 7300/7568 Train_loss 1.8294839481030356 
Epoch [3/10] Batch 7400/7568 Train_loss 1.8299273210477627 
Epoch [3/10] Batch 7500/7568 Train_loss 1.8302346072935085 
Epoch: 3/10 	Training Loss: 1.828586 	Validation Loss: 1.816964 Duration seconds: 1231.4107508659363 
Validation loss decreased (2.012548 --> 1.816964).  Saving model ... 
best_valid_loss_fold [1.8169636676525087] Best_Epoch [3]Epoch [4/10] Batch 0/7568 Train_loss 2.2696126997470856 
Epoch [4/10] Batch 100/7568 Train_loss 1.8728318559651327 
Epoch [4/10] Batch 200/7568 Train_loss 1.816140743926983 
Epoch [4/10] Batch 300/7568 Train_loss 1.829273355908172 
Epoch [4/10] Batch 400/7568 Train_loss 1.8223284755338753 
Epoch [4/10] Batch 500/7568 Train_loss 1.8306041935722748 
Epoch [4/10] Batch 600/7568 Train_loss 1.8102387780208953 
Epoch [4/10] Batch 700/7568 Train_loss 1.8048875227294192 
Epoch [4/10] Batch 800/7568 Train_loss 1.796997304210502 
Epoch [4/10] Batch 900/7568 Train_loss 1.8004946569425284 
Epoch [4/10] Batch 1000/7568 Train_loss 1.793643923608454 
Epoch [4/10] Batch 1100/7568 Train_loss 1.7907178535258954 
Epoch [4/10] Batch 1200/7568 Train_loss 1.7907693029864444 
Epoch [4/10] Batch 1300/7568 Train_loss 1.7957207484945346 
Epoch [4/10] Batch 1400/7568 Train_loss 1.7975002747138087 
Epoch [4/10] Batch 1500/7568 Train_loss 1.799514741425233 
Epoch [4/10] Batch 1600/7568 Train_loss 1.7973868085128453 
Epoch [4/10] Batch 1700/7568 Train_loss 1.7996846256161143 
Epoch [4/10] Batch 1800/7568 Train_loss 1.8034108399906337 
Epoch [4/10] Batch 1900/7568 Train_loss 1.8037598354958473 
Epoch [4/10] Batch 2000/7568 Train_loss 1.8050659737777317 
Epoch [4/10] Batch 2100/7568 Train_loss 1.8008204879525558 
Epoch [4/10] Batch 2200/7568 Train_loss 1.7999936470500397 
Epoch [4/10] Batch 2300/7568 Train_loss 1.8010201185507808 
Epoch [4/10] Batch 2400/7568 Train_loss 1.8070801450052096 
Epoch [4/10] Batch 2500/7568 Train_loss 1.8079399526184772 
Epoch [4/10] Batch 2600/7568 Train_loss 1.8091237631091839 
Epoch [4/10] Batch 2700/7568 Train_loss 1.8082889320755702 
Epoch [4/10] Batch 2800/7568 Train_loss 1.8078641627370906 
Epoch [4/10] Batch 2900/7568 Train_loss 1.8052653158683893 
Epoch [4/10] Batch 3000/7568 Train_loss 1.8079638742940778 
Epoch [4/10] Batch 3100/7568 Train_loss 1.8080900382804355 
Epoch [4/10] Batch 3200/7568 Train_loss 1.8099828963701854 
Epoch [4/10] Batch 3300/7568 Train_loss 1.8105812957050476 
Epoch [4/10] Batch 3400/7568 Train_loss 1.811374716408128 
Epoch [4/10] Batch 3500/7568 Train_loss 1.8150791278871357 
Epoch [4/10] Batch 3600/7568 Train_loss 1.814906246478079 
Epoch [4/10] Batch 3700/7568 Train_loss 1.814442361103332 
Epoch [4/10] Batch 3800/7568 Train_loss 1.8125755229405716 
Epoch [4/10] Batch 3900/7568 Train_loss 1.8136612134628711 
Epoch [4/10] Batch 4000/7568 Train_loss 1.813963756600236 
Epoch [4/10] Batch 4100/7568 Train_loss 1.8146013787074864 
Epoch [4/10] Batch 4200/7568 Train_loss 1.814073725206473 
Epoch [4/10] Batch 4300/7568 Train_loss 1.8137402744594355 
Epoch [4/10] Batch 4400/7568 Train_loss 1.813587496500277 
Epoch [4/10] Batch 4500/7568 Train_loss 1.8140047892656652 
Epoch [4/10] Batch 4600/7568 Train_loss 1.8116783055743428 
Epoch [4/10] Batch 4700/7568 Train_loss 1.8117186812166077 
Epoch [4/10] Batch 4800/7568 Train_loss 1.8115018240578273 
Epoch [4/10] Batch 4900/7568 Train_loss 1.8127277158953248 
Epoch [4/10] Batch 5000/7568 Train_loss 1.812432029330535 
Epoch [4/10] Batch 5100/7568 Train_loss 1.8124117905781962 
Epoch [4/10] Batch 5200/7568 Train_loss 1.8139706986729422 
Epoch [4/10] Batch 5300/7568 Train_loss 1.8123354359904655 
Epoch [4/10] Batch 5400/7568 Train_loss 1.8137142665227615 
Epoch [4/10] Batch 5500/7568 Train_loss 1.8149086724580081 
Epoch [4/10] Batch 5600/7568 Train_loss 1.8155688551497169 
Epoch [4/10] Batch 5700/7568 Train_loss 1.81590886760769 
Epoch [4/10] Batch 5800/7568 Train_loss 1.815654119726163 
Epoch [4/10] Batch 5900/7568 Train_loss 1.8157656770783488 
Epoch [4/10] Batch 6000/7568 Train_loss 1.8152445350683464 
Epoch [4/10] Batch 6100/7568 Train_loss 1.814601286309085 
Epoch [4/10] Batch 6200/7568 Train_loss 1.8142663924706268 
Epoch [4/10] Batch 6300/7568 Train_loss 1.8153211788416685 
Epoch [4/10] Batch 6400/7568 Train_loss 1.8141432380668354 
Epoch [4/10] Batch 6500/7568 Train_loss 1.815341651683779 
Epoch [4/10] Batch 6600/7568 Train_loss 1.815641000951543 
Epoch [4/10] Batch 6700/7568 Train_loss 1.8170277807078261 
Epoch [4/10] Batch 6800/7568 Train_loss 1.8186549295751668 
Epoch [4/10] Batch 6900/7568 Train_loss 1.8179491055361985 
Epoch [4/10] Batch 7000/7568 Train_loss 1.8185577750655093 
Epoch [4/10] Batch 7100/7568 Train_loss 1.8182204280207568 
Epoch [4/10] Batch 7200/7568 Train_loss 1.819410835099318 
Epoch [4/10] Batch 7300/7568 Train_loss 1.819589771544659 
Epoch [4/10] Batch 7400/7568 Train_loss 1.8202079724786717 
Epoch [4/10] Batch 7500/7568 Train_loss 1.8199022450054618 
Epoch: 4/10 	Training Loss: 1.819509 	Validation Loss: 1.825838 Duration seconds: 1250.2827117443085 
best_valid_loss_fold [1.8169636676525087] Best_Epoch [4]Epoch [5/10] Batch 0/7568 Train_loss 2.003895938396454 
Epoch [5/10] Batch 100/7568 Train_loss 1.8021871835583507 
Epoch [5/10] Batch 200/7568 Train_loss 1.850808675757688 
Epoch [5/10] Batch 300/7568 Train_loss 1.8450296063755833 
Epoch [5/10] Batch 400/7568 Train_loss 1.8393575201308043 
Epoch [5/10] Batch 500/7568 Train_loss 1.8333409638878353 
Epoch [5/10] Batch 600/7568 Train_loss 1.8319147341139306 
Epoch [5/10] Batch 700/7568 Train_loss 1.8248319763493606 
Epoch [5/10] Batch 800/7568 Train_loss 1.8142602474111296 
Epoch [5/10] Batch 900/7568 Train_loss 1.8089960681297275 
Epoch [5/10] Batch 1000/7568 Train_loss 1.809539265082135 
Epoch [5/10] Batch 1100/7568 Train_loss 1.813335596709821 
Epoch [5/10] Batch 1200/7568 Train_loss 1.8090502210240578 
Epoch [5/10] Batch 1300/7568 Train_loss 1.8137063601159482 
Epoch [5/10] Batch 1400/7568 Train_loss 1.8147297138394756 
Epoch [5/10] Batch 1500/7568 Train_loss 1.8137155595588572 
Epoch [5/10] Batch 1600/7568 Train_loss 1.8142193589571265 
Epoch [5/10] Batch 1700/7568 Train_loss 1.8082806148480346 
Epoch [5/10] Batch 1800/7568 Train_loss 1.8060937113253226 
Epoch [5/10] Batch 1900/7568 Train_loss 1.806043860792079 
Epoch [5/10] Batch 2000/7568 Train_loss 1.8044614210829086 
Epoch [5/10] Batch 2100/7568 Train_loss 1.803840784775337 
Epoch [5/10] Batch 2200/7568 Train_loss 1.8046233705712524 
Epoch [5/10] Batch 2300/7568 Train_loss 1.8063386260446441 
Epoch [5/10] Batch 2400/7568 Train_loss 1.8076026420582587 
Epoch [5/10] Batch 2500/7568 Train_loss 1.806776248877857 
Epoch [5/10] Batch 2600/7568 Train_loss 1.806308468506571 
Epoch [5/10] Batch 2700/7568 Train_loss 1.8103215641382446 
Epoch [5/10] Batch 2800/7568 Train_loss 1.8133956152029396 
Epoch [5/10] Batch 2900/7568 Train_loss 1.8124323131138396 
Epoch [5/10] Batch 3000/7568 Train_loss 1.8139911093306875 
Epoch [5/10] Batch 3100/7568 Train_loss 1.8151391492673021 
Epoch [5/10] Batch 3200/7568 Train_loss 1.8155905159794885 
Epoch [5/10] Batch 3300/7568 Train_loss 1.815543573985079 
Epoch [5/10] Batch 3400/7568 Train_loss 1.8152476055342217 
Epoch [5/10] Batch 3500/7568 Train_loss 1.8163884438891404 
Epoch [5/10] Batch 3600/7568 Train_loss 1.814868969108803 
Epoch [5/10] Batch 3700/7568 Train_loss 1.8148272618111871 
Epoch [5/10] Batch 3800/7568 Train_loss 1.8124988989821706 
Epoch [5/10] Batch 3900/7568 Train_loss 1.8145837634133395 
Epoch [5/10] Batch 4000/7568 Train_loss 1.8143707511667697 
Epoch [5/10] Batch 4100/7568 Train_loss 1.8145227892893432 
Epoch [5/10] Batch 4200/7568 Train_loss 1.8136875799346526 
Epoch [5/10] Batch 4300/7568 Train_loss 1.8143665008861862 
Epoch [5/10] Batch 4400/7568 Train_loss 1.816486797183989 
Epoch [5/10] Batch 4500/7568 Train_loss 1.8171290116634164 
Epoch [5/10] Batch 4600/7568 Train_loss 1.8171679226681996 
Epoch [5/10] Batch 4700/7568 Train_loss 1.8174636595773712 
Epoch [5/10] Batch 4800/7568 Train_loss 1.8174790982773865 
Epoch [5/10] Batch 4900/7568 Train_loss 1.8173645260950737 
Epoch [5/10] Batch 5000/7568 Train_loss 1.8172562267080805 
Epoch [5/10] Batch 5100/7568 Train_loss 1.8186275850652105 
Epoch [5/10] Batch 5200/7568 Train_loss 1.8184009044603233 
Epoch [5/10] Batch 5300/7568 Train_loss 1.8191141191813052 
Epoch [5/10] Batch 5400/7568 Train_loss 1.8188269769021415 
Epoch [5/10] Batch 5500/7568 Train_loss 1.8189485018087221 
Epoch [5/10] Batch 5600/7568 Train_loss 1.819480732728479 
Epoch [5/10] Batch 5700/7568 Train_loss 1.8195904331656532 
Epoch [5/10] Batch 5800/7568 Train_loss 1.8188136038536595 
Epoch [5/10] Batch 5900/7568 Train_loss 1.8191275873846915 
Epoch [5/10] Batch 6000/7568 Train_loss 1.8181951415588946 
Epoch [5/10] Batch 6100/7568 Train_loss 1.8168811272012173 
Epoch [5/10] Batch 6200/7568 Train_loss 1.8156516878602678 
Epoch [5/10] Batch 6300/7568 Train_loss 1.8159232263235587 
Epoch [5/10] Batch 6400/7568 Train_loss 1.8149238228716436 
Epoch [5/10] Batch 6500/7568 Train_loss 1.813687576421397 
Epoch [5/10] Batch 6600/7568 Train_loss 1.8135339204984404 
Epoch [5/10] Batch 6700/7568 Train_loss 1.8124453640041058 
Epoch [5/10] Batch 6800/7568 Train_loss 1.8119468232114215 
Epoch [5/10] Batch 6900/7568 Train_loss 1.8126358462362908 
Epoch [5/10] Batch 7000/7568 Train_loss 1.8119154016689103 
Epoch [5/10] Batch 7100/7568 Train_loss 1.8121331443108064 
Epoch [5/10] Batch 7200/7568 Train_loss 1.813336434384681 
Epoch [5/10] Batch 7300/7568 Train_loss 1.8139944328516906 
Epoch [5/10] Batch 7400/7568 Train_loss 1.8134486558384677 
Epoch [5/10] Batch 7500/7568 Train_loss 1.8142839092768728 
Epoch: 5/10 	Training Loss: 1.813901 	Validation Loss: 1.876922 Duration seconds: 1262.9955523014069 
best_valid_loss_fold [1.8169636676525087] Best_Epoch [5]Epoch [6/10] Batch 0/7568 Train_loss 1.1272581666707993 
Epoch [6/10] Batch 100/7568 Train_loss 1.774436795327923 
Epoch [6/10] Batch 200/7568 Train_loss 1.723925600300974 
Epoch [6/10] Batch 300/7568 Train_loss 1.7322938904116716 
Epoch [6/10] Batch 400/7568 Train_loss 1.7416569506958535 
Epoch [6/10] Batch 500/7568 Train_loss 1.7593759622521505 
Epoch [6/10] Batch 600/7568 Train_loss 1.7625708323895635 
Epoch [6/10] Batch 700/7568 Train_loss 1.7610029656222135 
Epoch [6/10] Batch 800/7568 Train_loss 1.7574319443415465 
Epoch [6/10] Batch 900/7568 Train_loss 1.7634555135133132 
Epoch [6/10] Batch 1000/7568 Train_loss 1.7684914050372569 
Epoch [6/10] Batch 1100/7568 Train_loss 1.7828363948903443 
Epoch [6/10] Batch 1200/7568 Train_loss 1.7792812009946095 
Epoch [6/10] Batch 1300/7568 Train_loss 1.7844613675002314 
Epoch [6/10] Batch 1400/7568 Train_loss 1.786406764535926 
Epoch [6/10] Batch 1500/7568 Train_loss 1.7864693603729662 
Epoch [6/10] Batch 1600/7568 Train_loss 1.7839453733996775 
Epoch [6/10] Batch 1700/7568 Train_loss 1.7856229568608784 
Epoch [6/10] Batch 1800/7568 Train_loss 1.7831312690234264 
Epoch [6/10] Batch 1900/7568 Train_loss 1.7865483217266658 
Epoch [6/10] Batch 2000/7568 Train_loss 1.78741998127152 
Epoch [6/10] Batch 2100/7568 Train_loss 1.7914457695079042 
Epoch [6/10] Batch 2200/7568 Train_loss 1.790153425912974 
Epoch [6/10] Batch 2300/7568 Train_loss 1.7924620167411758 
Epoch [6/10] Batch 2400/7568 Train_loss 1.7989118169467184 
Epoch [6/10] Batch 2500/7568 Train_loss 1.7964111674265688 
Epoch [6/10] Batch 2600/7568 Train_loss 1.7962484240749623 
Epoch [6/10] Batch 2700/7568 Train_loss 1.7992073700234696 
Epoch [6/10] Batch 2800/7568 Train_loss 1.7989770920999728 
Epoch [6/10] Batch 2900/7568 Train_loss 1.7982504749654573 
Epoch [6/10] Batch 3000/7568 Train_loss 1.798960128808391 
Epoch [6/10] Batch 3100/7568 Train_loss 1.7957045656251391 
Epoch [6/10] Batch 3200/7568 Train_loss 1.7947740448648688 
Epoch [6/10] Batch 3300/7568 Train_loss 1.7949755972048724 
Epoch [6/10] Batch 3400/7568 Train_loss 1.797153302111001 
Epoch [6/10] Batch 3500/7568 Train_loss 1.7947897992728097 
Epoch [6/10] Batch 3600/7568 Train_loss 1.7931677571347078 
Epoch [6/10] Batch 3700/7568 Train_loss 1.7953684207950853 
Epoch [6/10] Batch 3800/7568 Train_loss 1.7969854201134554 
Epoch [6/10] Batch 3900/7568 Train_loss 1.7981539480817896 
Epoch [6/10] Batch 4000/7568 Train_loss 1.7963729806614948 
Epoch [6/10] Batch 4100/7568 Train_loss 1.7955037029094447 
Epoch [6/10] Batch 4200/7568 Train_loss 1.7957639986313516 
Epoch [6/10] Batch 4300/7568 Train_loss 1.797357580324185 
Epoch [6/10] Batch 4400/7568 Train_loss 1.7973631254057265 
Epoch [6/10] Batch 4500/7568 Train_loss 1.7980631132161344 
Epoch [6/10] Batch 4600/7568 Train_loss 1.7986480030054062 
Epoch [6/10] Batch 4700/7568 Train_loss 1.7982894704902814 
Epoch [6/10] Batch 4800/7568 Train_loss 1.7976033061252388 
Epoch [6/10] Batch 4900/7568 Train_loss 1.797375678636059 
Epoch [6/10] Batch 5000/7568 Train_loss 1.7975579498350989 
Epoch [6/10] Batch 5100/7568 Train_loss 1.7970482239832342 
Epoch [6/10] Batch 5200/7568 Train_loss 1.798727315988616 
Epoch [6/10] Batch 5300/7568 Train_loss 1.7991694451289184 
Epoch [6/10] Batch 5400/7568 Train_loss 1.8009143327810304 
Epoch [6/10] Batch 5500/7568 Train_loss 1.800113203221723 
Epoch [6/10] Batch 5600/7568 Train_loss 1.8014530563509863 
Epoch [6/10] Batch 5700/7568 Train_loss 1.8015954008992776 
Epoch [6/10] Batch 5800/7568 Train_loss 1.802153588060551 
Epoch [6/10] Batch 5900/7568 Train_loss 1.802752643894329 
Epoch [6/10] Batch 6000/7568 Train_loss 1.8045248827215552 
Epoch [6/10] Batch 6100/7568 Train_loss 1.8050422112539428 
Epoch [6/10] Batch 6200/7568 Train_loss 1.8047558683394387 
Epoch [6/10] Batch 6300/7568 Train_loss 1.8057000125410858 
Epoch [6/10] Batch 6400/7568 Train_loss 1.805564256207754 
Epoch [6/10] Batch 6500/7568 Train_loss 1.8060397249362117 
Epoch [6/10] Batch 6600/7568 Train_loss 1.80691381297027 
Epoch [6/10] Batch 6700/7568 Train_loss 1.806328372078909 
Epoch [6/10] Batch 6800/7568 Train_loss 1.8048241225639137 
Epoch [6/10] Batch 6900/7568 Train_loss 1.8061119102296785 
Epoch [6/10] Batch 7000/7568 Train_loss 1.8059499830820616 
Epoch [6/10] Batch 7100/7568 Train_loss 1.8037137689262759 
Epoch [6/10] Batch 7200/7568 Train_loss 1.8034937445251764 
Epoch [6/10] Batch 7300/7568 Train_loss 1.8032368667369638 
Epoch [6/10] Batch 7400/7568 Train_loss 1.8025596885302795 
Epoch [6/10] Batch 7500/7568 Train_loss 1.803367365528593 
Epoch: 6/10 	Training Loss: 1.803096 	Validation Loss: 1.817605 Duration seconds: 1221.3120806217194 
best_valid_loss_fold [1.8169636676525087] Best_Epoch [6]Epoch [7/10] Batch 0/7568 Train_loss 2.7342382073402405 
Epoch [7/10] Batch 100/7568 Train_loss 1.8120502142032775 
Epoch [7/10] Batch 200/7568 Train_loss 1.8155986196662657 
Epoch [7/10] Batch 300/7568 Train_loss 1.8070342475888737 
Epoch [7/10] Batch 400/7568 Train_loss 1.7902781275516733 
Epoch [7/10] Batch 500/7568 Train_loss 1.8152839819828193 
Epoch [7/10] Batch 600/7568 Train_loss 1.8291482157050671 
Epoch [7/10] Batch 700/7568 Train_loss 1.817538841187784 
Epoch [7/10] Batch 800/7568 Train_loss 1.8056529887327541 
Epoch [7/10] Batch 900/7568 Train_loss 1.8012985468456537 
Epoch [7/10] Batch 1000/7568 Train_loss 1.8007284098795124 
Epoch [7/10] Batch 1100/7568 Train_loss 1.801865362375298 
Epoch [7/10] Batch 1200/7568 Train_loss 1.8027318263120893 
Epoch [7/10] Batch 1300/7568 Train_loss 1.8044376457761473 
Epoch [7/10] Batch 1400/7568 Train_loss 1.7957704048138443 
Epoch [7/10] Batch 1500/7568 Train_loss 1.7974426053578578 
Epoch [7/10] Batch 1600/7568 Train_loss 1.801816197129654 
Epoch [7/10] Batch 1700/7568 Train_loss 1.80391681252999 
Epoch [7/10] Batch 1800/7568 Train_loss 1.8006856571148662 
Epoch [7/10] Batch 1900/7568 Train_loss 1.8009873904848275 
Epoch [7/10] Batch 2000/7568 Train_loss 1.7992782185058007 
Epoch [7/10] Batch 2100/7568 Train_loss 1.7980215238892199 
Epoch [7/10] Batch 2200/7568 Train_loss 1.798307157474071 
Epoch [7/10] Batch 2300/7568 Train_loss 1.7970955323287063 
Epoch [7/10] Batch 2400/7568 Train_loss 1.7975621402685664 
Epoch [7/10] Batch 2500/7568 Train_loss 1.8000884629973695 
Epoch [7/10] Batch 2600/7568 Train_loss 1.8008169855511404 
Epoch [7/10] Batch 2700/7568 Train_loss 1.8019468407913828 
Epoch [7/10] Batch 2800/7568 Train_loss 1.8027465152648976 
Epoch [7/10] Batch 2900/7568 Train_loss 1.7993467227163829 
Epoch [7/10] Batch 3000/7568 Train_loss 1.7984704922553263 
Epoch [7/10] Batch 3100/7568 Train_loss 1.7984285090516283 
Epoch [7/10] Batch 3200/7568 Train_loss 1.7983880192646569 
Epoch [7/10] Batch 3300/7568 Train_loss 1.7970680320842525 
Epoch [7/10] Batch 3400/7568 Train_loss 1.7964401023155168 
Epoch [7/10] Batch 3500/7568 Train_loss 1.7969766371771458 
Epoch [7/10] Batch 3600/7568 Train_loss 1.797043535567694 
Epoch [7/10] Batch 3700/7568 Train_loss 1.797401957219918 
Epoch [7/10] Batch 3800/7568 Train_loss 1.797800505143252 
Epoch [7/10] Batch 3900/7568 Train_loss 1.797748414928013 
Epoch [7/10] Batch 4000/7568 Train_loss 1.7993825107023318 
Epoch [7/10] Batch 4100/7568 Train_loss 1.7998965135422371 
Epoch [7/10] Batch 4200/7568 Train_loss 1.8011831103783509 
Epoch [7/10] Batch 4300/7568 Train_loss 1.8009839192382737 
Epoch [7/10] Batch 4400/7568 Train_loss 1.8001821571637273 
Epoch [7/10] Batch 4500/7568 Train_loss 1.799304279134793 
Epoch [7/10] Batch 4600/7568 Train_loss 1.7984374422783231 
Epoch [7/10] Batch 4700/7568 Train_loss 1.7978941702133948 
Epoch [7/10] Batch 4800/7568 Train_loss 1.7992268920323422 
Epoch [7/10] Batch 4900/7568 Train_loss 1.7989244836446607 
Epoch [7/10] Batch 5000/7568 Train_loss 1.7996726835612487 
Epoch [7/10] Batch 5100/7568 Train_loss 1.7981781805044785 
Epoch [7/10] Batch 5200/7568 Train_loss 1.7981713282729062 
Epoch [7/10] Batch 5300/7568 Train_loss 1.7980006885101854 
Epoch [7/10] Batch 5400/7568 Train_loss 1.7966323437290244 
Epoch [7/10] Batch 5500/7568 Train_loss 1.7977575006915252 
Epoch [7/10] Batch 5600/7568 Train_loss 1.7974228591160506 
Epoch [7/10] Batch 5700/7568 Train_loss 1.7985223400132255 
Epoch [7/10] Batch 5800/7568 Train_loss 1.7976621340416337 
Epoch [7/10] Batch 5900/7568 Train_loss 1.7978540407719399 
Epoch [7/10] Batch 6000/7568 Train_loss 1.795513667222103 
Epoch [7/10] Batch 6100/7568 Train_loss 1.7960916122751653 
Epoch [7/10] Batch 6200/7568 Train_loss 1.7957902103278007 
Epoch [7/10] Batch 6300/7568 Train_loss 1.7970146360950345 
Epoch [7/10] Batch 6400/7568 Train_loss 1.7966264903131006 
Epoch [7/10] Batch 6500/7568 Train_loss 1.797661333875076 
Epoch [7/10] Batch 6600/7568 Train_loss 1.7981732738273575 
Epoch [7/10] Batch 6700/7568 Train_loss 1.7977343565816222 
Epoch [7/10] Batch 6800/7568 Train_loss 1.7986052388777507 
Epoch [7/10] Batch 6900/7568 Train_loss 1.7972700203148253 
Epoch [7/10] Batch 7000/7568 Train_loss 1.7968495692028146 
Epoch [7/10] Batch 7100/7568 Train_loss 1.7956124857926332 
Epoch [7/10] Batch 7200/7568 Train_loss 1.7965705938240981 
Epoch [7/10] Batch 7300/7568 Train_loss 1.7961331260914983 
Epoch [7/10] Batch 7400/7568 Train_loss 1.7964308137050706 
Epoch [7/10] Batch 7500/7568 Train_loss 1.7958788077649062 
Epoch: 7/10 	Training Loss: 1.795162 	Validation Loss: 1.887304 Duration seconds: 1225.388060092926 
best_valid_loss_fold [1.8169636676525087] Best_Epoch [7]Epoch [8/10] Batch 0/7568 Train_loss 0.8675174415111542 
Epoch [8/10] Batch 100/7568 Train_loss 1.892645911562561 
Epoch [8/10] Batch 200/7568 Train_loss 1.8326794697129312 
Epoch [8/10] Batch 300/7568 Train_loss 1.809425711928808 
Epoch [8/10] Batch 400/7568 Train_loss 1.8219903148617829 
Epoch [8/10] Batch 500/7568 Train_loss 1.799679088913752 
Epoch [8/10] Batch 600/7568 Train_loss 1.8001309935989078 
Epoch [8/10] Batch 700/7568 Train_loss 1.800955987818401 
Epoch [8/10] Batch 800/7568 Train_loss 1.803987386744567 
Epoch [8/10] Batch 900/7568 Train_loss 1.8029937828652205 
Epoch [8/10] Batch 1000/7568 Train_loss 1.8070527900765825 
Epoch [8/10] Batch 1100/7568 Train_loss 1.8030221663503405 
Epoch [8/10] Batch 1200/7568 Train_loss 1.8093855567369532 
Epoch [8/10] Batch 1300/7568 Train_loss 1.8080969715489505 
Epoch [8/10] Batch 1400/7568 Train_loss 1.8035318366358912 
Epoch [8/10] Batch 1500/7568 Train_loss 1.80235017515436 
Epoch [8/10] Batch 1600/7568 Train_loss 1.8023650264885185 
Epoch [8/10] Batch 1700/7568 Train_loss 1.8103956778055776 
Epoch [8/10] Batch 1800/7568 Train_loss 1.8112761440838396 
Epoch [8/10] Batch 1900/7568 Train_loss 1.8093087944128148 
Epoch [8/10] Batch 2000/7568 Train_loss 1.8056088001250863 
Epoch [8/10] Batch 2100/7568 Train_loss 1.8039233814118194 
Epoch [8/10] Batch 2200/7568 Train_loss 1.8021442662283054 
Epoch [8/10] Batch 2300/7568 Train_loss 1.7978579146882234 
Epoch [8/10] Batch 2400/7568 Train_loss 1.7976726886693362 
Epoch [8/10] Batch 2500/7568 Train_loss 1.7974301897993281 
Epoch [8/10] Batch 2600/7568 Train_loss 1.7968073639100628 
Epoch [8/10] Batch 2700/7568 Train_loss 1.7953428780533833 
Epoch [8/10] Batch 2800/7568 Train_loss 1.793518856949825 
Epoch [8/10] Batch 2900/7568 Train_loss 1.7920689767381315 
Epoch [8/10] Batch 3000/7568 Train_loss 1.7915993583814098 
Epoch [8/10] Batch 3100/7568 Train_loss 1.7894872394262764 
Epoch [8/10] Batch 3200/7568 Train_loss 1.7892878333625106 
Epoch [8/10] Batch 3300/7568 Train_loss 1.789386803879589 
Epoch [8/10] Batch 3400/7568 Train_loss 1.789033937278091 
Epoch [8/10] Batch 3500/7568 Train_loss 1.7893424333716692 
Epoch [8/10] Batch 3600/7568 Train_loss 1.7905232442713088 
Epoch [8/10] Batch 3700/7568 Train_loss 1.790994855700329 
Epoch [8/10] Batch 3800/7568 Train_loss 1.7907553936985254 
Epoch [8/10] Batch 3900/7568 Train_loss 1.7908079060540845 
Epoch [8/10] Batch 4000/7568 Train_loss 1.7921124828592505 
Epoch [8/10] Batch 4100/7568 Train_loss 1.790445644496578 
Epoch [8/10] Batch 4200/7568 Train_loss 1.7917068516607229 
Epoch [8/10] Batch 4300/7568 Train_loss 1.7914850823711141 
Epoch [8/10] Batch 4400/7568 Train_loss 1.7913476489035685 
Epoch [8/10] Batch 4500/7568 Train_loss 1.7942931777922824 
Epoch [8/10] Batch 4600/7568 Train_loss 1.7956048452696756 
Epoch [8/10] Batch 4700/7568 Train_loss 1.7969738226245247 
Epoch [8/10] Batch 4800/7568 Train_loss 1.7969603037938453 
Epoch [8/10] Batch 4900/7568 Train_loss 1.79813032777145 
Epoch [8/10] Batch 5000/7568 Train_loss 1.797304211887949 
Epoch [8/10] Batch 5100/7568 Train_loss 1.796140055902634 
Epoch [8/10] Batch 5200/7568 Train_loss 1.7957298507331265 
Epoch [8/10] Batch 5300/7568 Train_loss 1.7950701806184775 
Epoch [8/10] Batch 5400/7568 Train_loss 1.793932130346804 
Epoch [8/10] Batch 5500/7568 Train_loss 1.7927450966242335 
Epoch [8/10] Batch 5600/7568 Train_loss 1.794232655129695 
Epoch [8/10] Batch 5700/7568 Train_loss 1.7954444142286872 
Epoch [8/10] Batch 5800/7568 Train_loss 1.7945458824250065 
Epoch [8/10] Batch 5900/7568 Train_loss 1.7958289179038727 
Epoch [8/10] Batch 6000/7568 Train_loss 1.7974615715092728 
Epoch [8/10] Batch 6100/7568 Train_loss 1.7977989388062239 
Epoch [8/10] Batch 6200/7568 Train_loss 1.796631136321287 
Epoch [8/10] Batch 6300/7568 Train_loss 1.795793996242621 
Epoch [8/10] Batch 6400/7568 Train_loss 1.796995337899449 
Epoch [8/10] Batch 6500/7568 Train_loss 1.7962730767621093 
Epoch [8/10] Batch 6600/7568 Train_loss 1.796621515423651 
Epoch [8/10] Batch 6700/7568 Train_loss 1.7962736106042523 
Epoch [8/10] Batch 6800/7568 Train_loss 1.7965251301565761 
Epoch [8/10] Batch 6900/7568 Train_loss 1.7972650016616036 
Epoch [8/10] Batch 7000/7568 Train_loss 1.7972799438337754 
Epoch [8/10] Batch 7100/7568 Train_loss 1.79709058219808 
Epoch [8/10] Batch 7200/7568 Train_loss 1.7979219717520292 
Epoch [8/10] Batch 7300/7568 Train_loss 1.7974113203598414 
Epoch [8/10] Batch 7400/7568 Train_loss 1.7978185644141003 
Epoch [8/10] Batch 7500/7568 Train_loss 1.7970728034042085 
Epoch: 8/10 	Training Loss: 1.796823 	Validation Loss: 1.825084 Duration seconds: 1294.074287891388 
best_valid_loss_fold [1.8169636676525087] Best_Epoch [8]Epoch [9/10] Batch 0/7568 Train_loss 1.8242225050926208 
Epoch [9/10] Batch 100/7568 Train_loss 1.8243954161016067 
Epoch [9/10] Batch 200/7568 Train_loss 1.7808313330459358 
Epoch [9/10] Batch 300/7568 Train_loss 1.8154060318323464 
Epoch [9/10] Batch 400/7568 Train_loss 1.798326175594865 
Epoch [9/10] Batch 500/7568 Train_loss 1.7922258038006857 
Epoch [9/10] Batch 600/7568 Train_loss 1.7982761703196064 
Epoch [9/10] Batch 700/7568 Train_loss 1.7995913712554923 
Epoch [9/10] Batch 800/7568 Train_loss 1.8100804493259104 
Epoch [9/10] Batch 900/7568 Train_loss 1.8066381118968378 
Epoch [9/10] Batch 1000/7568 Train_loss 1.8003072805486835 
Epoch [9/10] Batch 1100/7568 Train_loss 1.8065549099613167 
Epoch [9/10] Batch 1200/7568 Train_loss 1.8058255334678637 
Epoch [9/10] Batch 1300/7568 Train_loss 1.804455817261171 
Epoch [9/10] Batch 1400/7568 Train_loss 1.7989598313010138 
Epoch [9/10] Batch 1500/7568 Train_loss 1.7981357691309439 
Epoch [9/10] Batch 1600/7568 Train_loss 1.7933122884903305 
Epoch [9/10] Batch 1700/7568 Train_loss 1.7931845084682483 
Epoch [9/10] Batch 1800/7568 Train_loss 1.797403094339609 
Epoch [9/10] Batch 1900/7568 Train_loss 1.7975568226188312 
Epoch [9/10] Batch 2000/7568 Train_loss 1.7979197804329694 
Epoch [9/10] Batch 2100/7568 Train_loss 1.7938754916091808 
Epoch [9/10] Batch 2200/7568 Train_loss 1.7984711890976735 
Epoch [9/10] Batch 2300/7568 Train_loss 1.7968654900025098 
Epoch [9/10] Batch 2400/7568 Train_loss 1.7947279523184576 
Epoch [9/10] Batch 2500/7568 Train_loss 1.7929724470132449 
Epoch [9/10] Batch 2600/7568 Train_loss 1.791593028559725 
Epoch [9/10] Batch 2700/7568 Train_loss 1.7892152481732744 
Epoch [9/10] Batch 2800/7568 Train_loss 1.7911232836339697 
Epoch [9/10] Batch 2900/7568 Train_loss 1.7901555319586033 
Epoch [9/10] Batch 3000/7568 Train_loss 1.7898561975179177 
Epoch [9/10] Batch 3100/7568 Train_loss 1.7883373901334474 
Epoch [9/10] Batch 3200/7568 Train_loss 1.7871622996441427 
Epoch [9/10] Batch 3300/7568 Train_loss 1.7885681269399798 
Epoch [9/10] Batch 3400/7568 Train_loss 1.787226216391636 
Epoch [9/10] Batch 3500/7568 Train_loss 1.786178951911145 
Epoch [9/10] Batch 3600/7568 Train_loss 1.786628745690762 
Epoch [9/10] Batch 3700/7568 Train_loss 1.7866934912510255 
Epoch [9/10] Batch 3800/7568 Train_loss 1.787242034038049 
Epoch [9/10] Batch 3900/7568 Train_loss 1.789534586321556 
Epoch [9/10] Batch 4000/7568 Train_loss 1.7903133532671565 
Epoch [9/10] Batch 4100/7568 Train_loss 1.7929737425303203 
Epoch [9/10] Batch 4200/7568 Train_loss 1.7923105104700692 
Epoch [9/10] Batch 4300/7568 Train_loss 1.7929800115378478 
Epoch [9/10] Batch 4400/7568 Train_loss 1.7901380750092695 
Epoch [9/10] Batch 4500/7568 Train_loss 1.791730095201785 
Epoch [9/10] Batch 4600/7568 Train_loss 1.7904318659409468 
Epoch [9/10] Batch 4700/7568 Train_loss 1.790617179435394 
Epoch [9/10] Batch 4800/7568 Train_loss 1.7898620099800746 
Epoch [9/10] Batch 4900/7568 Train_loss 1.790005877709795 
Epoch [9/10] Batch 5000/7568 Train_loss 1.7923870975987073 
Epoch [9/10] Batch 5100/7568 Train_loss 1.7918328803789965 
Epoch [9/10] Batch 5200/7568 Train_loss 1.7907194646461793 
Epoch [9/10] Batch 5300/7568 Train_loss 1.7917937420869385 
Epoch [9/10] Batch 5400/7568 Train_loss 1.7926327267499946 
Epoch [9/10] Batch 5500/7568 Train_loss 1.7929228653034563 
Epoch [9/10] Batch 5600/7568 Train_loss 1.7915962962575032 
Epoch [9/10] Batch 5700/7568 Train_loss 1.7920743755263062 
Epoch [9/10] Batch 5800/7568 Train_loss 1.7919890578130446 
Epoch [9/10] Batch 5900/7568 Train_loss 1.7926939399911375 
Epoch [9/10] Batch 6000/7568 Train_loss 1.7913304772330432 
Epoch [9/10] Batch 6100/7568 Train_loss 1.7914625595711016 
Epoch [9/10] Batch 6200/7568 Train_loss 1.7903482439253258 
Epoch [9/10] Batch 6300/7568 Train_loss 1.7889776728234958 
Epoch [9/10] Batch 6400/7568 Train_loss 1.7879791554413331 
Epoch [9/10] Batch 6500/7568 Train_loss 1.7873523916909921 
Epoch [9/10] Batch 6600/7568 Train_loss 1.787331118398662 
Epoch [9/10] Batch 6700/7568 Train_loss 1.7886569606574716 
Epoch [9/10] Batch 6800/7568 Train_loss 1.7875463400710385 
Epoch [9/10] Batch 6900/7568 Train_loss 1.7862287276016768 
Epoch [9/10] Batch 7000/7568 Train_loss 1.786465205804499 
Epoch [9/10] Batch 7100/7568 Train_loss 1.7871710116461017 
Epoch [9/10] Batch 7200/7568 Train_loss 1.786275453914157 
Epoch [9/10] Batch 7300/7568 Train_loss 1.7868190682549916 
Epoch [9/10] Batch 7400/7568 Train_loss 1.788067073153412 
Epoch [9/10] Batch 7500/7568 Train_loss 1.7877754810966582 
Epoch: 9/10 	Training Loss: 1.787671 	Validation Loss: 1.835864 Duration seconds: 1182.6467413902283 
best_valid_loss_fold [1.8169636676525087] Best_Epoch [9]Fold: 3/5 
Epoch [0/10] Batch 0/7568 Train_loss 1.5008091926574707 
Epoch [0/10] Batch 100/7568 Train_loss 1.8929339174439412 
Epoch [0/10] Batch 200/7568 Train_loss 1.9276220508994748 
Epoch [0/10] Batch 300/7568 Train_loss 1.907311924841515 
Epoch [0/10] Batch 400/7568 Train_loss 1.8643911496882426 
Epoch [0/10] Batch 500/7568 Train_loss 1.8742640314999217 
Epoch [0/10] Batch 600/7568 Train_loss 1.856917019270224 
Epoch [0/10] Batch 700/7568 Train_loss 1.8532797281820321 
Epoch [0/10] Batch 800/7568 Train_loss 1.8547988072800428 
Epoch [0/10] Batch 900/7568 Train_loss 1.8527706402388053 
Epoch [0/10] Batch 1000/7568 Train_loss 1.8543072483994505 
Epoch [0/10] Batch 1100/7568 Train_loss 1.8485799988395077 
Epoch [0/10] Batch 1200/7568 Train_loss 1.847016605812991 
Epoch [0/10] Batch 1300/7568 Train_loss 1.8476782598569246 
Epoch [0/10] Batch 1400/7568 Train_loss 1.8430445948715384 
Epoch [0/10] Batch 1500/7568 Train_loss 1.8418156787217022 
Epoch [0/10] Batch 1600/7568 Train_loss 1.8451403055589024 
Epoch [0/10] Batch 1700/7568 Train_loss 1.8400813810426582 
Epoch [0/10] Batch 1800/7568 Train_loss 1.8431322581482754 
Epoch [0/10] Batch 1900/7568 Train_loss 1.847783814192696 
Epoch [0/10] Batch 2000/7568 Train_loss 1.843789677819361 
Epoch [0/10] Batch 2100/7568 Train_loss 1.840865054052112 
Epoch [0/10] Batch 2200/7568 Train_loss 1.836838714764883 
Epoch [0/10] Batch 2300/7568 Train_loss 1.835725830996596 
Epoch [0/10] Batch 2400/7568 Train_loss 1.8324461853172769 
Epoch [0/10] Batch 2500/7568 Train_loss 1.8338415416174296 
Epoch [0/10] Batch 2600/7568 Train_loss 1.8350140424284143 
Epoch [0/10] Batch 2700/7568 Train_loss 1.8317391444652753 
Epoch [0/10] Batch 2800/7568 Train_loss 1.8309333419207163 
Epoch [0/10] Batch 2900/7568 Train_loss 1.8318066594596363 
Epoch [0/10] Batch 3000/7568 Train_loss 1.8271036511513639 
Epoch [0/10] Batch 3100/7568 Train_loss 1.8266019846082349 
Epoch [0/10] Batch 3200/7568 Train_loss 1.8256752260590263 
Epoch [0/10] Batch 3300/7568 Train_loss 1.8266746324256526 
Epoch [0/10] Batch 3400/7568 Train_loss 1.8266238074041017 
Epoch [0/10] Batch 3500/7568 Train_loss 1.8274325361675925 
Epoch [0/10] Batch 3600/7568 Train_loss 1.827414551658271 
Epoch [0/10] Batch 3700/7568 Train_loss 1.8282682843298985 
Epoch [0/10] Batch 3800/7568 Train_loss 1.827074595744423 
Epoch [0/10] Batch 3900/7568 Train_loss 1.8290157871589665 
Epoch [0/10] Batch 4000/7568 Train_loss 1.8268547257675256 
Epoch [0/10] Batch 4100/7568 Train_loss 1.8269277389722898 
Epoch [0/10] Batch 4200/7568 Train_loss 1.8267515313675704 
Epoch [0/10] Batch 4300/7568 Train_loss 1.8268485009666793 
Epoch [0/10] Batch 4400/7568 Train_loss 1.826462267033148 
Epoch [0/10] Batch 4500/7568 Train_loss 1.8270928693003745 
Epoch [0/10] Batch 4600/7568 Train_loss 1.8268720185033163 
Epoch [0/10] Batch 4700/7568 Train_loss 1.8265971116415747 
Epoch [0/10] Batch 4800/7568 Train_loss 1.8270097977806319 
Epoch [0/10] Batch 4900/7568 Train_loss 1.8290364374462693 
Epoch [0/10] Batch 5000/7568 Train_loss 1.8311298710912902 
Epoch [0/10] Batch 5100/7568 Train_loss 1.8318464361106361 
Epoch [0/10] Batch 5200/7568 Train_loss 1.8333377699652238 
Epoch [0/10] Batch 5300/7568 Train_loss 1.8331829122910857 
Epoch [0/10] Batch 5400/7568 Train_loss 1.8326457645802714 
Epoch [0/10] Batch 5500/7568 Train_loss 1.831082792827713 
Epoch [0/10] Batch 5600/7568 Train_loss 1.831073534030283 
Epoch [0/10] Batch 5700/7568 Train_loss 1.8313281456917823 
Epoch [0/10] Batch 5800/7568 Train_loss 1.832933944920192 
Epoch [0/10] Batch 5900/7568 Train_loss 1.832731470675251 
Epoch [0/10] Batch 6000/7568 Train_loss 1.8313451151915578 
Epoch [0/10] Batch 6100/7568 Train_loss 1.8305909739026591 
Epoch [0/10] Batch 6200/7568 Train_loss 1.8296140755324417 
Epoch [0/10] Batch 6300/7568 Train_loss 1.8290190905961625 
Epoch [0/10] Batch 6400/7568 Train_loss 1.8288070298096963 
Epoch [0/10] Batch 6500/7568 Train_loss 1.8299062712465575 
Epoch [0/10] Batch 6600/7568 Train_loss 1.8302932769678666 
Epoch [0/10] Batch 6700/7568 Train_loss 1.8300404737861526 
Epoch [0/10] Batch 6800/7568 Train_loss 1.8303012320301564 
Epoch [0/10] Batch 6900/7568 Train_loss 1.8295547981967686 
Epoch [0/10] Batch 7000/7568 Train_loss 1.8297471612300247 
Epoch [0/10] Batch 7100/7568 Train_loss 1.8294888908150262 
Epoch [0/10] Batch 7200/7568 Train_loss 1.829357516745315 
Epoch [0/10] Batch 7300/7568 Train_loss 1.8298134049243788 
Epoch [0/10] Batch 7400/7568 Train_loss 1.8295047230240364 
Epoch [0/10] Batch 7500/7568 Train_loss 1.830672412850494 
Epoch: 0/10 	Training Loss: 1.830654 	Validation Loss: 1.797054 Duration seconds: 1074.6042838096619 
Validation loss decreased (inf --> 1.797054).  Saving model ... 
best_valid_loss_fold [1.7970537815684056] Best_Epoch [0]Epoch [1/10] Batch 0/7568 Train_loss 1.9000744819641113 
Epoch [1/10] Batch 100/7568 Train_loss 1.7966537959504836 
Epoch [1/10] Batch 200/7568 Train_loss 1.8498423350835913 
Epoch [1/10] Batch 300/7568 Train_loss 1.8198952212484176 
Epoch [1/10] Batch 400/7568 Train_loss 1.8362133658289017 
Epoch [1/10] Batch 500/7568 Train_loss 1.819518804698885 
Epoch [1/10] Batch 600/7568 Train_loss 1.8023953702108635 
Epoch [1/10] Batch 700/7568 Train_loss 1.8003196994137662 
Epoch [1/10] Batch 800/7568 Train_loss 1.800409522600537 
Epoch [1/10] Batch 900/7568 Train_loss 1.7984252775317424 
Epoch [1/10] Batch 1000/7568 Train_loss 1.7993692679332567 
Epoch [1/10] Batch 1100/7568 Train_loss 1.8013163665470266 
Epoch [1/10] Batch 1200/7568 Train_loss 1.79654621586613 
Epoch [1/10] Batch 1300/7568 Train_loss 1.7981524764346124 
Epoch [1/10] Batch 1400/7568 Train_loss 1.802151010376398 
Epoch [1/10] Batch 1500/7568 Train_loss 1.804817788734744 
Epoch [1/10] Batch 1600/7568 Train_loss 1.805131393092115 
Epoch [1/10] Batch 1700/7568 Train_loss 1.802253571329644 
Epoch [1/10] Batch 1800/7568 Train_loss 1.80397207007383 
Epoch [1/10] Batch 1900/7568 Train_loss 1.8069285358929872 
Epoch [1/10] Batch 2000/7568 Train_loss 1.8082177677731226 
Epoch [1/10] Batch 2100/7568 Train_loss 1.8058590776420445 
Epoch [1/10] Batch 2200/7568 Train_loss 1.8073941693082283 
Epoch [1/10] Batch 2300/7568 Train_loss 1.808685685229892 
Epoch [1/10] Batch 2400/7568 Train_loss 1.8057630820101322 
Epoch [1/10] Batch 2500/7568 Train_loss 1.807196480957497 
Epoch [1/10] Batch 2600/7568 Train_loss 1.8066667876092117 
Epoch [1/10] Batch 2700/7568 Train_loss 1.8047389128927787 
Epoch [1/10] Batch 2800/7568 Train_loss 1.8048221318557278 
Epoch [1/10] Batch 2900/7568 Train_loss 1.8060735818184848 
Epoch [1/10] Batch 3000/7568 Train_loss 1.805920220976053 
Epoch [1/10] Batch 3100/7568 Train_loss 1.80770260042498 
Epoch [1/10] Batch 3200/7568 Train_loss 1.8069162746843082 
Epoch [1/10] Batch 3300/7568 Train_loss 1.8068978018772057 
Epoch [1/10] Batch 3400/7568 Train_loss 1.8095163341915037 
Epoch [1/10] Batch 3500/7568 Train_loss 1.8077789209048873 
Epoch [1/10] Batch 3600/7568 Train_loss 1.8087031847961046 
Epoch [1/10] Batch 3700/7568 Train_loss 1.8077909562035175 
Epoch [1/10] Batch 3800/7568 Train_loss 1.8058006536196922 
Epoch [1/10] Batch 3900/7568 Train_loss 1.8052946445927256 
Epoch [1/10] Batch 4000/7568 Train_loss 1.8055026556098202 
Epoch [1/10] Batch 4100/7568 Train_loss 1.8059869354213338 
Epoch [1/10] Batch 4200/7568 Train_loss 1.807691619630173 
Epoch [1/10] Batch 4300/7568 Train_loss 1.80880838571607 
Epoch [1/10] Batch 4400/7568 Train_loss 1.810653528820328 
Epoch [1/10] Batch 4500/7568 Train_loss 1.8107706911893346 
Epoch [1/10] Batch 4600/7568 Train_loss 1.8109437274663402 
Epoch [1/10] Batch 4700/7568 Train_loss 1.8116839323093779 
Epoch [1/10] Batch 4800/7568 Train_loss 1.8122399527283912 
Epoch [1/10] Batch 4900/7568 Train_loss 1.8127400963628957 
Epoch [1/10] Batch 5000/7568 Train_loss 1.811669617101708 
Epoch [1/10] Batch 5100/7568 Train_loss 1.8113870481589662 
Epoch [1/10] Batch 5200/7568 Train_loss 1.8119723984133256 
Epoch [1/10] Batch 5300/7568 Train_loss 1.8124910599188948 
Epoch [1/10] Batch 5400/7568 Train_loss 1.8144543526062722 
Epoch [1/10] Batch 5500/7568 Train_loss 1.8145952126564189 
Epoch [1/10] Batch 5600/7568 Train_loss 1.8140430991760514 
Epoch [1/10] Batch 5700/7568 Train_loss 1.814940242553213 
Epoch [1/10] Batch 5800/7568 Train_loss 1.8142800734353834 
Epoch [1/10] Batch 5900/7568 Train_loss 1.8143949173346674 
Epoch [1/10] Batch 6000/7568 Train_loss 1.8155796178662367 
Epoch [1/10] Batch 6100/7568 Train_loss 1.8154767601631876 
Epoch [1/10] Batch 6200/7568 Train_loss 1.8154121653612842 
Epoch [1/10] Batch 6300/7568 Train_loss 1.8150115624209695 
Epoch [1/10] Batch 6400/7568 Train_loss 1.8147634109111153 
Epoch [1/10] Batch 6500/7568 Train_loss 1.8160564334708678 
Epoch [1/10] Batch 6600/7568 Train_loss 1.8170293325290519 
Epoch [1/10] Batch 6700/7568 Train_loss 1.8167093390738709 
Epoch [1/10] Batch 6800/7568 Train_loss 1.817234295181116 
Epoch [1/10] Batch 6900/7568 Train_loss 1.8168480295151874 
Epoch [1/10] Batch 7000/7568 Train_loss 1.8172545382004603 
Epoch [1/10] Batch 7100/7568 Train_loss 1.8191167566368671 
Epoch [1/10] Batch 7200/7568 Train_loss 1.816920521240353 
Epoch [1/10] Batch 7300/7568 Train_loss 1.816163281801057 
Epoch [1/10] Batch 7400/7568 Train_loss 1.8151390561681715 
Epoch [1/10] Batch 7500/7568 Train_loss 1.8143726727685487 
Epoch: 1/10 	Training Loss: 1.814895 	Validation Loss: 1.869082 Duration seconds: 1308.6715881824493 
best_valid_loss_fold [1.7970537815684056] Best_Epoch [1]Epoch [2/10] Batch 0/7568 Train_loss 1.9943773746490479 
Epoch [2/10] Batch 100/7568 Train_loss 1.8222272669029709 
Epoch [2/10] Batch 200/7568 Train_loss 1.8150485885232241 
Epoch [2/10] Batch 300/7568 Train_loss 1.8499736851731012 
Epoch [2/10] Batch 400/7568 Train_loss 1.8187809957567593 
Epoch [2/10] Batch 500/7568 Train_loss 1.807787455172239 
Epoch [2/10] Batch 600/7568 Train_loss 1.8068986563957272 
Epoch [2/10] Batch 700/7568 Train_loss 1.800466013883643 
Epoch [2/10] Batch 800/7568 Train_loss 1.7977936446350462 
Epoch [2/10] Batch 900/7568 Train_loss 1.7980902839232893 
Epoch [2/10] Batch 1000/7568 Train_loss 1.7934513996754373 
Epoch [2/10] Batch 1100/7568 Train_loss 1.7919731226417284 
Epoch [2/10] Batch 1200/7568 Train_loss 1.7927384578454604 
Epoch [2/10] Batch 1300/7568 Train_loss 1.797629853225185 
Epoch [2/10] Batch 1400/7568 Train_loss 1.7937696203163995 
Epoch [2/10] Batch 1500/7568 Train_loss 1.7937073759550972 
Epoch [2/10] Batch 1600/7568 Train_loss 1.795741728507564 
Epoch [2/10] Batch 1700/7568 Train_loss 1.79549689332942 
Epoch [2/10] Batch 1800/7568 Train_loss 1.799543423310278 
Epoch [2/10] Batch 1900/7568 Train_loss 1.8022021561320176 
Epoch [2/10] Batch 2000/7568 Train_loss 1.8014588735919486 
Epoch [2/10] Batch 2100/7568 Train_loss 1.802824729205114 
Epoch [2/10] Batch 2200/7568 Train_loss 1.8058025976918004 
Epoch [2/10] Batch 2300/7568 Train_loss 1.804621691650823 
Epoch [2/10] Batch 2400/7568 Train_loss 1.8005816171916262 
Epoch [2/10] Batch 2500/7568 Train_loss 1.8026618174955351 
Epoch [2/10] Batch 2600/7568 Train_loss 1.806947781028701 
Epoch [2/10] Batch 2700/7568 Train_loss 1.807114187916289 
Epoch [2/10] Batch 2800/7568 Train_loss 1.8072678423122566 
Epoch [2/10] Batch 2900/7568 Train_loss 1.8073674820052759 
Epoch [2/10] Batch 3000/7568 Train_loss 1.8084783765330033 
Epoch [2/10] Batch 3100/7568 Train_loss 1.8114246590791576 
Epoch [2/10] Batch 3200/7568 Train_loss 1.8116574954754745 
Epoch [2/10] Batch 3300/7568 Train_loss 1.811513888523529 
Epoch [2/10] Batch 3400/7568 Train_loss 1.8113523367677355 
Epoch [2/10] Batch 3500/7568 Train_loss 1.8081671533474613 
Epoch [2/10] Batch 3600/7568 Train_loss 1.8090849738258747 
Epoch [2/10] Batch 3700/7568 Train_loss 1.8106371347516397 
Epoch [2/10] Batch 3800/7568 Train_loss 1.8105375726962836 
Epoch [2/10] Batch 3900/7568 Train_loss 1.8105205558517352 
Epoch [2/10] Batch 4000/7568 Train_loss 1.8100151486305618 
Epoch [2/10] Batch 4100/7568 Train_loss 1.811387745822037 
Epoch [2/10] Batch 4200/7568 Train_loss 1.8123435419184968 
Epoch [2/10] Batch 4300/7568 Train_loss 1.8144921086758021 
Epoch [2/10] Batch 4400/7568 Train_loss 1.81336972423167 
Epoch [2/10] Batch 4500/7568 Train_loss 1.8147867895336687 
Epoch [2/10] Batch 4600/7568 Train_loss 1.8152086685236135 
Epoch [2/10] Batch 4700/7568 Train_loss 1.8157610692554549 
Epoch [2/10] Batch 4800/7568 Train_loss 1.8158909216541792 
Epoch [2/10] Batch 4900/7568 Train_loss 1.8164476669961471 
Epoch [2/10] Batch 5000/7568 Train_loss 1.815530616209772 
Epoch [2/10] Batch 5100/7568 Train_loss 1.8143379282870145 
Epoch [2/10] Batch 5200/7568 Train_loss 1.813233216124655 
Epoch [2/10] Batch 5300/7568 Train_loss 1.8128425131592856 
Epoch [2/10] Batch 5400/7568 Train_loss 1.8129756281845901 
Epoch [2/10] Batch 5500/7568 Train_loss 1.813774571409173 
Epoch [2/10] Batch 5600/7568 Train_loss 1.813460629579053 
Epoch [2/10] Batch 5700/7568 Train_loss 1.8134954751355985 
Epoch [2/10] Batch 5800/7568 Train_loss 1.8127326499250116 
Epoch [2/10] Batch 5900/7568 Train_loss 1.81199255736677 
Epoch [2/10] Batch 6000/7568 Train_loss 1.8105785767325082 
Epoch [2/10] Batch 6100/7568 Train_loss 1.8112982275876388 
Epoch [2/10] Batch 6200/7568 Train_loss 1.8128560058360446 
Epoch [2/10] Batch 6300/7568 Train_loss 1.8119065534726075 
Epoch [2/10] Batch 6400/7568 Train_loss 1.8111284477690053 
Epoch [2/10] Batch 6500/7568 Train_loss 1.8120615042767843 
Epoch [2/10] Batch 6600/7568 Train_loss 1.8108227473825167 
Epoch [2/10] Batch 6700/7568 Train_loss 1.8119093837123934 
Epoch [2/10] Batch 6800/7568 Train_loss 1.8117106125830131 
Epoch [2/10] Batch 6900/7568 Train_loss 1.8103606052725856 
Epoch [2/10] Batch 7000/7568 Train_loss 1.8095785305766 
Epoch [2/10] Batch 7100/7568 Train_loss 1.8088621971112309 
Epoch [2/10] Batch 7200/7568 Train_loss 1.8091465006655512 
Epoch [2/10] Batch 7300/7568 Train_loss 1.8092910703457867 
Epoch [2/10] Batch 7400/7568 Train_loss 1.8086224987070039 
Epoch [2/10] Batch 7500/7568 Train_loss 1.8083490424902102 
Epoch: 2/10 	Training Loss: 1.808614 	Validation Loss: 1.979340 Duration seconds: 1225.1087222099304 
best_valid_loss_fold [1.7970537815684056] Best_Epoch [2]Epoch [3/10] Batch 0/7568 Train_loss 1.023944616317749 
Epoch [3/10] Batch 100/7568 Train_loss 1.7674694077508284 
Epoch [3/10] Batch 200/7568 Train_loss 1.7882179654207988 
Epoch [3/10] Batch 300/7568 Train_loss 1.8210299973967068 
Epoch [3/10] Batch 400/7568 Train_loss 1.8139610971855997 
Epoch [3/10] Batch 500/7568 Train_loss 1.804394855307129 
Epoch [3/10] Batch 600/7568 Train_loss 1.8003611893279976 
Epoch [3/10] Batch 700/7568 Train_loss 1.8058641007789191 
Epoch [3/10] Batch 800/7568 Train_loss 1.8068151680587532 
Epoch [3/10] Batch 900/7568 Train_loss 1.8032477400394842 
Epoch [3/10] Batch 1000/7568 Train_loss 1.7953879271190125 
Epoch [3/10] Batch 1100/7568 Train_loss 1.8026385762535042 
Epoch [3/10] Batch 1200/7568 Train_loss 1.8055401210495574 
Epoch [3/10] Batch 1300/7568 Train_loss 1.8004296505558866 
Epoch [3/10] Batch 1400/7568 Train_loss 1.7962846769117016 
Epoch [3/10] Batch 1500/7568 Train_loss 1.7955114604839477 
Epoch [3/10] Batch 1600/7568 Train_loss 1.8028947178839818 
Epoch [3/10] Batch 1700/7568 Train_loss 1.8050631396976167 
Epoch [3/10] Batch 1800/7568 Train_loss 1.8079608584702975 
Epoch [3/10] Batch 1900/7568 Train_loss 1.8102143786216585 
Epoch [3/10] Batch 2000/7568 Train_loss 1.8117749286459006 
Epoch [3/10] Batch 2100/7568 Train_loss 1.8143570698843803 
Epoch [3/10] Batch 2200/7568 Train_loss 1.8152791827490578 
Epoch [3/10] Batch 2300/7568 Train_loss 1.813076039851489 
Epoch [3/10] Batch 2400/7568 Train_loss 1.8103693808424677 
Epoch [3/10] Batch 2500/7568 Train_loss 1.8065422294480427 
Epoch [3/10] Batch 2600/7568 Train_loss 1.8065402589620925 
Epoch [3/10] Batch 2700/7568 Train_loss 1.801950049244201 
Epoch [3/10] Batch 2800/7568 Train_loss 1.8039896517263188 
Epoch [3/10] Batch 2900/7568 Train_loss 1.8047192808200845 
Epoch [3/10] Batch 3000/7568 Train_loss 1.8046714040869396 
Epoch [3/10] Batch 3100/7568 Train_loss 1.8022846092609044 
Epoch [3/10] Batch 3200/7568 Train_loss 1.8035026031387147 
Epoch [3/10] Batch 3300/7568 Train_loss 1.8047018986228054 
Epoch [3/10] Batch 3400/7568 Train_loss 1.807019835553233 
Epoch [3/10] Batch 3500/7568 Train_loss 1.8074244010413554 
Epoch [3/10] Batch 3600/7568 Train_loss 1.8049384929059744 
Epoch [3/10] Batch 3700/7568 Train_loss 1.8056120649906178 
Epoch [3/10] Batch 3800/7568 Train_loss 1.8037679483704867 
Epoch [3/10] Batch 3900/7568 Train_loss 1.8027481553616262 
Epoch [3/10] Batch 4000/7568 Train_loss 1.802309785290558 
Epoch [3/10] Batch 4100/7568 Train_loss 1.8022974099263538 
Epoch [3/10] Batch 4200/7568 Train_loss 1.8006786812780529 
Epoch [3/10] Batch 4300/7568 Train_loss 1.800496128150209 
Epoch [3/10] Batch 4400/7568 Train_loss 1.8000575472782536 
Epoch [3/10] Batch 4500/7568 Train_loss 1.8003411690703075 
Epoch [3/10] Batch 4600/7568 Train_loss 1.8021348442002187 
Epoch [3/10] Batch 4700/7568 Train_loss 1.8019229630426354 
Epoch [3/10] Batch 4800/7568 Train_loss 1.8019349332390615 
Epoch [3/10] Batch 4900/7568 Train_loss 1.80324484367464 
Epoch [3/10] Batch 5000/7568 Train_loss 1.8034446187774746 
Epoch [3/10] Batch 5100/7568 Train_loss 1.8040550703033469 
Epoch [3/10] Batch 5200/7568 Train_loss 1.8022440095414016 
Epoch [3/10] Batch 5300/7568 Train_loss 1.801686270172653 
Epoch [3/10] Batch 5400/7568 Train_loss 1.8004927781997313 
Epoch [3/10] Batch 5500/7568 Train_loss 1.8011298658652342 
Epoch [3/10] Batch 5600/7568 Train_loss 1.8001271293187138 
Epoch [3/10] Batch 5700/7568 Train_loss 1.8004632693184632 
Epoch [3/10] Batch 5800/7568 Train_loss 1.800276544417077 
Epoch [3/10] Batch 5900/7568 Train_loss 1.7984814703189262 
Epoch [3/10] Batch 6000/7568 Train_loss 1.7987120001974295 
Epoch [3/10] Batch 6100/7568 Train_loss 1.7994129001041588 
Epoch [3/10] Batch 6200/7568 Train_loss 1.7995413632604158 
Epoch [3/10] Batch 6300/7568 Train_loss 1.7982023495603148 
Epoch [3/10] Batch 6400/7568 Train_loss 1.7984585004464815 
Epoch [3/10] Batch 6500/7568 Train_loss 1.7981247806288685 
Epoch [3/10] Batch 6600/7568 Train_loss 1.7972111144836627 
Epoch [3/10] Batch 6700/7568 Train_loss 1.7970477218866578 
Epoch [3/10] Batch 6800/7568 Train_loss 1.7977764054808594 
Epoch [3/10] Batch 6900/7568 Train_loss 1.798380995351471 
Epoch [3/10] Batch 7000/7568 Train_loss 1.7980965021548059 
Epoch [3/10] Batch 7100/7568 Train_loss 1.7984519488799993 
Epoch [3/10] Batch 7200/7568 Train_loss 1.7972948695388773 
Epoch [3/10] Batch 7300/7568 Train_loss 1.799141641743476 
Epoch [3/10] Batch 7400/7568 Train_loss 1.7983829169164813 
Epoch [3/10] Batch 7500/7568 Train_loss 1.7981085582918015 
Epoch: 3/10 	Training Loss: 1.797559 	Validation Loss: 1.944408 Duration seconds: 1208.9237949848175 
best_valid_loss_fold [1.7970537815684056] Best_Epoch [3]Epoch [4/10] Batch 0/7568 Train_loss 1.5509016811847687 
Epoch [4/10] Batch 100/7568 Train_loss 1.8155336291483133 
Epoch [4/10] Batch 200/7568 Train_loss 1.8270483364661534 
Epoch [4/10] Batch 300/7568 Train_loss 1.8036795947143802 
Epoch [4/10] Batch 400/7568 Train_loss 1.8150295738939037 
Epoch [4/10] Batch 500/7568 Train_loss 1.810903344266191 
Epoch [4/10] Batch 600/7568 Train_loss 1.8130782582547622 
Epoch [4/10] Batch 700/7568 Train_loss 1.8066498235441988 
Epoch [4/10] Batch 800/7568 Train_loss 1.8018321251117633 
Epoch [4/10] Batch 900/7568 Train_loss 1.8003126999547294 
Epoch [4/10] Batch 1000/7568 Train_loss 1.7954399942845611 
Epoch [4/10] Batch 1100/7568 Train_loss 1.8008283367355122 
Epoch [4/10] Batch 1200/7568 Train_loss 1.7957190347899008 
Epoch [4/10] Batch 1300/7568 Train_loss 1.800826051652569 
Epoch [4/10] Batch 1400/7568 Train_loss 1.7990584791685156 
Epoch [4/10] Batch 1500/7568 Train_loss 1.8003141230281237 
Epoch [4/10] Batch 1600/7568 Train_loss 1.7999330369626485 
Epoch [4/10] Batch 1700/7568 Train_loss 1.7974542746695963 
Epoch [4/10] Batch 1800/7568 Train_loss 1.8012852231655962 
Epoch [4/10] Batch 1900/7568 Train_loss 1.806274822147378 
Epoch [4/10] Batch 2000/7568 Train_loss 1.808963108776153 
Epoch [4/10] Batch 2100/7568 Train_loss 1.8118039886336392 
Epoch [4/10] Batch 2200/7568 Train_loss 1.8122781438753854 
Epoch [4/10] Batch 2300/7568 Train_loss 1.8102914988282244 
Epoch [4/10] Batch 2400/7568 Train_loss 1.8102490739964385 
Epoch [4/10] Batch 2500/7568 Train_loss 1.8072237823305966 
Epoch [4/10] Batch 2600/7568 Train_loss 1.807018657806102 
Epoch [4/10] Batch 2700/7568 Train_loss 1.8081338130425895 
Epoch [4/10] Batch 2800/7568 Train_loss 1.8071528593053652 
Epoch [4/10] Batch 2900/7568 Train_loss 1.8054659157652397 
Epoch [4/10] Batch 3000/7568 Train_loss 1.8058390923090197 
Epoch [4/10] Batch 3100/7568 Train_loss 1.8046383060916167 
Epoch [4/10] Batch 3200/7568 Train_loss 1.8025375125241332 
Epoch [4/10] Batch 3300/7568 Train_loss 1.8018011236774925 
Epoch [4/10] Batch 3400/7568 Train_loss 1.8018996169074812 
Epoch [4/10] Batch 3500/7568 Train_loss 1.804003736956039 
Epoch [4/10] Batch 3600/7568 Train_loss 1.8046750682252244 
Epoch [4/10] Batch 3700/7568 Train_loss 1.8029907820366324 
Epoch [4/10] Batch 3800/7568 Train_loss 1.802653609261689 
Epoch [4/10] Batch 3900/7568 Train_loss 1.8010517344321846 
Epoch [4/10] Batch 4000/7568 Train_loss 1.7997548960929541 
Epoch [4/10] Batch 4100/7568 Train_loss 1.799767608968943 
Epoch [4/10] Batch 4200/7568 Train_loss 1.7995293563024766 
Epoch [4/10] Batch 4300/7568 Train_loss 1.7986256957930677 
Epoch [4/10] Batch 4400/7568 Train_loss 1.8002719148973176 
Epoch [4/10] Batch 4500/7568 Train_loss 1.7985734916516634 
Epoch [4/10] Batch 4600/7568 Train_loss 1.7996624764156222 
Epoch [4/10] Batch 4700/7568 Train_loss 1.7992710368024292 
Epoch [4/10] Batch 4800/7568 Train_loss 1.79812002626 
Epoch [4/10] Batch 4900/7568 Train_loss 1.7966360807072215 
Epoch [4/10] Batch 5000/7568 Train_loss 1.796942578240934 
Epoch [4/10] Batch 5100/7568 Train_loss 1.796189408754454 
Epoch [4/10] Batch 5200/7568 Train_loss 1.7959871442142965 
Epoch [4/10] Batch 5300/7568 Train_loss 1.7953125021788134 
Epoch [4/10] Batch 5400/7568 Train_loss 1.7947090106142536 
Epoch [4/10] Batch 5500/7568 Train_loss 1.7943184603858247 
Epoch [4/10] Batch 5600/7568 Train_loss 1.795014360501572 
Epoch [4/10] Batch 5700/7568 Train_loss 1.7965265901828205 
Epoch [4/10] Batch 5800/7568 Train_loss 1.7961233837661343 
Epoch [4/10] Batch 5900/7568 Train_loss 1.7942122007450134 
Epoch [4/10] Batch 6000/7568 Train_loss 1.7951843577737094 
Epoch [4/10] Batch 6100/7568 Train_loss 1.7951007565635186 
Epoch [4/10] Batch 6200/7568 Train_loss 1.7938446025430934 
Epoch [4/10] Batch 6300/7568 Train_loss 1.7946940110344543 
Epoch [4/10] Batch 6400/7568 Train_loss 1.7940138118192286 
Epoch [4/10] Batch 6500/7568 Train_loss 1.7958631961815743 
Epoch [4/10] Batch 6600/7568 Train_loss 1.7948939329739269 
Epoch [4/10] Batch 6700/7568 Train_loss 1.7942859643313924 
Epoch [4/10] Batch 6800/7568 Train_loss 1.7942385304893944 
Epoch [4/10] Batch 6900/7568 Train_loss 1.793387770974503 
Epoch [4/10] Batch 7000/7568 Train_loss 1.7924239460276188 
Epoch [4/10] Batch 7100/7568 Train_loss 1.7922438589213112 
Epoch [4/10] Batch 7200/7568 Train_loss 1.7916693401645412 
Epoch [4/10] Batch 7300/7568 Train_loss 1.7919332959618148 
Epoch [4/10] Batch 7400/7568 Train_loss 1.7913474806620533 
Epoch [4/10] Batch 7500/7568 Train_loss 1.79061354395092 
Epoch: 4/10 	Training Loss: 1.791104 	Validation Loss: 1.777250 Duration seconds: 1251.3271152973175 
Validation loss decreased (1.797054 --> 1.777250).  Saving model ... 
best_valid_loss_fold [1.7772504331004544] Best_Epoch [4]Epoch [5/10] Batch 0/7568 Train_loss 2.198270261287689 
Epoch [5/10] Batch 100/7568 Train_loss 1.8205264822976424 
Epoch [5/10] Batch 200/7568 Train_loss 1.8018190885212884 
Epoch [5/10] Batch 300/7568 Train_loss 1.8225103931074522 
Epoch [5/10] Batch 400/7568 Train_loss 1.8224704936332536 
Epoch [5/10] Batch 500/7568 Train_loss 1.8262132377681617 
Epoch [5/10] Batch 600/7568 Train_loss 1.8213306596741303 
Epoch [5/10] Batch 700/7568 Train_loss 1.834841023349388 
Epoch [5/10] Batch 800/7568 Train_loss 1.8172265871075655 
Epoch [5/10] Batch 900/7568 Train_loss 1.8142638016454917 
Epoch [5/10] Batch 1000/7568 Train_loss 1.8128706844566347 
Epoch [5/10] Batch 1100/7568 Train_loss 1.8121758305030342 
Epoch [5/10] Batch 1200/7568 Train_loss 1.8106098382498799 
Epoch [5/10] Batch 1300/7568 Train_loss 1.8005597888621616 
Epoch [5/10] Batch 1400/7568 Train_loss 1.8058570643500036 
Epoch [5/10] Batch 1500/7568 Train_loss 1.8080268356996247 
Epoch [5/10] Batch 1600/7568 Train_loss 1.7955962134796257 
Epoch [5/10] Batch 1700/7568 Train_loss 1.792861750149012 
Epoch [5/10] Batch 1800/7568 Train_loss 1.7905425471044263 
Epoch [5/10] Batch 1900/7568 Train_loss 1.7914095943865183 
Epoch [5/10] Batch 2000/7568 Train_loss 1.7906853257269337 
Epoch [5/10] Batch 2100/7568 Train_loss 1.7882651964761938 
Epoch [5/10] Batch 2200/7568 Train_loss 1.7885023043378816 
Epoch [5/10] Batch 2300/7568 Train_loss 1.789195194471302 
Epoch [5/10] Batch 2400/7568 Train_loss 1.7903349839805662 
Epoch [5/10] Batch 2500/7568 Train_loss 1.7876666792180957 
Epoch [5/10] Batch 2600/7568 Train_loss 1.789428348803763 
Epoch [5/10] Batch 2700/7568 Train_loss 1.7919289208576221 
Epoch [5/10] Batch 2800/7568 Train_loss 1.79019882622009 
Epoch [5/10] Batch 2900/7568 Train_loss 1.7900243944590233 
Epoch [5/10] Batch 3000/7568 Train_loss 1.7896476279111235 
Epoch [5/10] Batch 3100/7568 Train_loss 1.7901098291980992 
Epoch [5/10] Batch 3200/7568 Train_loss 1.793722671627421 
Epoch [5/10] Batch 3300/7568 Train_loss 1.7931078030521563 
Epoch [5/10] Batch 3400/7568 Train_loss 1.79197118048833 
Epoch [5/10] Batch 3500/7568 Train_loss 1.7928533358948975 
Epoch [5/10] Batch 3600/7568 Train_loss 1.7928751998044854 
Epoch [5/10] Batch 3700/7568 Train_loss 1.7917838682096188 
Epoch [5/10] Batch 3800/7568 Train_loss 1.7919463639497224 
Epoch [5/10] Batch 3900/7568 Train_loss 1.7929382228703872 
Epoch [5/10] Batch 4000/7568 Train_loss 1.7934591263081991 
Epoch [5/10] Batch 4100/7568 Train_loss 1.791710266532316 
Epoch [5/10] Batch 4200/7568 Train_loss 1.7920138177095468 
Epoch [5/10] Batch 4300/7568 Train_loss 1.7926526872990742 
Epoch [5/10] Batch 4400/7568 Train_loss 1.793733558925857 
Epoch [5/10] Batch 4500/7568 Train_loss 1.7952615220221353 
Epoch [5/10] Batch 4600/7568 Train_loss 1.7944362931644962 
Epoch [5/10] Batch 4700/7568 Train_loss 1.7943512998526083 
Epoch [5/10] Batch 4800/7568 Train_loss 1.7935447673332912 
Epoch [5/10] Batch 4900/7568 Train_loss 1.7938842907414172 
Epoch [5/10] Batch 5000/7568 Train_loss 1.7924211674754058 
Epoch [5/10] Batch 5100/7568 Train_loss 1.7926188659633497 
Epoch [5/10] Batch 5200/7568 Train_loss 1.7921315898613457 
Epoch [5/10] Batch 5300/7568 Train_loss 1.79091383385936 
Epoch [5/10] Batch 5400/7568 Train_loss 1.7913958420702825 
Epoch [5/10] Batch 5500/7568 Train_loss 1.7917185938712077 
Epoch [5/10] Batch 5600/7568 Train_loss 1.7922774529315024 
Epoch [5/10] Batch 5700/7568 Train_loss 1.7920046078243437 
Epoch [5/10] Batch 5800/7568 Train_loss 1.7912427606014543 
Epoch [5/10] Batch 5900/7568 Train_loss 1.7914140153183227 
Epoch [5/10] Batch 6000/7568 Train_loss 1.7916693968564605 
Epoch [5/10] Batch 6100/7568 Train_loss 1.7906056333945688 
Epoch [5/10] Batch 6200/7568 Train_loss 1.791152839927813 
Epoch [5/10] Batch 6300/7568 Train_loss 1.7921850680087015 
Epoch [5/10] Batch 6400/7568 Train_loss 1.7917923053409082 
Epoch [5/10] Batch 6500/7568 Train_loss 1.7899273003746208 
Epoch [5/10] Batch 6600/7568 Train_loss 1.789662711347139 
Epoch [5/10] Batch 6700/7568 Train_loss 1.7896957386831547 
Epoch [5/10] Batch 6800/7568 Train_loss 1.7904313916411405 
Epoch [5/10] Batch 6900/7568 Train_loss 1.7902339536115288 
Epoch [5/10] Batch 7000/7568 Train_loss 1.7900818216055312 
Epoch [5/10] Batch 7100/7568 Train_loss 1.7902582258021742 
Epoch [5/10] Batch 7200/7568 Train_loss 1.7894255333533389 
Epoch [5/10] Batch 7300/7568 Train_loss 1.7891660586628109 
Epoch [5/10] Batch 7400/7568 Train_loss 1.7886617975484405 
Epoch [5/10] Batch 7500/7568 Train_loss 1.7882889078565238 
Epoch: 5/10 	Training Loss: 1.787140 	Validation Loss: 1.767474 Duration seconds: 1250.075438261032 
Validation loss decreased (1.777250 --> 1.767474).  Saving model ... 
best_valid_loss_fold [1.7674740006237528] Best_Epoch [5]Epoch [6/10] Batch 0/7568 Train_loss 1.7634958028793335 
Epoch [6/10] Batch 100/7568 Train_loss 1.793044553977428 
Epoch [6/10] Batch 200/7568 Train_loss 1.7829607321477647 
Epoch [6/10] Batch 300/7568 Train_loss 1.7941227507007083 
Epoch [6/10] Batch 400/7568 Train_loss 1.8027657934666572 
Epoch [6/10] Batch 500/7568 Train_loss 1.8008833633598216 
Epoch [6/10] Batch 600/7568 Train_loss 1.8163552999917958 
Epoch [6/10] Batch 700/7568 Train_loss 1.803365265538706 
Epoch [6/10] Batch 800/7568 Train_loss 1.7999706932165649 
Epoch [6/10] Batch 900/7568 Train_loss 1.7989436472113864 
Epoch [6/10] Batch 1000/7568 Train_loss 1.8030315474955947 
Epoch [6/10] Batch 1100/7568 Train_loss 1.7964896751562647 
Epoch [6/10] Batch 1200/7568 Train_loss 1.7890998363172284 
Epoch [6/10] Batch 1300/7568 Train_loss 1.7870644990821511 
Epoch [6/10] Batch 1400/7568 Train_loss 1.7875833095162363 
Epoch [6/10] Batch 1500/7568 Train_loss 1.7880669068904975 
Epoch [6/10] Batch 1600/7568 Train_loss 1.790620111171005 
Epoch [6/10] Batch 1700/7568 Train_loss 1.7922392462340262 
Epoch [6/10] Batch 1800/7568 Train_loss 1.7951658168918287 
Epoch [6/10] Batch 1900/7568 Train_loss 1.7938377847919522 
Epoch [6/10] Batch 2000/7568 Train_loss 1.7957549032637443 
Epoch [6/10] Batch 2100/7568 Train_loss 1.7905896118904567 
Epoch [6/10] Batch 2200/7568 Train_loss 1.7868684919915758 
Epoch [6/10] Batch 2300/7568 Train_loss 1.7903730797636825 
Epoch [6/10] Batch 2400/7568 Train_loss 1.7911827490540506 
Epoch [6/10] Batch 2500/7568 Train_loss 1.790961471126705 
Epoch [6/10] Batch 2600/7568 Train_loss 1.7920062186390517 
Epoch [6/10] Batch 2700/7568 Train_loss 1.7918477220800753 
Epoch [6/10] Batch 2800/7568 Train_loss 1.7895629990982906 
Epoch [6/10] Batch 2900/7568 Train_loss 1.7895741063571107 
Epoch [6/10] Batch 3000/7568 Train_loss 1.7867607104454546 
Epoch [6/10] Batch 3100/7568 Train_loss 1.788019965763478 
Epoch [6/10] Batch 3200/7568 Train_loss 1.7907107872344374 
Epoch [6/10] Batch 3300/7568 Train_loss 1.7921138104075052 
Epoch [6/10] Batch 3400/7568 Train_loss 1.792351741226534 
Epoch [6/10] Batch 3500/7568 Train_loss 1.79035924239921 
Epoch [6/10] Batch 3600/7568 Train_loss 1.790208266114351 
Epoch [6/10] Batch 3700/7568 Train_loss 1.7879642589761096 
Epoch [6/10] Batch 3800/7568 Train_loss 1.7884205640542354 
Epoch [6/10] Batch 3900/7568 Train_loss 1.7857129459780963 
Epoch [6/10] Batch 4000/7568 Train_loss 1.787583579455233 
Epoch [6/10] Batch 4100/7568 Train_loss 1.7871629518779915 
Epoch [6/10] Batch 4200/7568 Train_loss 1.786469623377297 
Epoch [6/10] Batch 4300/7568 Train_loss 1.7859818008844173 
Epoch [6/10] Batch 4400/7568 Train_loss 1.7865471309188468 
Epoch [6/10] Batch 4500/7568 Train_loss 1.7837873941887037 
Epoch [6/10] Batch 4600/7568 Train_loss 1.783530033899763 
Epoch [6/10] Batch 4700/7568 Train_loss 1.7834508944587286 
Epoch [6/10] Batch 4800/7568 Train_loss 1.7848804843094268 
Epoch [6/10] Batch 4900/7568 Train_loss 1.7848557135300402 
Epoch [6/10] Batch 5000/7568 Train_loss 1.78561379226106 
Epoch [6/10] Batch 5100/7568 Train_loss 1.7849824400982 
Epoch [6/10] Batch 5200/7568 Train_loss 1.7844675510829513 
Epoch [6/10] Batch 5300/7568 Train_loss 1.7849806295326578 
Epoch [6/10] Batch 5400/7568 Train_loss 1.7852081712269823 
Epoch [6/10] Batch 5500/7568 Train_loss 1.7850202954106928 
Epoch [6/10] Batch 5600/7568 Train_loss 1.7854486106815264 
Epoch [6/10] Batch 5700/7568 Train_loss 1.7840809806452114 
Epoch [6/10] Batch 5800/7568 Train_loss 1.7850833797050003 
Epoch [6/10] Batch 5900/7568 Train_loss 1.7845239784203026 
Epoch [6/10] Batch 6000/7568 Train_loss 1.7842256247257315 
Epoch [6/10] Batch 6100/7568 Train_loss 1.783146256038428 
Epoch [6/10] Batch 6200/7568 Train_loss 1.7824862731707478 
Epoch [6/10] Batch 6300/7568 Train_loss 1.7827180015427824 
Epoch [6/10] Batch 6400/7568 Train_loss 1.781943924754155 
Epoch [6/10] Batch 6500/7568 Train_loss 1.7832308182705734 
Epoch [6/10] Batch 6600/7568 Train_loss 1.7839485271769642 
Epoch [6/10] Batch 6700/7568 Train_loss 1.7835487509383994 
Epoch [6/10] Batch 6800/7568 Train_loss 1.7834952558871315 
Epoch [6/10] Batch 6900/7568 Train_loss 1.781301313233607 
Epoch [6/10] Batch 7000/7568 Train_loss 1.7810900560120586 
Epoch [6/10] Batch 7100/7568 Train_loss 1.7817533560755086 
Epoch [6/10] Batch 7200/7568 Train_loss 1.7822924949960268 
Epoch [6/10] Batch 7300/7568 Train_loss 1.781561085896906 
Epoch [6/10] Batch 7400/7568 Train_loss 1.7814746212555 
Epoch [6/10] Batch 7500/7568 Train_loss 1.780995486281567 
Epoch: 6/10 	Training Loss: 1.779531 	Validation Loss: 1.847914 Duration seconds: 1215.0948867797852 
best_valid_loss_fold [1.7674740006237528] Best_Epoch [6]Epoch [7/10] Batch 0/7568 Train_loss 2.2720210254192352 
Epoch [7/10] Batch 100/7568 Train_loss 1.8562615559537812 
Epoch [7/10] Batch 200/7568 Train_loss 1.7491044719420856 
Epoch [7/10] Batch 300/7568 Train_loss 1.7258491175267783 
Epoch [7/10] Batch 400/7568 Train_loss 1.7325058229211858 
Epoch [7/10] Batch 500/7568 Train_loss 1.750862084388614 
Epoch [7/10] Batch 600/7568 Train_loss 1.7561167035369627 
Epoch [7/10] Batch 700/7568 Train_loss 1.7568696472001994 
Epoch [7/10] Batch 800/7568 Train_loss 1.7678212377239255 
Epoch [7/10] Batch 900/7568 Train_loss 1.764946042350209 
Epoch [7/10] Batch 1000/7568 Train_loss 1.7645850172498962 
Epoch [7/10] Batch 1100/7568 Train_loss 1.7693256541788849 
Epoch [7/10] Batch 1200/7568 Train_loss 1.777722116507013 
Epoch [7/10] Batch 1300/7568 Train_loss 1.7797737389654036 
Epoch [7/10] Batch 1400/7568 Train_loss 1.7833440200153798 
Epoch [7/10] Batch 1500/7568 Train_loss 1.7812809730562904 
Epoch [7/10] Batch 1600/7568 Train_loss 1.7813486226373059 
Epoch [7/10] Batch 1700/7568 Train_loss 1.7743491794013893 
Epoch [7/10] Batch 1800/7568 Train_loss 1.7812388933893046 
Epoch [7/10] Batch 1900/7568 Train_loss 1.7794038235420055 
Epoch [7/10] Batch 2000/7568 Train_loss 1.7792485754141982 
Epoch [7/10] Batch 2100/7568 Train_loss 1.7801420220742619 
Epoch [7/10] Batch 2200/7568 Train_loss 1.7809496570491943 
Epoch [7/10] Batch 2300/7568 Train_loss 1.780818940821496 
Epoch [7/10] Batch 2400/7568 Train_loss 1.7785744892835915 
Epoch [7/10] Batch 2500/7568 Train_loss 1.7779401743092664 
Epoch [7/10] Batch 2600/7568 Train_loss 1.77947233633347 
Epoch [7/10] Batch 2700/7568 Train_loss 1.779237432844636 
Epoch [7/10] Batch 2800/7568 Train_loss 1.779470881743118 
Epoch [7/10] Batch 2900/7568 Train_loss 1.7788607654377167 
Epoch [7/10] Batch 3000/7568 Train_loss 1.77695424737572 
Epoch [7/10] Batch 3100/7568 Train_loss 1.7779070209809098 
Epoch [7/10] Batch 3200/7568 Train_loss 1.7794310591442255 
Epoch [7/10] Batch 3300/7568 Train_loss 1.780762378046257 
Epoch [7/10] Batch 3400/7568 Train_loss 1.7800880213106811 
Epoch [7/10] Batch 3500/7568 Train_loss 1.7772398801066642 
Epoch [7/10] Batch 3600/7568 Train_loss 1.776288685995865 
Epoch [7/10] Batch 3700/7568 Train_loss 1.7775095609398959 
Epoch [7/10] Batch 3800/7568 Train_loss 1.777384748745595 
Epoch [7/10] Batch 3900/7568 Train_loss 1.7768992804577184 
Epoch [7/10] Batch 4000/7568 Train_loss 1.778652035612385 
Epoch [7/10] Batch 4100/7568 Train_loss 1.7794955772445626 
Epoch [7/10] Batch 4200/7568 Train_loss 1.7799015094806994 
Epoch [7/10] Batch 4300/7568 Train_loss 1.7797019788326287 
Epoch [7/10] Batch 4400/7568 Train_loss 1.77964436992837 
Epoch [7/10] Batch 4500/7568 Train_loss 1.778962832598905 
Epoch [7/10] Batch 4600/7568 Train_loss 1.7777053404488503 
Epoch [7/10] Batch 4700/7568 Train_loss 1.7769521808378925 
Epoch [7/10] Batch 4800/7568 Train_loss 1.7762785686354194 
Epoch [7/10] Batch 4900/7568 Train_loss 1.7781358367978761 
Epoch [7/10] Batch 5000/7568 Train_loss 1.7791189500118607 
Epoch [7/10] Batch 5100/7568 Train_loss 1.778820649751949 
Epoch [7/10] Batch 5200/7568 Train_loss 1.7790997809293727 
Epoch [7/10] Batch 5300/7568 Train_loss 1.7787594037083743 
Epoch [7/10] Batch 5400/7568 Train_loss 1.7770235939649737 
Epoch [7/10] Batch 5500/7568 Train_loss 1.7754072666918552 
Epoch [7/10] Batch 5600/7568 Train_loss 1.7758229558788927 
Epoch [7/10] Batch 5700/7568 Train_loss 1.7769473803995304 
Epoch [7/10] Batch 5800/7568 Train_loss 1.7762213660966957 
Epoch [7/10] Batch 5900/7568 Train_loss 1.7768247103152226 
Epoch [7/10] Batch 6000/7568 Train_loss 1.7764469626046304 
Epoch [7/10] Batch 6100/7568 Train_loss 1.7755751028275943 
Epoch [7/10] Batch 6200/7568 Train_loss 1.7767138988647109 
Epoch [7/10] Batch 6300/7568 Train_loss 1.7754208998613614 
Epoch [7/10] Batch 6400/7568 Train_loss 1.775102670976816 
Epoch [7/10] Batch 6500/7568 Train_loss 1.7745901347112059 
Epoch [7/10] Batch 6600/7568 Train_loss 1.7740203587767518 
Epoch [7/10] Batch 6700/7568 Train_loss 1.7747884457593195 
Epoch [7/10] Batch 6800/7568 Train_loss 1.7745933603068658 
Epoch [7/10] Batch 6900/7568 Train_loss 1.7752949009683059 
Epoch [7/10] Batch 7000/7568 Train_loss 1.7756145288605056 
Epoch [7/10] Batch 7100/7568 Train_loss 1.7764639948232295 
Epoch [7/10] Batch 7200/7568 Train_loss 1.7766615725953352 
Epoch [7/10] Batch 7300/7568 Train_loss 1.7766734916467304 
Epoch [7/10] Batch 7400/7568 Train_loss 1.7758597689469977 
Epoch [7/10] Batch 7500/7568 Train_loss 1.7753411478447034 
Epoch: 7/10 	Training Loss: 1.774722 	Validation Loss: 1.765516 Duration seconds: 1223.1379418373108 
Validation loss decreased (1.767474 --> 1.765516).  Saving model ... 
best_valid_loss_fold [1.7655161968284694] Best_Epoch [7]Epoch [8/10] Batch 0/7568 Train_loss 1.4059440791606903 
Epoch [8/10] Batch 100/7568 Train_loss 1.8630103261606528 
Epoch [8/10] Batch 200/7568 Train_loss 1.8425647901435989 
Epoch [8/10] Batch 300/7568 Train_loss 1.8130486937505859 
Epoch [8/10] Batch 400/7568 Train_loss 1.8086807287653486 
Epoch [8/10] Batch 500/7568 Train_loss 1.8008840252866527 
Epoch [8/10] Batch 600/7568 Train_loss 1.797120754549686 
Epoch [8/10] Batch 700/7568 Train_loss 1.7864636124009412 
Epoch [8/10] Batch 800/7568 Train_loss 1.7956253930405432 
Epoch [8/10] Batch 900/7568 Train_loss 1.7947970218667046 
Epoch [8/10] Batch 1000/7568 Train_loss 1.7868755468642794 
Epoch [8/10] Batch 1100/7568 Train_loss 1.7860744445906998 
Epoch [8/10] Batch 1200/7568 Train_loss 1.7750414460587263 
Epoch [8/10] Batch 1300/7568 Train_loss 1.7760460743026307 
Epoch [8/10] Batch 1400/7568 Train_loss 1.7759153694127134 
Epoch [8/10] Batch 1500/7568 Train_loss 1.777938002550467 
Epoch [8/10] Batch 1600/7568 Train_loss 1.7753946984525326 
Epoch [8/10] Batch 1700/7568 Train_loss 1.774322913761422 
Epoch [8/10] Batch 1800/7568 Train_loss 1.7770295180607876 
Epoch [8/10] Batch 1900/7568 Train_loss 1.7800561458201611 
Epoch [8/10] Batch 2000/7568 Train_loss 1.7802281372401787 
Epoch [8/10] Batch 2100/7568 Train_loss 1.779892756747723 
Epoch [8/10] Batch 2200/7568 Train_loss 1.7798293300010246 
Epoch [8/10] Batch 2300/7568 Train_loss 1.780912421387934 
Epoch [8/10] Batch 2400/7568 Train_loss 1.7836802006884944 
Epoch [8/10] Batch 2500/7568 Train_loss 1.7861807903424591 
Epoch [8/10] Batch 2600/7568 Train_loss 1.7865133598040086 
Epoch [8/10] Batch 2700/7568 Train_loss 1.7861090806822828 
Epoch [8/10] Batch 2800/7568 Train_loss 1.7835964300257101 
Epoch [8/10] Batch 2900/7568 Train_loss 1.7837892244680715 
Epoch [8/10] Batch 3000/7568 Train_loss 1.7847494108205793 
Epoch [8/10] Batch 3100/7568 Train_loss 1.7823666628475383 
Epoch [8/10] Batch 3200/7568 Train_loss 1.7846722886901987 
Epoch [8/10] Batch 3300/7568 Train_loss 1.7825705628482114 
Epoch [8/10] Batch 3400/7568 Train_loss 1.7833809937073601 
Epoch [8/10] Batch 3500/7568 Train_loss 1.782950093233902 
Epoch [8/10] Batch 3600/7568 Train_loss 1.7821287110311328 
Epoch [8/10] Batch 3700/7568 Train_loss 1.7817947768546607 
Epoch [8/10] Batch 3800/7568 Train_loss 1.7814607985300066 
Epoch [8/10] Batch 3900/7568 Train_loss 1.781257896931683 
Epoch [8/10] Batch 4000/7568 Train_loss 1.782386477620445 
Epoch [8/10] Batch 4100/7568 Train_loss 1.781354052457714 
Epoch [8/10] Batch 4200/7568 Train_loss 1.777685288684804 
Epoch [8/10] Batch 4300/7568 Train_loss 1.7784123181330136 
Epoch [8/10] Batch 4400/7568 Train_loss 1.7787739057140415 
Epoch [8/10] Batch 4500/7568 Train_loss 1.7791946619745758 
Epoch [8/10] Batch 4600/7568 Train_loss 1.779232599412817 
Epoch [8/10] Batch 4700/7568 Train_loss 1.778360624591408 
Epoch [8/10] Batch 4800/7568 Train_loss 1.7779620208421913 
Epoch [8/10] Batch 4900/7568 Train_loss 1.7768536972813498 
Epoch [8/10] Batch 5000/7568 Train_loss 1.7756753992620788 
Epoch [8/10] Batch 5100/7568 Train_loss 1.774828320530157 
Epoch [8/10] Batch 5200/7568 Train_loss 1.7749274643401278 
Epoch [8/10] Batch 5300/7568 Train_loss 1.7747907175801425 
Epoch [8/10] Batch 5400/7568 Train_loss 1.7719536943720173 
Epoch [8/10] Batch 5500/7568 Train_loss 1.7704439087903103 
Epoch [8/10] Batch 5600/7568 Train_loss 1.7713145096532747 
Epoch [8/10] Batch 5700/7568 Train_loss 1.7716573410871843 
Epoch [8/10] Batch 5800/7568 Train_loss 1.770471090120481 
Epoch [8/10] Batch 5900/7568 Train_loss 1.7713319405299732 
Epoch [8/10] Batch 6000/7568 Train_loss 1.7720195015874074 
Epoch [8/10] Batch 6100/7568 Train_loss 1.7714498487481756 
Epoch [8/10] Batch 6200/7568 Train_loss 1.7715390983694428 
Epoch [8/10] Batch 6300/7568 Train_loss 1.770709377876531 
Epoch [8/10] Batch 6400/7568 Train_loss 1.7715172164917337 
Epoch [8/10] Batch 6500/7568 Train_loss 1.7713805382082535 
Epoch [8/10] Batch 6600/7568 Train_loss 1.7721632142999761 
Epoch [8/10] Batch 6700/7568 Train_loss 1.7719150570672522 
Epoch [8/10] Batch 6800/7568 Train_loss 1.7717440758755525 
Epoch [8/10] Batch 6900/7568 Train_loss 1.7722764963017195 
Epoch [8/10] Batch 7000/7568 Train_loss 1.7714865637259123 
Epoch [8/10] Batch 7100/7568 Train_loss 1.7721384850095492 
Epoch [8/10] Batch 7200/7568 Train_loss 1.7724202605399708 
Epoch [8/10] Batch 7300/7568 Train_loss 1.7728038414993355 
Epoch [8/10] Batch 7400/7568 Train_loss 1.77354981073295 
Epoch [8/10] Batch 7500/7568 Train_loss 1.7733784962890984 
Epoch: 8/10 	Training Loss: 1.773290 	Validation Loss: 1.756732 Duration seconds: 1283.3063402175903 
Validation loss decreased (1.765516 --> 1.756732).  Saving model ... 
best_valid_loss_fold [1.7567324164750775] Best_Epoch [8]Epoch [9/10] Batch 0/7568 Train_loss 2.2094009816646576 
Epoch [9/10] Batch 100/7568 Train_loss 1.691574701430774 
Epoch [9/10] Batch 200/7568 Train_loss 1.733057897778886 
Epoch [9/10] Batch 300/7568 Train_loss 1.7541413803730297 
Epoch [9/10] Batch 400/7568 Train_loss 1.7630840356733437 
Epoch [9/10] Batch 500/7568 Train_loss 1.7582516556609415 
Epoch [9/10] Batch 600/7568 Train_loss 1.7523990023552678 
Epoch [9/10] Batch 700/7568 Train_loss 1.742878399906247 
Epoch [9/10] Batch 800/7568 Train_loss 1.7485776358515432 
Epoch [9/10] Batch 900/7568 Train_loss 1.7554983544627516 
Epoch [9/10] Batch 1000/7568 Train_loss 1.7529200047224789 
Epoch [9/10] Batch 1100/7568 Train_loss 1.7478962697352636 
Epoch [9/10] Batch 1200/7568 Train_loss 1.7473027118784898 
Epoch [9/10] Batch 1300/7568 Train_loss 1.744917382748882 
Epoch [9/10] Batch 1400/7568 Train_loss 1.7460558961260582 
Epoch [9/10] Batch 1500/7568 Train_loss 1.7495899575996368 
Epoch [9/10] Batch 1600/7568 Train_loss 1.752037384095302 
Epoch [9/10] Batch 1700/7568 Train_loss 1.7522410020398644 
Epoch [9/10] Batch 1800/7568 Train_loss 1.7511539321717522 
Epoch [9/10] Batch 1900/7568 Train_loss 1.7524084255985057 
Epoch [9/10] Batch 2000/7568 Train_loss 1.7550028594343856 
Epoch [9/10] Batch 2100/7568 Train_loss 1.7579300098053787 
Epoch [9/10] Batch 2200/7568 Train_loss 1.7553240110397825 
Epoch [9/10] Batch 2300/7568 Train_loss 1.7564582561459658 
Epoch [9/10] Batch 2400/7568 Train_loss 1.7572694202913735 
Epoch [9/10] Batch 2500/7568 Train_loss 1.757999464708631 
Epoch [9/10] Batch 2600/7568 Train_loss 1.758272755677679 
Epoch [9/10] Batch 2700/7568 Train_loss 1.757505159109268 
Epoch [9/10] Batch 2800/7568 Train_loss 1.7576678873161808 
Epoch [9/10] Batch 2900/7568 Train_loss 1.7601630749917367 
Epoch [9/10] Batch 3000/7568 Train_loss 1.7594689252559481 
Epoch [9/10] Batch 3100/7568 Train_loss 1.7579083968889748 
Epoch [9/10] Batch 3200/7568 Train_loss 1.759749121675358 
Epoch [9/10] Batch 3300/7568 Train_loss 1.7594664142447614 
Epoch [9/10] Batch 3400/7568 Train_loss 1.759681119075578 
Epoch [9/10] Batch 3500/7568 Train_loss 1.7602449366426474 
Epoch [9/10] Batch 3600/7568 Train_loss 1.7607928443078331 
Epoch [9/10] Batch 3700/7568 Train_loss 1.7595626560780397 
Epoch [9/10] Batch 3800/7568 Train_loss 1.7621239125375778 
Epoch [9/10] Batch 3900/7568 Train_loss 1.7622965436368623 
Epoch [9/10] Batch 4000/7568 Train_loss 1.762010848163158 
Epoch [9/10] Batch 4100/7568 Train_loss 1.7611014686744697 
Epoch [9/10] Batch 4200/7568 Train_loss 1.7598939237424913 
Epoch [9/10] Batch 4300/7568 Train_loss 1.759577381804194 
Epoch [9/10] Batch 4400/7568 Train_loss 1.7604722387172829 
Epoch [9/10] Batch 4500/7568 Train_loss 1.7596987597818137 
Epoch [9/10] Batch 4600/7568 Train_loss 1.7594325355578184 
Epoch [9/10] Batch 4700/7568 Train_loss 1.761017985130036 
Epoch [9/10] Batch 4800/7568 Train_loss 1.7596417539003266 
Epoch [9/10] Batch 4900/7568 Train_loss 1.7596609380276342 
Epoch [9/10] Batch 5000/7568 Train_loss 1.7580450157705867 
Epoch [9/10] Batch 5100/7568 Train_loss 1.7578521286318336 
Epoch [9/10] Batch 5200/7568 Train_loss 1.757456687144925 
Epoch [9/10] Batch 5300/7568 Train_loss 1.757553029475831 
Epoch [9/10] Batch 5400/7568 Train_loss 1.7578339667629035 
Epoch [9/10] Batch 5500/7568 Train_loss 1.7591937793897208 
Epoch [9/10] Batch 5600/7568 Train_loss 1.7596708888071944 
Epoch [9/10] Batch 5700/7568 Train_loss 1.759799387775802 
Epoch [9/10] Batch 5800/7568 Train_loss 1.7601008700465997 
Epoch [9/10] Batch 5900/7568 Train_loss 1.7598927883821773 
Epoch [9/10] Batch 6000/7568 Train_loss 1.7608651637328285 
Epoch [9/10] Batch 6100/7568 Train_loss 1.7606558744459635 
Epoch [9/10] Batch 6200/7568 Train_loss 1.7604967162200489 
Epoch [9/10] Batch 6300/7568 Train_loss 1.7617552003627674 
Epoch [9/10] Batch 6400/7568 Train_loss 1.7618611360623806 
Epoch [9/10] Batch 6500/7568 Train_loss 1.7618795565503265 
Epoch [9/10] Batch 6600/7568 Train_loss 1.7614686740975707 
Epoch [9/10] Batch 6700/7568 Train_loss 1.7625392680947967 
Epoch [9/10] Batch 6800/7568 Train_loss 1.7626806119216696 
Epoch [9/10] Batch 6900/7568 Train_loss 1.761438496445395 
Epoch [9/10] Batch 7000/7568 Train_loss 1.7618977691653097 
Epoch [9/10] Batch 7100/7568 Train_loss 1.761274365127221 
Epoch [9/10] Batch 7200/7568 Train_loss 1.7619793520822093 
Epoch [9/10] Batch 7300/7568 Train_loss 1.7614034747347949 
Epoch [9/10] Batch 7400/7568 Train_loss 1.7634190915917773 
Epoch [9/10] Batch 7500/7568 Train_loss 1.7632124912246532 
Epoch: 9/10 	Training Loss: 1.762602 	Validation Loss: 1.751947 Duration seconds: 1213.6710209846497 
Validation loss decreased (1.756732 --> 1.751947).  Saving model ... 
best_valid_loss_fold [1.7519470257901175] Best_Epoch [9]Fold: 4/5 
Epoch [0/10] Batch 0/7568 Train_loss 1.7951053380966187 
Epoch [0/10] Batch 100/7568 Train_loss 1.7844450658205713 
Epoch [0/10] Batch 200/7568 Train_loss 1.83872157402003 
Epoch [0/10] Batch 300/7568 Train_loss 1.8449034600014307 
Epoch [0/10] Batch 400/7568 Train_loss 1.8355227671470726 
Epoch [0/10] Batch 500/7568 Train_loss 1.8492860618435931 
Epoch [0/10] Batch 600/7568 Train_loss 1.850164317759916 
Epoch [0/10] Batch 700/7568 Train_loss 1.8494404703059821 
Epoch [0/10] Batch 800/7568 Train_loss 1.84680603914307 
Epoch [0/10] Batch 900/7568 Train_loss 1.8422920110901109 
Epoch [0/10] Batch 1000/7568 Train_loss 1.831445866017849 
Epoch [0/10] Batch 1100/7568 Train_loss 1.8315888503794775 
Epoch [0/10] Batch 1200/7568 Train_loss 1.832654981732021 
Epoch [0/10] Batch 1300/7568 Train_loss 1.827664638480757 
Epoch [0/10] Batch 1400/7568 Train_loss 1.8303386503026198 
Epoch [0/10] Batch 1500/7568 Train_loss 1.8324721715525338 
Epoch [0/10] Batch 1600/7568 Train_loss 1.8283817323993923 
Epoch [0/10] Batch 1700/7568 Train_loss 1.8292335986977182 
Epoch [0/10] Batch 1800/7568 Train_loss 1.82733598459452 
Epoch [0/10] Batch 1900/7568 Train_loss 1.8243139110768174 
Epoch [0/10] Batch 2000/7568 Train_loss 1.8238418842891584 
Epoch [0/10] Batch 2100/7568 Train_loss 1.8230297376199145 
Epoch [0/10] Batch 2200/7568 Train_loss 1.8207866308710512 
Epoch [0/10] Batch 2300/7568 Train_loss 1.8166292262875168 
Epoch [0/10] Batch 2400/7568 Train_loss 1.8135976208616722 
Epoch [0/10] Batch 2500/7568 Train_loss 1.8107802392255778 
Epoch [0/10] Batch 2600/7568 Train_loss 1.806534536237306 
Epoch [0/10] Batch 2700/7568 Train_loss 1.8096983591380802 
Epoch [0/10] Batch 2800/7568 Train_loss 1.8114724412416825 
Epoch [0/10] Batch 2900/7568 Train_loss 1.8109429544187834 
Epoch [0/10] Batch 3000/7568 Train_loss 1.8095699608852291 
Epoch [0/10] Batch 3100/7568 Train_loss 1.8128008488655782 
Epoch [0/10] Batch 3200/7568 Train_loss 1.8120173830626272 
Epoch [0/10] Batch 3300/7568 Train_loss 1.811118499302243 
Epoch [0/10] Batch 3400/7568 Train_loss 1.810298697019072 
Epoch [0/10] Batch 3500/7568 Train_loss 1.8103589290118292 
Epoch [0/10] Batch 3600/7568 Train_loss 1.8086401039413265 
Epoch [0/10] Batch 3700/7568 Train_loss 1.8094380028994783 
Epoch [0/10] Batch 3800/7568 Train_loss 1.807382120649161 
Epoch [0/10] Batch 3900/7568 Train_loss 1.805960154201443 
Epoch [0/10] Batch 4000/7568 Train_loss 1.8065817683834609 
Epoch [0/10] Batch 4100/7568 Train_loss 1.8041946368427777 
Epoch [0/10] Batch 4200/7568 Train_loss 1.8027582686590427 
Epoch [0/10] Batch 4300/7568 Train_loss 1.802099956863427 
Epoch [0/10] Batch 4400/7568 Train_loss 1.799546508823192 
Epoch [0/10] Batch 4500/7568 Train_loss 1.8004054367588616 
Epoch [0/10] Batch 4600/7568 Train_loss 1.7999689691489325 
Epoch [0/10] Batch 4700/7568 Train_loss 1.7987768724323003 
Epoch [0/10] Batch 4800/7568 Train_loss 1.8011178256285436 
Epoch [0/10] Batch 4900/7568 Train_loss 1.8013125555116627 
Epoch [0/10] Batch 5000/7568 Train_loss 1.801485118166229 
Epoch [0/10] Batch 5100/7568 Train_loss 1.8006522104164544 
Epoch [0/10] Batch 5200/7568 Train_loss 1.8021626366380086 
Epoch [0/10] Batch 5300/7568 Train_loss 1.8027229868107484 
Epoch [0/10] Batch 5400/7568 Train_loss 1.8027592511629311 
Epoch [0/10] Batch 5500/7568 Train_loss 1.802807718046772 
Epoch [0/10] Batch 5600/7568 Train_loss 1.8044581032399514 
Epoch [0/10] Batch 5700/7568 Train_loss 1.8030667135489027 
Epoch [0/10] Batch 5800/7568 Train_loss 1.8023917354981132 
Epoch [0/10] Batch 5900/7568 Train_loss 1.801255509368425 
Epoch [0/10] Batch 6000/7568 Train_loss 1.8006869567655321 
Epoch [0/10] Batch 6100/7568 Train_loss 1.7990772815326885 
Epoch [0/10] Batch 6200/7568 Train_loss 1.7986761458630216 
Epoch [0/10] Batch 6300/7568 Train_loss 1.797936860187586 
Epoch [0/10] Batch 6400/7568 Train_loss 1.798157757126322 
Epoch [0/10] Batch 6500/7568 Train_loss 1.7989284199300206 
Epoch [0/10] Batch 6600/7568 Train_loss 1.798416736136872 
Epoch [0/10] Batch 6700/7568 Train_loss 1.7977138917902622 
Epoch [0/10] Batch 6800/7568 Train_loss 1.7967138258876423 
Epoch [0/10] Batch 6900/7568 Train_loss 1.795500309881724 
Epoch [0/10] Batch 7000/7568 Train_loss 1.79601729887798 
Epoch [0/10] Batch 7100/7568 Train_loss 1.7955335690247107 
Epoch [0/10] Batch 7200/7568 Train_loss 1.7952013067893893 
Epoch [0/10] Batch 7300/7568 Train_loss 1.7946667197738146 
Epoch [0/10] Batch 7400/7568 Train_loss 1.7944155611704853 
Epoch [0/10] Batch 7500/7568 Train_loss 1.7946165592598766 
Epoch: 0/10 	Training Loss: 1.795349 	Validation Loss: 1.816611 Duration seconds: 1098.4103517532349 
Validation loss decreased (inf --> 1.816611).  Saving model ... 
best_valid_loss_fold [1.8166105576355727] Best_Epoch [0]Epoch [1/10] Batch 0/7568 Train_loss 2.7594056725502014 
Epoch [1/10] Batch 100/7568 Train_loss 1.7636874926709893 
Epoch [1/10] Batch 200/7568 Train_loss 1.720412263637455 
Epoch [1/10] Batch 300/7568 Train_loss 1.7474504098593198 
Epoch [1/10] Batch 400/7568 Train_loss 1.7480207406486061 
Epoch [1/10] Batch 500/7568 Train_loss 1.7526590674669442 
Epoch [1/10] Batch 600/7568 Train_loss 1.7673340984643398 
Epoch [1/10] Batch 700/7568 Train_loss 1.768905001768202 
Epoch [1/10] Batch 800/7568 Train_loss 1.7650355514924922 
Epoch [1/10] Batch 900/7568 Train_loss 1.7749061576038037 
Epoch [1/10] Batch 1000/7568 Train_loss 1.7758417278751508 
Epoch [1/10] Batch 1100/7568 Train_loss 1.764936448666153 
Epoch [1/10] Batch 1200/7568 Train_loss 1.7699445587746507 
Epoch [1/10] Batch 1300/7568 Train_loss 1.7709150289877573 
Epoch [1/10] Batch 1400/7568 Train_loss 1.7645010518855406 
Epoch [1/10] Batch 1500/7568 Train_loss 1.7706291993296916 
Epoch [1/10] Batch 1600/7568 Train_loss 1.771509014070034 
Epoch [1/10] Batch 1700/7568 Train_loss 1.7693255364737184 
Epoch [1/10] Batch 1800/7568 Train_loss 1.7733887474837071 
Epoch [1/10] Batch 1900/7568 Train_loss 1.7749788608403974 
Epoch [1/10] Batch 2000/7568 Train_loss 1.777683008017241 
Epoch [1/10] Batch 2100/7568 Train_loss 1.7761356059084161 
Epoch [1/10] Batch 2200/7568 Train_loss 1.7765661652306977 
Epoch [1/10] Batch 2300/7568 Train_loss 1.7794412794138026 
Epoch [1/10] Batch 2400/7568 Train_loss 1.7832173338809245 
Epoch [1/10] Batch 2500/7568 Train_loss 1.7834192203407715 
Epoch [1/10] Batch 2600/7568 Train_loss 1.7860028290922758 
Epoch [1/10] Batch 2700/7568 Train_loss 1.7857116592930053 
Epoch [1/10] Batch 2800/7568 Train_loss 1.7856158632672032 
Epoch [1/10] Batch 2900/7568 Train_loss 1.7824384036946814 
Epoch [1/10] Batch 3000/7568 Train_loss 1.7835048833903533 
Epoch [1/10] Batch 3100/7568 Train_loss 1.7855479943294827 
Epoch [1/10] Batch 3200/7568 Train_loss 1.7837975284283103 
Epoch [1/10] Batch 3300/7568 Train_loss 1.782981828216241 
Epoch [1/10] Batch 3400/7568 Train_loss 1.7836676058664773 
Epoch [1/10] Batch 3500/7568 Train_loss 1.7811962749763273 
Epoch [1/10] Batch 3600/7568 Train_loss 1.7789465143668324 
Epoch [1/10] Batch 3700/7568 Train_loss 1.7789666099391799 
Epoch [1/10] Batch 3800/7568 Train_loss 1.7798976639177384 
Epoch [1/10] Batch 3900/7568 Train_loss 1.7824338359714806 
Epoch [1/10] Batch 4000/7568 Train_loss 1.7799756780054474 
Epoch [1/10] Batch 4100/7568 Train_loss 1.7812646376420975 
Epoch [1/10] Batch 4200/7568 Train_loss 1.781288259524131 
Epoch [1/10] Batch 4300/7568 Train_loss 1.7800442243877939 
Epoch [1/10] Batch 4400/7568 Train_loss 1.7795870386925754 
Epoch [1/10] Batch 4500/7568 Train_loss 1.778359651959517 
Epoch [1/10] Batch 4600/7568 Train_loss 1.7769566109661847 
Epoch [1/10] Batch 4700/7568 Train_loss 1.776213056370822 
Epoch [1/10] Batch 4800/7568 Train_loss 1.7773500656357186 
Epoch [1/10] Batch 4900/7568 Train_loss 1.777493396188271 
Epoch [1/10] Batch 5000/7568 Train_loss 1.7766482266657975 
Epoch [1/10] Batch 5100/7568 Train_loss 1.777764137171923 
Epoch [1/10] Batch 5200/7568 Train_loss 1.7780110237404054 
Epoch [1/10] Batch 5300/7568 Train_loss 1.778536833031428 
Epoch [1/10] Batch 5400/7568 Train_loss 1.7789637998582433 
Epoch [1/10] Batch 5500/7568 Train_loss 1.7776246104505231 
Epoch [1/10] Batch 5600/7568 Train_loss 1.7790855731235788 
Epoch [1/10] Batch 5700/7568 Train_loss 1.779373138228167 
Epoch [1/10] Batch 5800/7568 Train_loss 1.7791930962674394 
Epoch [1/10] Batch 5900/7568 Train_loss 1.7782621958224052 
Epoch [1/10] Batch 6000/7568 Train_loss 1.7789755678439096 
Epoch [1/10] Batch 6100/7568 Train_loss 1.778666135769382 
Epoch [1/10] Batch 6200/7568 Train_loss 1.7782032910065773 
Epoch [1/10] Batch 6300/7568 Train_loss 1.7784213805398266 
Epoch [1/10] Batch 6400/7568 Train_loss 1.7779619946500842 
Epoch [1/10] Batch 6500/7568 Train_loss 1.7773298330237142 
Epoch [1/10] Batch 6600/7568 Train_loss 1.778108609018859 
Epoch [1/10] Batch 6700/7568 Train_loss 1.7772019192867716 
Epoch [1/10] Batch 6800/7568 Train_loss 1.7774507539107718 
Epoch [1/10] Batch 6900/7568 Train_loss 1.777255318042956 
Epoch [1/10] Batch 7000/7568 Train_loss 1.777688684967599 
Epoch [1/10] Batch 7100/7568 Train_loss 1.7786782358453956 
Epoch [1/10] Batch 7200/7568 Train_loss 1.7784939688581127 
Epoch [1/10] Batch 7300/7568 Train_loss 1.7786035696119826 
Epoch [1/10] Batch 7400/7568 Train_loss 1.778891057424361 
Epoch [1/10] Batch 7500/7568 Train_loss 1.7793987588101492 
Epoch: 1/10 	Training Loss: 1.779025 	Validation Loss: 1.840810 Duration seconds: 1301.5145010948181 
best_valid_loss_fold [1.8166105576355727] Best_Epoch [1]Epoch [2/10] Batch 0/7568 Train_loss 1.9666323065757751 
Epoch [2/10] Batch 100/7568 Train_loss 1.7210785078235191 
Epoch [2/10] Batch 200/7568 Train_loss 1.7332949793309123 
Epoch [2/10] Batch 300/7568 Train_loss 1.7653681904671596 
Epoch [2/10] Batch 400/7568 Train_loss 1.7530356844575923 
Epoch [2/10] Batch 500/7568 Train_loss 1.762670173631338 
Epoch [2/10] Batch 600/7568 Train_loss 1.7673733181470443 
Epoch [2/10] Batch 700/7568 Train_loss 1.776357635313621 
Epoch [2/10] Batch 800/7568 Train_loss 1.7738992349243492 
Epoch [2/10] Batch 900/7568 Train_loss 1.7645392889832947 
Epoch [2/10] Batch 1000/7568 Train_loss 1.7682145521431774 
Epoch [2/10] Batch 1100/7568 Train_loss 1.7634869195338165 
Epoch [2/10] Batch 1200/7568 Train_loss 1.7675073021444354 
Epoch [2/10] Batch 1300/7568 Train_loss 1.7651177833084415 
Epoch [2/10] Batch 1400/7568 Train_loss 1.7649592177079543 
Epoch [2/10] Batch 1500/7568 Train_loss 1.7660149680990367 
Epoch [2/10] Batch 1600/7568 Train_loss 1.7645077724510025 
Epoch [2/10] Batch 1700/7568 Train_loss 1.7655309106385897 
Epoch [2/10] Batch 1800/7568 Train_loss 1.7611574225421418 
Epoch [2/10] Batch 1900/7568 Train_loss 1.759696231879635 
Epoch [2/10] Batch 2000/7568 Train_loss 1.759136493249365 
Epoch [2/10] Batch 2100/7568 Train_loss 1.7605672138204522 
Epoch [2/10] Batch 2200/7568 Train_loss 1.7620415878446196 
Epoch [2/10] Batch 2300/7568 Train_loss 1.7609285607105076 
Epoch [2/10] Batch 2400/7568 Train_loss 1.7650684640928638 
Epoch [2/10] Batch 2500/7568 Train_loss 1.7668926722851575 
Epoch [2/10] Batch 2600/7568 Train_loss 1.7685929685974295 
Epoch [2/10] Batch 2700/7568 Train_loss 1.770756651103629 
Epoch [2/10] Batch 2800/7568 Train_loss 1.7702148446293866 
Epoch [2/10] Batch 2900/7568 Train_loss 1.772435237915197 
Epoch [2/10] Batch 3000/7568 Train_loss 1.770286036444898 
Epoch [2/10] Batch 3100/7568 Train_loss 1.7736275726553894 
Epoch [2/10] Batch 3200/7568 Train_loss 1.7746676705365663 
Epoch [2/10] Batch 3300/7568 Train_loss 1.7726285597301006 
Epoch [2/10] Batch 3400/7568 Train_loss 1.7716009024025692 
Epoch [2/10] Batch 3500/7568 Train_loss 1.7727756950237077 
Epoch [2/10] Batch 3600/7568 Train_loss 1.772688078711891 
Epoch [2/10] Batch 3700/7568 Train_loss 1.7754430944788009 
Epoch [2/10] Batch 3800/7568 Train_loss 1.7741512473191408 
Epoch [2/10] Batch 3900/7568 Train_loss 1.7738322039000867 
Epoch [2/10] Batch 4000/7568 Train_loss 1.7723481234845535 
Epoch [2/10] Batch 4100/7568 Train_loss 1.771577849658967 
Epoch [2/10] Batch 4200/7568 Train_loss 1.772943773581581 
Epoch [2/10] Batch 4300/7568 Train_loss 1.771161718481545 
Epoch [2/10] Batch 4400/7568 Train_loss 1.7702728276871276 
Epoch [2/10] Batch 4500/7568 Train_loss 1.7731154768900907 
Epoch [2/10] Batch 4600/7568 Train_loss 1.7740924488789516 
Epoch [2/10] Batch 4700/7568 Train_loss 1.7736474682145666 
Epoch [2/10] Batch 4800/7568 Train_loss 1.7728495984412909 
Epoch [2/10] Batch 4900/7568 Train_loss 1.7744491494381776 
Epoch [2/10] Batch 5000/7568 Train_loss 1.77400266976982 
Epoch [2/10] Batch 5100/7568 Train_loss 1.7739702801997348 
Epoch [2/10] Batch 5200/7568 Train_loss 1.7746626750820875 
Epoch [2/10] Batch 5300/7568 Train_loss 1.7739034274238457 
Epoch [2/10] Batch 5400/7568 Train_loss 1.773188653867778 
Epoch [2/10] Batch 5500/7568 Train_loss 1.7717371284934613 
Epoch [2/10] Batch 5600/7568 Train_loss 1.7712681072750298 
Epoch [2/10] Batch 5700/7568 Train_loss 1.771235063180675 
Epoch [2/10] Batch 5800/7568 Train_loss 1.7721771750418693 
Epoch [2/10] Batch 5900/7568 Train_loss 1.7715817853906886 
Epoch [2/10] Batch 6000/7568 Train_loss 1.7726181544223203 
Epoch [2/10] Batch 6100/7568 Train_loss 1.772734312332374 
Epoch [2/10] Batch 6200/7568 Train_loss 1.7728868692301105 
Epoch [2/10] Batch 6300/7568 Train_loss 1.7737526921923437 
Epoch [2/10] Batch 6400/7568 Train_loss 1.7730695653140518 
Epoch [2/10] Batch 6500/7568 Train_loss 1.7740869904995662 
Epoch [2/10] Batch 6600/7568 Train_loss 1.775142405192846 
Epoch [2/10] Batch 6700/7568 Train_loss 1.7747405516404817 
Epoch [2/10] Batch 6800/7568 Train_loss 1.7750076785050426 
Epoch [2/10] Batch 6900/7568 Train_loss 1.7758504699911868 
Epoch [2/10] Batch 7000/7568 Train_loss 1.7754007572191917 
Epoch [2/10] Batch 7100/7568 Train_loss 1.7759063246363134 
Epoch [2/10] Batch 7200/7568 Train_loss 1.776794074689112 
Epoch [2/10] Batch 7300/7568 Train_loss 1.7764689961090887 
Epoch [2/10] Batch 7400/7568 Train_loss 1.7759218709506854 
Epoch [2/10] Batch 7500/7568 Train_loss 1.7747502417711238 
Epoch: 2/10 	Training Loss: 1.774990 	Validation Loss: 1.805658 Duration seconds: 1216.6526889801025 
Validation loss decreased (1.816611 --> 1.805658).  Saving model ... 
best_valid_loss_fold [1.8056583259349204] Best_Epoch [2]Epoch [3/10] Batch 0/7568 Train_loss 3.1021705269813538 
Epoch [3/10] Batch 100/7568 Train_loss 1.8395665553536746 
Epoch [3/10] Batch 200/7568 Train_loss 1.8238639813009185 
Epoch [3/10] Batch 300/7568 Train_loss 1.8354551518874311 
Epoch [3/10] Batch 400/7568 Train_loss 1.8100001460447572 
Epoch [3/10] Batch 500/7568 Train_loss 1.7951038458985007 
Epoch [3/10] Batch 600/7568 Train_loss 1.7899107714857714 
Epoch [3/10] Batch 700/7568 Train_loss 1.789383615198387 
Epoch [3/10] Batch 800/7568 Train_loss 1.7999544013733424 
Epoch [3/10] Batch 900/7568 Train_loss 1.802677792371907 
Epoch [3/10] Batch 1000/7568 Train_loss 1.794712036944829 
Epoch [3/10] Batch 1100/7568 Train_loss 1.7921076509610943 
Epoch [3/10] Batch 1200/7568 Train_loss 1.7815369351159822 
Epoch [3/10] Batch 1300/7568 Train_loss 1.7805769555929256 
Epoch [3/10] Batch 1400/7568 Train_loss 1.7827192648479464 
Epoch [3/10] Batch 1500/7568 Train_loss 1.7804883658975303 
Epoch [3/10] Batch 1600/7568 Train_loss 1.775727953591956 
Epoch [3/10] Batch 1700/7568 Train_loss 1.773740392920727 
Epoch [3/10] Batch 1800/7568 Train_loss 1.767335739472818 
Epoch [3/10] Batch 1900/7568 Train_loss 1.7704883838040084 
Epoch [3/10] Batch 2000/7568 Train_loss 1.7708646886046977 
Epoch [3/10] Batch 2100/7568 Train_loss 1.771247537486585 
Epoch [3/10] Batch 2200/7568 Train_loss 1.7750875937030133 
Epoch [3/10] Batch 2300/7568 Train_loss 1.7715040100548176 
Epoch [3/10] Batch 2400/7568 Train_loss 1.7703098444107521 
Epoch [3/10] Batch 2500/7568 Train_loss 1.7675991873534285 
Epoch [3/10] Batch 2600/7568 Train_loss 1.7651932376054835 
Epoch [3/10] Batch 2700/7568 Train_loss 1.7656538093732226 
Epoch [3/10] Batch 2800/7568 Train_loss 1.7683699521974596 
Epoch [3/10] Batch 2900/7568 Train_loss 1.7663751597015744 
Epoch [3/10] Batch 3000/7568 Train_loss 1.764847839580739 
Epoch [3/10] Batch 3100/7568 Train_loss 1.7636160477299108 
Epoch [3/10] Batch 3200/7568 Train_loss 1.763551770798194 
Epoch [3/10] Batch 3300/7568 Train_loss 1.7639617022195753 
Epoch [3/10] Batch 3400/7568 Train_loss 1.7640736039921803 
Epoch [3/10] Batch 3500/7568 Train_loss 1.7644351907127656 
Epoch [3/10] Batch 3600/7568 Train_loss 1.7636366562710408 
Epoch [3/10] Batch 3700/7568 Train_loss 1.7652127787197032 
Epoch [3/10] Batch 3800/7568 Train_loss 1.767844316221027 
Epoch [3/10] Batch 3900/7568 Train_loss 1.7645676502892524 
Epoch [3/10] Batch 4000/7568 Train_loss 1.764746974307324 
Epoch [3/10] Batch 4100/7568 Train_loss 1.764616150980358 
Epoch [3/10] Batch 4200/7568 Train_loss 1.7648344531733346 
Epoch [3/10] Batch 4300/7568 Train_loss 1.764317611048344 
Epoch [3/10] Batch 4400/7568 Train_loss 1.7641259302228345 
Epoch [3/10] Batch 4500/7568 Train_loss 1.7659616539032559 
Epoch [3/10] Batch 4600/7568 Train_loss 1.767447224630529 
Epoch [3/10] Batch 4700/7568 Train_loss 1.7665557982509021 
Epoch [3/10] Batch 4800/7568 Train_loss 1.7676724366714893 
Epoch [3/10] Batch 4900/7568 Train_loss 1.766761193845107 
Epoch [3/10] Batch 5000/7568 Train_loss 1.76559400056254 
Epoch [3/10] Batch 5100/7568 Train_loss 1.7654895076484007 
Epoch [3/10] Batch 5200/7568 Train_loss 1.764911169011108 
Epoch [3/10] Batch 5300/7568 Train_loss 1.76475343365244 
Epoch [3/10] Batch 5400/7568 Train_loss 1.7653150378987317 
Epoch [3/10] Batch 5500/7568 Train_loss 1.7651273442983866 
Epoch [3/10] Batch 5600/7568 Train_loss 1.7637768878846418 
Epoch [3/10] Batch 5700/7568 Train_loss 1.7646563095670378 
Epoch [3/10] Batch 5800/7568 Train_loss 1.7655124745003068 
Epoch [3/10] Batch 5900/7568 Train_loss 1.7659493072826142 
Epoch [3/10] Batch 6000/7568 Train_loss 1.7650025570291794 
Epoch [3/10] Batch 6100/7568 Train_loss 1.7653654121537812 
Epoch [3/10] Batch 6200/7568 Train_loss 1.7667639599516785 
Epoch [3/10] Batch 6300/7568 Train_loss 1.7684112921643307 
Epoch [3/10] Batch 6400/7568 Train_loss 1.7679925777868932 
Epoch [3/10] Batch 6500/7568 Train_loss 1.7686917129121842 
Epoch [3/10] Batch 6600/7568 Train_loss 1.7686015943655315 
Epoch [3/10] Batch 6700/7568 Train_loss 1.7688439330626755 
Epoch [3/10] Batch 6800/7568 Train_loss 1.7680723403772611 
Epoch [3/10] Batch 6900/7568 Train_loss 1.7680581728013474 
Epoch [3/10] Batch 7000/7568 Train_loss 1.7679740471235685 
Epoch [3/10] Batch 7100/7568 Train_loss 1.766652557057212 
Epoch [3/10] Batch 7200/7568 Train_loss 1.7673607086205363 
Epoch [3/10] Batch 7300/7568 Train_loss 1.766553571502071 
Epoch [3/10] Batch 7400/7568 Train_loss 1.7657668753016835 
Epoch [3/10] Batch 7500/7568 Train_loss 1.7662406910482509 
Epoch: 3/10 	Training Loss: 1.765750 	Validation Loss: 1.789619 Duration seconds: 1180.0995507240295 
Validation loss decreased (1.805658 --> 1.789619).  Saving model ... 
best_valid_loss_fold [1.7896190052005378] Best_Epoch [3]Epoch [4/10] Batch 0/7568 Train_loss 1.4727573096752167 
Epoch [4/10] Batch 100/7568 Train_loss 1.8373962130286905 
Epoch [4/10] Batch 200/7568 Train_loss 1.7254870796381538 
Epoch [4/10] Batch 300/7568 Train_loss 1.7371458783003182 
Epoch [4/10] Batch 400/7568 Train_loss 1.7405895944545393 
Epoch [4/10] Batch 500/7568 Train_loss 1.7396307183418445 
Epoch [4/10] Batch 600/7568 Train_loss 1.7489489301816001 
Epoch [4/10] Batch 700/7568 Train_loss 1.7565969031266921 
Epoch [4/10] Batch 800/7568 Train_loss 1.7497035156884295 
Epoch [4/10] Batch 900/7568 Train_loss 1.749923007271266 
Epoch [4/10] Batch 1000/7568 Train_loss 1.7483092667339566 
Epoch [4/10] Batch 1100/7568 Train_loss 1.7401723574836832 
Epoch [4/10] Batch 1200/7568 Train_loss 1.7451471119784099 
Epoch [4/10] Batch 1300/7568 Train_loss 1.7443653807433452 
Epoch [4/10] Batch 1400/7568 Train_loss 1.7471725543925032 
Epoch [4/10] Batch 1500/7568 Train_loss 1.7494199413813645 
Epoch [4/10] Batch 1600/7568 Train_loss 1.7492468662788017 
Epoch [4/10] Batch 1700/7568 Train_loss 1.7461197240949378 
Epoch [4/10] Batch 1800/7568 Train_loss 1.7499277446375563 
Epoch [4/10] Batch 1900/7568 Train_loss 1.7513391937764615 
Epoch [4/10] Batch 2000/7568 Train_loss 1.7530335129416925 
Epoch [4/10] Batch 2100/7568 Train_loss 1.7509522745257216 
Epoch [4/10] Batch 2200/7568 Train_loss 1.7499022218130058 
Epoch [4/10] Batch 2300/7568 Train_loss 1.7524961524459655 
Epoch [4/10] Batch 2400/7568 Train_loss 1.7497305754030659 
Epoch [4/10] Batch 2500/7568 Train_loss 1.74985436173748 
Epoch [4/10] Batch 2600/7568 Train_loss 1.7506507464592524 
Epoch [4/10] Batch 2700/7568 Train_loss 1.7562897797840826 
Epoch [4/10] Batch 2800/7568 Train_loss 1.7543244231405832 
Epoch [4/10] Batch 2900/7568 Train_loss 1.755112481696139 
Epoch [4/10] Batch 3000/7568 Train_loss 1.75541132282601 
Epoch [4/10] Batch 3100/7568 Train_loss 1.7552956112356233 
Epoch [4/10] Batch 3200/7568 Train_loss 1.7556414690004851 
Epoch [4/10] Batch 3300/7568 Train_loss 1.7564460772302106 
Epoch [4/10] Batch 3400/7568 Train_loss 1.7599884814193094 
Epoch [4/10] Batch 3500/7568 Train_loss 1.759385471736336 
Epoch [4/10] Batch 3600/7568 Train_loss 1.7581496411410513 
Epoch [4/10] Batch 3700/7568 Train_loss 1.7571270058696027 
Epoch [4/10] Batch 3800/7568 Train_loss 1.7572728350711508 
Epoch [4/10] Batch 3900/7568 Train_loss 1.7563339401780413 
Epoch [4/10] Batch 4000/7568 Train_loss 1.7577509572638688 
Epoch [4/10] Batch 4100/7568 Train_loss 1.7595645129567994 
Epoch [4/10] Batch 4200/7568 Train_loss 1.7607127616598304 
Epoch [4/10] Batch 4300/7568 Train_loss 1.7607544746160702 
Epoch [4/10] Batch 4400/7568 Train_loss 1.7605360095672542 
Epoch [4/10] Batch 4500/7568 Train_loss 1.7616696432034218 
Epoch [4/10] Batch 4600/7568 Train_loss 1.7639071124564745 
Epoch [4/10] Batch 4700/7568 Train_loss 1.763319500770132 
Epoch [4/10] Batch 4800/7568 Train_loss 1.7614587945516371 
Epoch [4/10] Batch 4900/7568 Train_loss 1.76130970105015 
Epoch [4/10] Batch 5000/7568 Train_loss 1.7616459229166592 
Epoch [4/10] Batch 5100/7568 Train_loss 1.7597815071825442 
Epoch [4/10] Batch 5200/7568 Train_loss 1.7586565143118063 
Epoch [4/10] Batch 5300/7568 Train_loss 1.759154714272902 
Epoch [4/10] Batch 5400/7568 Train_loss 1.757677329548629 
Epoch [4/10] Batch 5500/7568 Train_loss 1.7585918626605306 
Epoch [4/10] Batch 5600/7568 Train_loss 1.7592809739519477 
Epoch [4/10] Batch 5700/7568 Train_loss 1.7596825458000906 
Epoch [4/10] Batch 5800/7568 Train_loss 1.7600767572783351 
Epoch [4/10] Batch 5900/7568 Train_loss 1.7601703622306915 
Epoch [4/10] Batch 6000/7568 Train_loss 1.7618586902749218 
Epoch [4/10] Batch 6100/7568 Train_loss 1.7614329238528308 
Epoch [4/10] Batch 6200/7568 Train_loss 1.7621692800328166 
Epoch [4/10] Batch 6300/7568 Train_loss 1.7614891336202962 
Epoch [4/10] Batch 6400/7568 Train_loss 1.759641186098925 
Epoch [4/10] Batch 6500/7568 Train_loss 1.7597323392830597 
Epoch [4/10] Batch 6600/7568 Train_loss 1.7605448891549125 
Epoch [4/10] Batch 6700/7568 Train_loss 1.761341153198864 
Epoch [4/10] Batch 6800/7568 Train_loss 1.7611776758902846 
Epoch [4/10] Batch 6900/7568 Train_loss 1.7607565649979842 
Epoch [4/10] Batch 7000/7568 Train_loss 1.7595316256719595 
Epoch [4/10] Batch 7100/7568 Train_loss 1.7592873479113866 
Epoch [4/10] Batch 7200/7568 Train_loss 1.760098549171738 
Epoch [4/10] Batch 7300/7568 Train_loss 1.761069809192624 
Epoch [4/10] Batch 7400/7568 Train_loss 1.7615407721061849 
Epoch [4/10] Batch 7500/7568 Train_loss 1.7617142963556032 
Epoch: 4/10 	Training Loss: 1.760668 	Validation Loss: 1.807409 Duration seconds: 1287.4801018238068 
best_valid_loss_fold [1.7896190052005378] Best_Epoch [4]Epoch [5/10] Batch 0/7568 Train_loss 2.271510809659958 
Epoch [5/10] Batch 100/7568 Train_loss 1.792660495727369 
Epoch [5/10] Batch 200/7568 Train_loss 1.7965387271263114 
Epoch [5/10] Batch 300/7568 Train_loss 1.7892496847235086 
Epoch [5/10] Batch 400/7568 Train_loss 1.7809481868645793 
Epoch [5/10] Batch 500/7568 Train_loss 1.7645894293626863 
Epoch [5/10] Batch 600/7568 Train_loss 1.769999719152038 
Epoch [5/10] Batch 700/7568 Train_loss 1.7683201749807758 
Epoch [5/10] Batch 800/7568 Train_loss 1.7568535104710215 
Epoch [5/10] Batch 900/7568 Train_loss 1.7599467102119846 
Epoch [5/10] Batch 1000/7568 Train_loss 1.7556492058368591 
Epoch [5/10] Batch 1100/7568 Train_loss 1.7620184892394346 
Epoch [5/10] Batch 1200/7568 Train_loss 1.763191436053712 
Epoch [5/10] Batch 1300/7568 Train_loss 1.7588928059533777 
Epoch [5/10] Batch 1400/7568 Train_loss 1.755087005345418 
Epoch [5/10] Batch 1500/7568 Train_loss 1.759640590140694 
Epoch [5/10] Batch 1600/7568 Train_loss 1.755106759366395 
Epoch [5/10] Batch 1700/7568 Train_loss 1.758498542562679 
Epoch [5/10] Batch 1800/7568 Train_loss 1.7584793739380735 
Epoch [5/10] Batch 1900/7568 Train_loss 1.7595246454788032 
Epoch [5/10] Batch 2000/7568 Train_loss 1.7579876104819363 
Epoch [5/10] Batch 2100/7568 Train_loss 1.7590390143828243 
Epoch [5/10] Batch 2200/7568 Train_loss 1.7581676355461975 
Epoch [5/10] Batch 2300/7568 Train_loss 1.7525548489358775 
Epoch [5/10] Batch 2400/7568 Train_loss 1.7532739999150544 
Epoch [5/10] Batch 2500/7568 Train_loss 1.7540973481066415 
Epoch [5/10] Batch 2600/7568 Train_loss 1.7533804197401461 
Epoch [5/10] Batch 2700/7568 Train_loss 1.7554164028656523 
Epoch [5/10] Batch 2800/7568 Train_loss 1.7561044955612166 
Epoch [5/10] Batch 2900/7568 Train_loss 1.7561277856691382 
Epoch [5/10] Batch 3000/7568 Train_loss 1.7543113744460477 
Epoch [5/10] Batch 3100/7568 Train_loss 1.7537201282089852 
Epoch [5/10] Batch 3200/7568 Train_loss 1.7544471718643242 
Epoch [5/10] Batch 3300/7568 Train_loss 1.7557439881201153 
Epoch [5/10] Batch 3400/7568 Train_loss 1.756457406594819 
Epoch [5/10] Batch 3500/7568 Train_loss 1.7578293206593167 
Epoch [5/10] Batch 3600/7568 Train_loss 1.7574800103613153 
Epoch [5/10] Batch 3700/7568 Train_loss 1.7558370793318303 
Epoch [5/10] Batch 3800/7568 Train_loss 1.7543733741042333 
Epoch [5/10] Batch 3900/7568 Train_loss 1.7555025230899313 
Epoch [5/10] Batch 4000/7568 Train_loss 1.7560319691594766 
Epoch [5/10] Batch 4100/7568 Train_loss 1.755668647247563 
Epoch [5/10] Batch 4200/7568 Train_loss 1.7575326330305365 
Epoch [5/10] Batch 4300/7568 Train_loss 1.7575996667770308 
Epoch [5/10] Batch 4400/7568 Train_loss 1.7577441624316863 
Epoch [5/10] Batch 4500/7568 Train_loss 1.7580942725160604 
Epoch [5/10] Batch 4600/7568 Train_loss 1.7585409863393895 
Epoch [5/10] Batch 4700/7568 Train_loss 1.7577822650177533 
Epoch [5/10] Batch 4800/7568 Train_loss 1.7578875225590582 
Epoch [5/10] Batch 4900/7568 Train_loss 1.756885269703524 
Epoch [5/10] Batch 5000/7568 Train_loss 1.7564206581486033 
Epoch [5/10] Batch 5100/7568 Train_loss 1.755504155780925 
Epoch [5/10] Batch 5200/7568 Train_loss 1.7559800859217665 
Epoch [5/10] Batch 5300/7568 Train_loss 1.7557890825303877 
Epoch [5/10] Batch 5400/7568 Train_loss 1.7553177683058436 
Epoch [5/10] Batch 5500/7568 Train_loss 1.7543474350234982 
Epoch [5/10] Batch 5600/7568 Train_loss 1.7547119110609841 
Epoch [5/10] Batch 5700/7568 Train_loss 1.7542842820213564 
Epoch [5/10] Batch 5800/7568 Train_loss 1.7557159059328922 
Epoch [5/10] Batch 5900/7568 Train_loss 1.7568021250670591 
Epoch [5/10] Batch 6000/7568 Train_loss 1.7573016767285006 
Epoch [5/10] Batch 6100/7568 Train_loss 1.757638064890714 
Epoch [5/10] Batch 6200/7568 Train_loss 1.7572629552017949 
Epoch [5/10] Batch 6300/7568 Train_loss 1.757218558994565 
Epoch [5/10] Batch 6400/7568 Train_loss 1.7577681261472842 
Epoch [5/10] Batch 6500/7568 Train_loss 1.757931419053851 
Epoch [5/10] Batch 6600/7568 Train_loss 1.7587216580341647 
Epoch [5/10] Batch 6700/7568 Train_loss 1.7581683698774202 
Epoch [5/10] Batch 6800/7568 Train_loss 1.7597401628692244 
Epoch [5/10] Batch 6900/7568 Train_loss 1.759496282577221 
Epoch [5/10] Batch 7000/7568 Train_loss 1.7595103263446321 
Epoch [5/10] Batch 7100/7568 Train_loss 1.7599707820468882 
Epoch [5/10] Batch 7200/7568 Train_loss 1.7600271066242317 
Epoch [5/10] Batch 7300/7568 Train_loss 1.7596807853795702 
Epoch [5/10] Batch 7400/7568 Train_loss 1.7580374303101334 
Epoch [5/10] Batch 7500/7568 Train_loss 1.7578058816376343 
Epoch: 5/10 	Training Loss: 1.759326 	Validation Loss: 1.822463 Duration seconds: 1240.5169396400452 
best_valid_loss_fold [1.7896190052005378] Best_Epoch [5]Epoch [6/10] Batch 0/7568 Train_loss 1.9097979366779327 
Epoch [6/10] Batch 100/7568 Train_loss 1.7576553794269514 
Epoch [6/10] Batch 200/7568 Train_loss 1.7958228910962741 
Epoch [6/10] Batch 300/7568 Train_loss 1.8091210330592042 
Epoch [6/10] Batch 400/7568 Train_loss 1.779112808889433 
Epoch [6/10] Batch 500/7568 Train_loss 1.781390911849316 
Epoch [6/10] Batch 600/7568 Train_loss 1.7792929155432842 
Epoch [6/10] Batch 700/7568 Train_loss 1.7750977041909255 
Epoch [6/10] Batch 800/7568 Train_loss 1.7689867827869235 
Epoch [6/10] Batch 900/7568 Train_loss 1.7536257035807024 
Epoch [6/10] Batch 1000/7568 Train_loss 1.7489689395769492 
Epoch [6/10] Batch 1100/7568 Train_loss 1.7493467289382603 
Epoch [6/10] Batch 1200/7568 Train_loss 1.7446098690674168 
Epoch [6/10] Batch 1300/7568 Train_loss 1.7454410594623881 
Epoch [6/10] Batch 1400/7568 Train_loss 1.7423935181898018 
Epoch [6/10] Batch 1500/7568 Train_loss 1.7385538255449615 
Epoch [6/10] Batch 1600/7568 Train_loss 1.7440346248219865 
Epoch [6/10] Batch 1700/7568 Train_loss 1.7447123962122157 
Epoch [6/10] Batch 1800/7568 Train_loss 1.746325712984036 
Epoch [6/10] Batch 1900/7568 Train_loss 1.7457771572484775 
Epoch [6/10] Batch 2000/7568 Train_loss 1.748296440667894 
Epoch [6/10] Batch 2100/7568 Train_loss 1.748095765951019 
Epoch [6/10] Batch 2200/7568 Train_loss 1.7509341447435147 
Epoch [6/10] Batch 2300/7568 Train_loss 1.7506966705570424 
Epoch [6/10] Batch 2400/7568 Train_loss 1.74960840141237 
Epoch [6/10] Batch 2500/7568 Train_loss 1.747606435253686 
Epoch [6/10] Batch 2600/7568 Train_loss 1.7473448455184755 
Epoch [6/10] Batch 2700/7568 Train_loss 1.746566923944123 
Epoch [6/10] Batch 2800/7568 Train_loss 1.7464772516097482 
Epoch [6/10] Batch 2900/7568 Train_loss 1.7465478627949245 
Epoch [6/10] Batch 3000/7568 Train_loss 1.7471317475322126 
Epoch [6/10] Batch 3100/7568 Train_loss 1.7462813438561147 
Epoch [6/10] Batch 3200/7568 Train_loss 1.7461740608673026 
Epoch [6/10] Batch 3300/7568 Train_loss 1.7443137541435878 
Epoch [6/10] Batch 3400/7568 Train_loss 1.7448085363350767 
Epoch [6/10] Batch 3500/7568 Train_loss 1.7443377559302842 
Epoch [6/10] Batch 3600/7568 Train_loss 1.745865925860981 
Epoch [6/10] Batch 3700/7568 Train_loss 1.747272539540614 
Epoch [6/10] Batch 3800/7568 Train_loss 1.745515705111528 
Epoch [6/10] Batch 3900/7568 Train_loss 1.7454829465782176 
Epoch [6/10] Batch 4000/7568 Train_loss 1.7455437447865496 
Epoch [6/10] Batch 4100/7568 Train_loss 1.7460578604006558 
Epoch [6/10] Batch 4200/7568 Train_loss 1.7489481037821806 
Epoch [6/10] Batch 4300/7568 Train_loss 1.7501010427282206 
Epoch [6/10] Batch 4400/7568 Train_loss 1.747636031541424 
Epoch [6/10] Batch 4500/7568 Train_loss 1.7468261301196355 
Epoch [6/10] Batch 4600/7568 Train_loss 1.7479032823697325 
Epoch [6/10] Batch 4700/7568 Train_loss 1.748693973005097 
Epoch [6/10] Batch 4800/7568 Train_loss 1.7499495869771289 
Epoch [6/10] Batch 4900/7568 Train_loss 1.7494695618067608 
Epoch [6/10] Batch 5000/7568 Train_loss 1.7493846454359465 
Epoch [6/10] Batch 5100/7568 Train_loss 1.7495147399480169 
Epoch [6/10] Batch 5200/7568 Train_loss 1.7486758441108279 
Epoch [6/10] Batch 5300/7568 Train_loss 1.7476800437668791 
Epoch [6/10] Batch 5400/7568 Train_loss 1.747523977244823 
Epoch [6/10] Batch 5500/7568 Train_loss 1.7481096717668672 
Epoch [6/10] Batch 5600/7568 Train_loss 1.7468813595865331 
Epoch [6/10] Batch 5700/7568 Train_loss 1.7475531583299366 
Epoch [6/10] Batch 5800/7568 Train_loss 1.7480645132188632 
Epoch [6/10] Batch 5900/7568 Train_loss 1.7485641732488704 
Epoch [6/10] Batch 6000/7568 Train_loss 1.7484404990308942 
Epoch [6/10] Batch 6100/7568 Train_loss 1.7472977516172832 
Epoch [6/10] Batch 6200/7568 Train_loss 1.7473456988174518 
Epoch [6/10] Batch 6300/7568 Train_loss 1.7470588834701768 
Epoch [6/10] Batch 6400/7568 Train_loss 1.7471954882158895 
Epoch [6/10] Batch 6500/7568 Train_loss 1.7479090610571484 
Epoch [6/10] Batch 6600/7568 Train_loss 1.7486782965245093 
Epoch [6/10] Batch 6700/7568 Train_loss 1.7481326868535512 
Epoch [6/10] Batch 6800/7568 Train_loss 1.748030793511715 
Epoch [6/10] Batch 6900/7568 Train_loss 1.7468482077363574 
Epoch [6/10] Batch 7000/7568 Train_loss 1.7460853551655322 
Epoch [6/10] Batch 7100/7568 Train_loss 1.7463188634264923 
Epoch [6/10] Batch 7200/7568 Train_loss 1.7455610556745509 
Epoch [6/10] Batch 7300/7568 Train_loss 1.7466600811056137 
Epoch [6/10] Batch 7400/7568 Train_loss 1.7462259326157836 
Epoch [6/10] Batch 7500/7568 Train_loss 1.747302121628429 
Epoch: 6/10 	Training Loss: 1.747591 	Validation Loss: 1.922947 Duration seconds: 1216.3236150741577 
best_valid_loss_fold [1.7896190052005378] Best_Epoch [6]Epoch [7/10] Batch 0/7568 Train_loss 1.63801908493042 
Epoch [7/10] Batch 100/7568 Train_loss 1.799480704461584 
Epoch [7/10] Batch 200/7568 Train_loss 1.7299087325212967 
Epoch [7/10] Batch 300/7568 Train_loss 1.7493865682288658 
Epoch [7/10] Batch 400/7568 Train_loss 1.7426203490418388 
Epoch [7/10] Batch 500/7568 Train_loss 1.7363825957932157 
Epoch [7/10] Batch 600/7568 Train_loss 1.7296922946977933 
Epoch [7/10] Batch 700/7568 Train_loss 1.7258488655770556 
Epoch [7/10] Batch 800/7568 Train_loss 1.7378489327118192 
Epoch [7/10] Batch 900/7568 Train_loss 1.737411636755813 
Epoch [7/10] Batch 1000/7568 Train_loss 1.742124965006893 
Epoch [7/10] Batch 1100/7568 Train_loss 1.7375137249885853 
Epoch [7/10] Batch 1200/7568 Train_loss 1.7418242064455864 
Epoch [7/10] Batch 1300/7568 Train_loss 1.7430659312809273 
Epoch [7/10] Batch 1400/7568 Train_loss 1.733889201128168 
Epoch [7/10] Batch 1500/7568 Train_loss 1.7320340392809561 
Epoch [7/10] Batch 1600/7568 Train_loss 1.7322110376791384 
Epoch [7/10] Batch 1700/7568 Train_loss 1.7298741206342791 
Epoch [7/10] Batch 1800/7568 Train_loss 1.7302034832102797 
Epoch [7/10] Batch 1900/7568 Train_loss 1.7318561794662526 
Epoch [7/10] Batch 2000/7568 Train_loss 1.7320107860394802 
Epoch [7/10] Batch 2100/7568 Train_loss 1.7310050784520228 
Epoch [7/10] Batch 2200/7568 Train_loss 1.7370173378404516 
Epoch [7/10] Batch 2300/7568 Train_loss 1.7373756646967515 
Epoch [7/10] Batch 2400/7568 Train_loss 1.737456083986621 
Epoch [7/10] Batch 2500/7568 Train_loss 1.7414008888279329 
Epoch [7/10] Batch 2600/7568 Train_loss 1.7416042161647836 
Epoch [7/10] Batch 2700/7568 Train_loss 1.7404265153390397 
Epoch [7/10] Batch 2800/7568 Train_loss 1.7408172795013717 
Epoch [7/10] Batch 2900/7568 Train_loss 1.7413812693913449 
Epoch [7/10] Batch 3000/7568 Train_loss 1.7420162735472755 
Epoch [7/10] Batch 3100/7568 Train_loss 1.7401801651570152 
Epoch [7/10] Batch 3200/7568 Train_loss 1.7418280781311604 
Epoch [7/10] Batch 3300/7568 Train_loss 1.7423613299270069 
Epoch [7/10] Batch 3400/7568 Train_loss 1.7441432524151537 
Epoch [7/10] Batch 3500/7568 Train_loss 1.7451668743515518 
Epoch [7/10] Batch 3600/7568 Train_loss 1.7445213435814135 
Epoch [7/10] Batch 3700/7568 Train_loss 1.7445531735884696 
Epoch [7/10] Batch 3800/7568 Train_loss 1.742368315010126 
Epoch [7/10] Batch 3900/7568 Train_loss 1.741373011987046 
Epoch [7/10] Batch 4000/7568 Train_loss 1.7423624640493147 
Epoch [7/10] Batch 4100/7568 Train_loss 1.7415860933073408 
Epoch [7/10] Batch 4200/7568 Train_loss 1.7440428293512509 
Epoch [7/10] Batch 4300/7568 Train_loss 1.7446044833767251 
Epoch [7/10] Batch 4400/7568 Train_loss 1.745268762106585 
Epoch [7/10] Batch 4500/7568 Train_loss 1.7470111178887655 
Epoch [7/10] Batch 4600/7568 Train_loss 1.7459713159482444 
Epoch [7/10] Batch 4700/7568 Train_loss 1.7471159989055078 
Epoch [7/10] Batch 4800/7568 Train_loss 1.7471549652673581 
Epoch [7/10] Batch 4900/7568 Train_loss 1.7455566063799972 
Epoch [7/10] Batch 5000/7568 Train_loss 1.745656956135452 
Epoch [7/10] Batch 5100/7568 Train_loss 1.7469728386792405 
Epoch [7/10] Batch 5200/7568 Train_loss 1.7478935006655676 
Epoch [7/10] Batch 5300/7568 Train_loss 1.748432643280077 
Epoch [7/10] Batch 5400/7568 Train_loss 1.7480612862934517 
Epoch [7/10] Batch 5500/7568 Train_loss 1.7478722392282926 
Epoch [7/10] Batch 5600/7568 Train_loss 1.7490048641882674 
Epoch [7/10] Batch 5700/7568 Train_loss 1.7488640619396452 
Epoch [7/10] Batch 5800/7568 Train_loss 1.7488601442039209 
Epoch [7/10] Batch 5900/7568 Train_loss 1.7491582918762554 
Epoch [7/10] Batch 6000/7568 Train_loss 1.7496267972159258 
Epoch [7/10] Batch 6100/7568 Train_loss 1.748189355398305 
Epoch [7/10] Batch 6200/7568 Train_loss 1.7486062224121117 
Epoch [7/10] Batch 6300/7568 Train_loss 1.747204999684332 
Epoch [7/10] Batch 6400/7568 Train_loss 1.7480579856361211 
Epoch [7/10] Batch 6500/7568 Train_loss 1.7484683872192956 
Epoch [7/10] Batch 6600/7568 Train_loss 1.7477211997098063 
Epoch [7/10] Batch 6700/7568 Train_loss 1.7462606783836765 
Epoch [7/10] Batch 6800/7568 Train_loss 1.7460275810212444 
Epoch [7/10] Batch 6900/7568 Train_loss 1.745530224304418 
Epoch [7/10] Batch 7000/7568 Train_loss 1.7455243417127713 
Epoch [7/10] Batch 7100/7568 Train_loss 1.7457566903283723 
Epoch [7/10] Batch 7200/7568 Train_loss 1.7454466077984994 
Epoch [7/10] Batch 7300/7568 Train_loss 1.745905605439592 
Epoch [7/10] Batch 7400/7568 Train_loss 1.7469930382884125 
Epoch [7/10] Batch 7500/7568 Train_loss 1.7470744083297918 
Epoch: 7/10 	Training Loss: 1.747708 	Validation Loss: 1.987326 Duration seconds: 1232.1333961486816 
best_valid_loss_fold [1.7896190052005378] Best_Epoch [7]Epoch [8/10] Batch 0/7568 Train_loss 1.6245809197425842 
Epoch [8/10] Batch 100/7568 Train_loss 1.717528908854664 
Epoch [8/10] Batch 200/7568 Train_loss 1.7001055166199432 
Epoch [8/10] Batch 300/7568 Train_loss 1.6789348701949531 
Epoch [8/10] Batch 400/7568 Train_loss 1.6820681363604313 
Epoch [8/10] Batch 500/7568 Train_loss 1.687231659740507 
Epoch [8/10] Batch 600/7568 Train_loss 1.679770491161878 
Epoch [8/10] Batch 700/7568 Train_loss 1.6905959176906675 
Epoch [8/10] Batch 800/7568 Train_loss 1.6940919880972671 
Epoch [8/10] Batch 900/7568 Train_loss 1.698166526663846 
Epoch [8/10] Batch 1000/7568 Train_loss 1.6987452075465934 
Epoch [8/10] Batch 1100/7568 Train_loss 1.6967523321306348 
Epoch [8/10] Batch 1200/7568 Train_loss 1.6968489651700838 
Epoch [8/10] Batch 1300/7568 Train_loss 1.7002301662873984 
Epoch [8/10] Batch 1400/7568 Train_loss 1.7124479450418386 
Epoch [8/10] Batch 1500/7568 Train_loss 1.715070998679591 
Epoch [8/10] Batch 1600/7568 Train_loss 1.7185644429644744 
Epoch [8/10] Batch 1700/7568 Train_loss 1.7214371438966576 
Epoch [8/10] Batch 1800/7568 Train_loss 1.7229999236344695 
Epoch [8/10] Batch 1900/7568 Train_loss 1.722800640868457 
Epoch [8/10] Batch 2000/7568 Train_loss 1.7277139174504794 
Epoch [8/10] Batch 2100/7568 Train_loss 1.7323599454868357 
Epoch [8/10] Batch 2200/7568 Train_loss 1.7291428342035065 
Epoch [8/10] Batch 2300/7568 Train_loss 1.728627144730205 
Epoch [8/10] Batch 2400/7568 Train_loss 1.727885558087222 
Epoch [8/10] Batch 2500/7568 Train_loss 1.7302547083174835 
Epoch [8/10] Batch 2600/7568 Train_loss 1.7328628057936941 
Epoch [8/10] Batch 2700/7568 Train_loss 1.7397544340888071 
Epoch [8/10] Batch 2800/7568 Train_loss 1.741586088035648 
Epoch [8/10] Batch 2900/7568 Train_loss 1.7418222272269728 
Epoch [8/10] Batch 3000/7568 Train_loss 1.743534177862001 
Epoch [8/10] Batch 3100/7568 Train_loss 1.744001970903618 
Epoch [8/10] Batch 3200/7568 Train_loss 1.742087955511257 
Epoch [8/10] Batch 3300/7568 Train_loss 1.7420799252107373 
Epoch [8/10] Batch 3400/7568 Train_loss 1.742245943653289 
Epoch [8/10] Batch 3500/7568 Train_loss 1.742563853144084 
Epoch [8/10] Batch 3600/7568 Train_loss 1.741318794035127 
Epoch [8/10] Batch 3700/7568 Train_loss 1.739714199300152 
Epoch [8/10] Batch 3800/7568 Train_loss 1.7399311840428362 
Epoch [8/10] Batch 3900/7568 Train_loss 1.739614569815771 
Epoch [8/10] Batch 4000/7568 Train_loss 1.7397936502733669 
Epoch [8/10] Batch 4100/7568 Train_loss 1.7395094125805266 
Epoch [8/10] Batch 4200/7568 Train_loss 1.7397078742969039 
Epoch [8/10] Batch 4300/7568 Train_loss 1.7390054786892664 
Epoch [8/10] Batch 4400/7568 Train_loss 1.7396077115687152 
Epoch [8/10] Batch 4500/7568 Train_loss 1.7409059755095269 
Epoch [8/10] Batch 4600/7568 Train_loss 1.7407248302617297 
Epoch [8/10] Batch 4700/7568 Train_loss 1.7422348876293354 
Epoch [8/10] Batch 4800/7568 Train_loss 1.7430122730366218 
Epoch [8/10] Batch 4900/7568 Train_loss 1.74573522872145 
Epoch [8/10] Batch 5000/7568 Train_loss 1.7468435288226454 
Epoch [8/10] Batch 5100/7568 Train_loss 1.7470069612283095 
Epoch [8/10] Batch 5200/7568 Train_loss 1.748840090498354 
Epoch [8/10] Batch 5300/7568 Train_loss 1.7487198384673701 
Epoch [8/10] Batch 5400/7568 Train_loss 1.7488768334533082 
Epoch [8/10] Batch 5500/7568 Train_loss 1.7479579403950418 
Epoch [8/10] Batch 5600/7568 Train_loss 1.7482088694847433 
Epoch [8/10] Batch 5700/7568 Train_loss 1.747139720182485 
Epoch [8/10] Batch 5800/7568 Train_loss 1.7466602696517155 
Epoch [8/10] Batch 5900/7568 Train_loss 1.747371580081155 
Epoch [8/10] Batch 6000/7568 Train_loss 1.7480710768596348 
Epoch [8/10] Batch 6100/7568 Train_loss 1.7472372781024315 
Epoch [8/10] Batch 6200/7568 Train_loss 1.7471480341152095 
Epoch [8/10] Batch 6300/7568 Train_loss 1.747726913883873 
Epoch [8/10] Batch 6400/7568 Train_loss 1.747880282852123 
Epoch [8/10] Batch 6500/7568 Train_loss 1.7481431189378378 
Epoch [8/10] Batch 6600/7568 Train_loss 1.7481966652207492 
Epoch [8/10] Batch 6700/7568 Train_loss 1.747451577663039 
Epoch [8/10] Batch 6800/7568 Train_loss 1.7480808988743768 
Epoch [8/10] Batch 6900/7568 Train_loss 1.7479620849150381 
Epoch [8/10] Batch 7000/7568 Train_loss 1.7485660941011054 
Epoch [8/10] Batch 7100/7568 Train_loss 1.7491548959836793 
Epoch [8/10] Batch 7200/7568 Train_loss 1.748563559670785 
Epoch [8/10] Batch 7300/7568 Train_loss 1.7485344118644417 
Epoch [8/10] Batch 7400/7568 Train_loss 1.7480558397618602 
Epoch [8/10] Batch 7500/7568 Train_loss 1.7471681000611938 
Epoch: 8/10 	Training Loss: 1.746921 	Validation Loss: 1.780629 Duration seconds: 1271.973269701004 
Validation loss decreased (1.789619 --> 1.780629).  Saving model ... 
best_valid_loss_fold [1.780628668609022] Best_Epoch [8]Epoch [9/10] Batch 0/7568 Train_loss 1.2322542816400528 
Epoch [9/10] Batch 100/7568 Train_loss 1.6727721930877997 
Epoch [9/10] Batch 200/7568 Train_loss 1.6968828754786829 
Epoch [9/10] Batch 300/7568 Train_loss 1.7118545787477017 
Epoch [9/10] Batch 400/7568 Train_loss 1.7215330094247685 
Epoch [9/10] Batch 500/7568 Train_loss 1.7279250794898964 
Epoch [9/10] Batch 600/7568 Train_loss 1.7314066537059285 
Epoch [9/10] Batch 700/7568 Train_loss 1.7371361442282265 
Epoch [9/10] Batch 800/7568 Train_loss 1.7339726620715359 
Epoch [9/10] Batch 900/7568 Train_loss 1.727547675090016 
Epoch [9/10] Batch 1000/7568 Train_loss 1.7272849302817057 
Epoch [9/10] Batch 1100/7568 Train_loss 1.7246192628069426 
Epoch [9/10] Batch 1200/7568 Train_loss 1.7315839075302701 
Epoch [9/10] Batch 1300/7568 Train_loss 1.7340964619514119 
Epoch [9/10] Batch 1400/7568 Train_loss 1.730866127678703 
Epoch [9/10] Batch 1500/7568 Train_loss 1.7346697810945473 
Epoch [9/10] Batch 1600/7568 Train_loss 1.733347806127871 
Epoch [9/10] Batch 1700/7568 Train_loss 1.7313662601710418 
Epoch [9/10] Batch 1800/7568 Train_loss 1.7359882093622643 
Epoch [9/10] Batch 1900/7568 Train_loss 1.734877833111828 
Epoch [9/10] Batch 2000/7568 Train_loss 1.7352722433843892 
Epoch [9/10] Batch 2100/7568 Train_loss 1.7376468197461652 
Epoch [9/10] Batch 2200/7568 Train_loss 1.7371132232753161 
Epoch [9/10] Batch 2300/7568 Train_loss 1.7361200555368954 
Epoch [9/10] Batch 2400/7568 Train_loss 1.7335188251201286 
Epoch [9/10] Batch 2500/7568 Train_loss 1.736221375395922 
Epoch [9/10] Batch 2600/7568 Train_loss 1.7339365167263288 
Epoch [9/10] Batch 2700/7568 Train_loss 1.7341435357641397 
Epoch [9/10] Batch 2800/7568 Train_loss 1.7339650730534946 
Epoch [9/10] Batch 2900/7568 Train_loss 1.73508395454681 
Epoch [9/10] Batch 3000/7568 Train_loss 1.7364922253137547 
Epoch [9/10] Batch 3100/7568 Train_loss 1.7351621050977468 
Epoch [9/10] Batch 3200/7568 Train_loss 1.7379088482072635 
Epoch [9/10] Batch 3300/7568 Train_loss 1.7363109643086092 
Epoch [9/10] Batch 3400/7568 Train_loss 1.7375743399535477 
Epoch [9/10] Batch 3500/7568 Train_loss 1.7352847448611082 
Epoch [9/10] Batch 3600/7568 Train_loss 1.7341273166891007 
Epoch [9/10] Batch 3700/7568 Train_loss 1.7341644031047756 
Epoch [9/10] Batch 3800/7568 Train_loss 1.7372912243185028 
Epoch [9/10] Batch 3900/7568 Train_loss 1.737591793554864 
Epoch [9/10] Batch 4000/7568 Train_loss 1.7380535540372781 
Epoch [9/10] Batch 4100/7568 Train_loss 1.7386874297833157 
Epoch [9/10] Batch 4200/7568 Train_loss 1.7368361816409654 
Epoch [9/10] Batch 4300/7568 Train_loss 1.736179171946403 
Epoch [9/10] Batch 4400/7568 Train_loss 1.7345120896976987 
Epoch [9/10] Batch 4500/7568 Train_loss 1.7335868269615613 
Epoch [9/10] Batch 4600/7568 Train_loss 1.7340675610753407 
Epoch [9/10] Batch 4700/7568 Train_loss 1.7340747995107535 
Epoch [9/10] Batch 4800/7568 Train_loss 1.7333283000838084 
Epoch [9/10] Batch 4900/7568 Train_loss 1.7342190242888758 
Epoch [9/10] Batch 5000/7568 Train_loss 1.7344001845088488 
Epoch [9/10] Batch 5100/7568 Train_loss 1.7337171500265727 
Epoch [9/10] Batch 5200/7568 Train_loss 1.7337850391452208 
Epoch [9/10] Batch 5300/7568 Train_loss 1.7344941880495632 
Epoch [9/10] Batch 5400/7568 Train_loss 1.733932332959115 
Epoch [9/10] Batch 5500/7568 Train_loss 1.7337919715734378 
Epoch [9/10] Batch 5600/7568 Train_loss 1.7347621614832769 
Epoch [9/10] Batch 5700/7568 Train_loss 1.7349587998326317 
Epoch [9/10] Batch 5800/7568 Train_loss 1.7354633257183467 
Epoch [9/10] Batch 5900/7568 Train_loss 1.736477522611608 
Epoch [9/10] Batch 6000/7568 Train_loss 1.7373810081474683 
Epoch [9/10] Batch 6100/7568 Train_loss 1.7379820338467011 
Epoch [9/10] Batch 6200/7568 Train_loss 1.7369159068230409 
Epoch [9/10] Batch 6300/7568 Train_loss 1.7361882883908162 
Epoch [9/10] Batch 6400/7568 Train_loss 1.7354905019838573 
Epoch [9/10] Batch 6500/7568 Train_loss 1.735984470177808 
Epoch [9/10] Batch 6600/7568 Train_loss 1.7362058127755795 
Epoch [9/10] Batch 6700/7568 Train_loss 1.735774270507484 
Epoch [9/10] Batch 6800/7568 Train_loss 1.736286327438948 
Epoch [9/10] Batch 6900/7568 Train_loss 1.737049652395508 
Epoch [9/10] Batch 7000/7568 Train_loss 1.7367738820830596 
Epoch [9/10] Batch 7100/7568 Train_loss 1.7356578211923448 
Epoch [9/10] Batch 7200/7568 Train_loss 1.735581163269888 
Epoch [9/10] Batch 7300/7568 Train_loss 1.7358341007871654 
Epoch [9/10] Batch 7400/7568 Train_loss 1.7352555387820396 
Epoch [9/10] Batch 7500/7568 Train_loss 1.7354847550632524 
Epoch: 9/10 	Training Loss: 1.735104 	Validation Loss: 1.763588 Duration seconds: 1217.0624506473541 
Validation loss decreased (1.780629 --> 1.763588).  Saving model ... 
best_valid_loss_fold [1.7635880091619416] Best_Epoch [9]Fold: 5/5 
Epoch [0/10] Batch 0/7568 Train_loss 1.7552478611469269 
Epoch [0/10] Batch 100/7568 Train_loss 1.9276722992705826 
Epoch [0/10] Batch 200/7568 Train_loss 1.869382897938662 
Epoch [0/10] Batch 300/7568 Train_loss 1.8430978626606869 
Epoch [0/10] Batch 400/7568 Train_loss 1.811773131353005 
Epoch [0/10] Batch 500/7568 Train_loss 1.8056346964604126 
Epoch [0/10] Batch 600/7568 Train_loss 1.801721820183185 
Epoch [0/10] Batch 700/7568 Train_loss 1.802207728563549 
Epoch [0/10] Batch 800/7568 Train_loss 1.7949248809883658 
Epoch [0/10] Batch 900/7568 Train_loss 1.7983128907330557 
Epoch [0/10] Batch 1000/7568 Train_loss 1.8025521919518321 
Epoch [0/10] Batch 1100/7568 Train_loss 1.795932706699222 
Epoch [0/10] Batch 1200/7568 Train_loss 1.7939900560653477 
Epoch [0/10] Batch 1300/7568 Train_loss 1.7858577193513363 
Epoch [0/10] Batch 1400/7568 Train_loss 1.7905659132838505 
Epoch [0/10] Batch 1500/7568 Train_loss 1.790179393932392 
Epoch [0/10] Batch 1600/7568 Train_loss 1.788781758707289 
Epoch [0/10] Batch 1700/7568 Train_loss 1.7909549727711798 
Epoch [0/10] Batch 1800/7568 Train_loss 1.7897614369014778 
Epoch [0/10] Batch 1900/7568 Train_loss 1.789375513513768 
Epoch [0/10] Batch 2000/7568 Train_loss 1.7881540195836776 
Epoch [0/10] Batch 2100/7568 Train_loss 1.7884515732462947 
Epoch [0/10] Batch 2200/7568 Train_loss 1.7901658848848792 
Epoch [0/10] Batch 2300/7568 Train_loss 1.7879138903927927 
Epoch [0/10] Batch 2400/7568 Train_loss 1.7869539765399827 
Epoch [0/10] Batch 2500/7568 Train_loss 1.788133886225316 
Epoch [0/10] Batch 2600/7568 Train_loss 1.7867833313771002 
Epoch [0/10] Batch 2700/7568 Train_loss 1.7866939680443257 
Epoch [0/10] Batch 2800/7568 Train_loss 1.7886408117973178 
Epoch [0/10] Batch 2900/7568 Train_loss 1.787580513542724 
Epoch [0/10] Batch 3000/7568 Train_loss 1.786699283064921 
Epoch [0/10] Batch 3100/7568 Train_loss 1.7874099929648537 
Epoch [0/10] Batch 3200/7568 Train_loss 1.7871066924883998 
Epoch [0/10] Batch 3300/7568 Train_loss 1.7866333752590684 
Epoch [0/10] Batch 3400/7568 Train_loss 1.784122054654337 
Epoch [0/10] Batch 3500/7568 Train_loss 1.7830841086211118 
Epoch [0/10] Batch 3600/7568 Train_loss 1.7823100060025907 
Epoch [0/10] Batch 3700/7568 Train_loss 1.7810280076098488 
Epoch [0/10] Batch 3800/7568 Train_loss 1.7816167513055101 
Epoch [0/10] Batch 3900/7568 Train_loss 1.7797666337265627 
Epoch [0/10] Batch 4000/7568 Train_loss 1.7810473603472743 
Epoch [0/10] Batch 4100/7568 Train_loss 1.7792949146506873 
Epoch [0/10] Batch 4200/7568 Train_loss 1.7797528762326897 
Epoch [0/10] Batch 4300/7568 Train_loss 1.7798815902267964 
Epoch [0/10] Batch 4400/7568 Train_loss 1.7800732969414967 
Epoch [0/10] Batch 4500/7568 Train_loss 1.779869666748896 
Epoch [0/10] Batch 4600/7568 Train_loss 1.7781117854575519 
Epoch [0/10] Batch 4700/7568 Train_loss 1.7772053525650584 
Epoch [0/10] Batch 4800/7568 Train_loss 1.7781992539782072 
Epoch [0/10] Batch 4900/7568 Train_loss 1.7787509705963316 
Epoch [0/10] Batch 5000/7568 Train_loss 1.7778017382098183 
Epoch [0/10] Batch 5100/7568 Train_loss 1.777529473438599 
Epoch [0/10] Batch 5200/7568 Train_loss 1.7772440468776778 
Epoch [0/10] Batch 5300/7568 Train_loss 1.7757520508010576 
Epoch [0/10] Batch 5400/7568 Train_loss 1.776192879479829 
Epoch [0/10] Batch 5500/7568 Train_loss 1.7783507911958731 
Epoch [0/10] Batch 5600/7568 Train_loss 1.777431227185645 
Epoch [0/10] Batch 5700/7568 Train_loss 1.7786439484152914 
Epoch [0/10] Batch 5800/7568 Train_loss 1.7804449428064908 
Epoch [0/10] Batch 5900/7568 Train_loss 1.7804718987252062 
Epoch [0/10] Batch 6000/7568 Train_loss 1.77974765840654 
Epoch [0/10] Batch 6100/7568 Train_loss 1.7794836027319656 
Epoch [0/10] Batch 6200/7568 Train_loss 1.7780150074756274 
Epoch [0/10] Batch 6300/7568 Train_loss 1.7774042242319408 
Epoch [0/10] Batch 6400/7568 Train_loss 1.7773886048056766 
Epoch [0/10] Batch 6500/7568 Train_loss 1.7767840872036367 
Epoch [0/10] Batch 6600/7568 Train_loss 1.7774826627342633 
Epoch [0/10] Batch 6700/7568 Train_loss 1.7767708800669713 
Epoch [0/10] Batch 6800/7568 Train_loss 1.7765922146294595 
Epoch [0/10] Batch 6900/7568 Train_loss 1.7760218357039248 
Epoch [0/10] Batch 7000/7568 Train_loss 1.775665173296153 
Epoch [0/10] Batch 7100/7568 Train_loss 1.7760429037830607 
Epoch [0/10] Batch 7200/7568 Train_loss 1.7762017470391702 
Epoch [0/10] Batch 7300/7568 Train_loss 1.776496777182131 
Epoch [0/10] Batch 7400/7568 Train_loss 1.7775988365074151 
Epoch [0/10] Batch 7500/7568 Train_loss 1.7777703748438585 
Epoch: 0/10 	Training Loss: 1.778429 	Validation Loss: 1.773605 Duration seconds: 1104.4899682998657 
Validation loss decreased (inf --> 1.773605).  Saving model ... 
best_valid_loss_fold [1.7736050108364974] Best_Epoch [0]Epoch [1/10] Batch 0/7568 Train_loss 2.157316356897354 
Epoch [1/10] Batch 100/7568 Train_loss 1.789310995009866 
Epoch [1/10] Batch 200/7568 Train_loss 1.7676171597408419 
Epoch [1/10] Batch 300/7568 Train_loss 1.785433978040353 
Epoch [1/10] Batch 400/7568 Train_loss 1.7800212534437156 
Epoch [1/10] Batch 500/7568 Train_loss 1.787368847343498 
Epoch [1/10] Batch 600/7568 Train_loss 1.7812210773296047 
Epoch [1/10] Batch 700/7568 Train_loss 1.7774502323480714 
Epoch [1/10] Batch 800/7568 Train_loss 1.786780238067836 
Epoch [1/10] Batch 900/7568 Train_loss 1.7903412950042217 
Epoch [1/10] Batch 1000/7568 Train_loss 1.7904822582325022 
Epoch [1/10] Batch 1100/7568 Train_loss 1.7791801330707053 
Epoch [1/10] Batch 1200/7568 Train_loss 1.7790202759547495 
Epoch [1/10] Batch 1300/7568 Train_loss 1.7749333182646163 
Epoch [1/10] Batch 1400/7568 Train_loss 1.780488666314512 
Epoch [1/10] Batch 1500/7568 Train_loss 1.7819004726014798 
Epoch [1/10] Batch 1600/7568 Train_loss 1.7771269307285082 
Epoch [1/10] Batch 1700/7568 Train_loss 1.779698450435862 
Epoch [1/10] Batch 1800/7568 Train_loss 1.775106069161818 
Epoch [1/10] Batch 1900/7568 Train_loss 1.7792459405656866 
Epoch [1/10] Batch 2000/7568 Train_loss 1.7801857584419993 
Epoch [1/10] Batch 2100/7568 Train_loss 1.7829229455726707 
Epoch [1/10] Batch 2200/7568 Train_loss 1.778591978162703 
Epoch [1/10] Batch 2300/7568 Train_loss 1.7813248650657774 
Epoch [1/10] Batch 2400/7568 Train_loss 1.7792087492037694 
Epoch [1/10] Batch 2500/7568 Train_loss 1.7800640344607834 
Epoch [1/10] Batch 2600/7568 Train_loss 1.780994968808921 
Epoch [1/10] Batch 2700/7568 Train_loss 1.7792653549781159 
Epoch [1/10] Batch 2800/7568 Train_loss 1.7786187597712377 
Epoch [1/10] Batch 2900/7568 Train_loss 1.778094832679807 
Epoch [1/10] Batch 3000/7568 Train_loss 1.7784413604468594 
Epoch [1/10] Batch 3100/7568 Train_loss 1.7774827474645936 
Epoch [1/10] Batch 3200/7568 Train_loss 1.7769824367008407 
Epoch [1/10] Batch 3300/7568 Train_loss 1.776823566928032 
Epoch [1/10] Batch 3400/7568 Train_loss 1.7755969101713707 
Epoch [1/10] Batch 3500/7568 Train_loss 1.771692868914818 
Epoch [1/10] Batch 3600/7568 Train_loss 1.772344519560849 
Epoch [1/10] Batch 3700/7568 Train_loss 1.7683797357344717 
Epoch [1/10] Batch 3800/7568 Train_loss 1.7710208472955293 
Epoch [1/10] Batch 3900/7568 Train_loss 1.7707202822099861 
Epoch [1/10] Batch 4000/7568 Train_loss 1.7704679820313212 
Epoch [1/10] Batch 4100/7568 Train_loss 1.7687125119769669 
Epoch [1/10] Batch 4200/7568 Train_loss 1.76679918093359 
Epoch [1/10] Batch 4300/7568 Train_loss 1.7657564189809059 
Epoch [1/10] Batch 4400/7568 Train_loss 1.7648869640219484 
Epoch [1/10] Batch 4500/7568 Train_loss 1.764173290074216 
Epoch [1/10] Batch 4600/7568 Train_loss 1.764852185101062 
Epoch [1/10] Batch 4700/7568 Train_loss 1.7655077044668033 
Epoch [1/10] Batch 4800/7568 Train_loss 1.7662159335104854 
Epoch [1/10] Batch 4900/7568 Train_loss 1.7650242556107698 
Epoch [1/10] Batch 5000/7568 Train_loss 1.76498332915098 
Epoch [1/10] Batch 5100/7568 Train_loss 1.7651567033440358 
Epoch [1/10] Batch 5200/7568 Train_loss 1.7655559386603192 
Epoch [1/10] Batch 5300/7568 Train_loss 1.7651792080506157 
Epoch [1/10] Batch 5400/7568 Train_loss 1.7651708057218185 
Epoch [1/10] Batch 5500/7568 Train_loss 1.7660696039487938 
Epoch [1/10] Batch 5600/7568 Train_loss 1.76635254848229 
Epoch [1/10] Batch 5700/7568 Train_loss 1.7678797812738705 
Epoch [1/10] Batch 5800/7568 Train_loss 1.768205949529001 
Epoch [1/10] Batch 5900/7568 Train_loss 1.7676719190725492 
Epoch [1/10] Batch 6000/7568 Train_loss 1.7676821798632452 
Epoch [1/10] Batch 6100/7568 Train_loss 1.767747109205415 
Epoch [1/10] Batch 6200/7568 Train_loss 1.7683628967869591 
Epoch [1/10] Batch 6300/7568 Train_loss 1.7679015771287658 
Epoch [1/10] Batch 6400/7568 Train_loss 1.7674369741347018 
Epoch [1/10] Batch 6500/7568 Train_loss 1.7679662068962243 
Epoch [1/10] Batch 6600/7568 Train_loss 1.7658504455719568 
Epoch [1/10] Batch 6700/7568 Train_loss 1.7671755210027698 
Epoch [1/10] Batch 6800/7568 Train_loss 1.766867723588189 
Epoch [1/10] Batch 6900/7568 Train_loss 1.7665150334499586 
Epoch [1/10] Batch 7000/7568 Train_loss 1.7681885185139194 
Epoch [1/10] Batch 7100/7568 Train_loss 1.768322719614903 
Epoch [1/10] Batch 7200/7568 Train_loss 1.7681492142169248 
Epoch [1/10] Batch 7300/7568 Train_loss 1.7691137202654517 
Epoch [1/10] Batch 7400/7568 Train_loss 1.76879039414528 
Epoch [1/10] Batch 7500/7568 Train_loss 1.7679338669509923 
Epoch: 1/10 	Training Loss: 1.767637 	Validation Loss: 1.762344 Duration seconds: 1295.6101324558258 
Validation loss decreased (1.773605 --> 1.762344).  Saving model ... 
best_valid_loss_fold [1.7623443915892636] Best_Epoch [1]Epoch [2/10] Batch 0/7568 Train_loss 1.705535650253296 
Epoch [2/10] Batch 100/7568 Train_loss 1.71052713426623 
Epoch [2/10] Batch 200/7568 Train_loss 1.700449950659453 
Epoch [2/10] Batch 300/7568 Train_loss 1.7050859285054414 
Epoch [2/10] Batch 400/7568 Train_loss 1.7089889605443673 
Epoch [2/10] Batch 500/7568 Train_loss 1.7266822412996712 
Epoch [2/10] Batch 600/7568 Train_loss 1.7263956071681865 
Epoch [2/10] Batch 700/7568 Train_loss 1.7368722921728919 
Epoch [2/10] Batch 800/7568 Train_loss 1.7348723279105291 
Epoch [2/10] Batch 900/7568 Train_loss 1.733582816381301 
Epoch [2/10] Batch 1000/7568 Train_loss 1.7336911725235749 
Epoch [2/10] Batch 1100/7568 Train_loss 1.7316570919308198 
Epoch [2/10] Batch 1200/7568 Train_loss 1.7378064534695916 
Epoch [2/10] Batch 1300/7568 Train_loss 1.73996968711687 
Epoch [2/10] Batch 1400/7568 Train_loss 1.7407902019586332 
Epoch [2/10] Batch 1500/7568 Train_loss 1.7382978407950263 
Epoch [2/10] Batch 1600/7568 Train_loss 1.741258701673714 
Epoch [2/10] Batch 1700/7568 Train_loss 1.7449583633332515 
Epoch [2/10] Batch 1800/7568 Train_loss 1.7461401209450311 
Epoch [2/10] Batch 1900/7568 Train_loss 1.7470062724160307 
Epoch [2/10] Batch 2000/7568 Train_loss 1.7516966035839976 
Epoch [2/10] Batch 2100/7568 Train_loss 1.7523220447253351 
Epoch [2/10] Batch 2200/7568 Train_loss 1.7567846824714186 
Epoch [2/10] Batch 2300/7568 Train_loss 1.758648391662474 
Epoch [2/10] Batch 2400/7568 Train_loss 1.755340354612523 
Epoch [2/10] Batch 2500/7568 Train_loss 1.7530890868478468 
Epoch [2/10] Batch 2600/7568 Train_loss 1.7532139511256895 
Epoch [2/10] Batch 2700/7568 Train_loss 1.7534878550415258 
Epoch [2/10] Batch 2800/7568 Train_loss 1.7546370679759333 
Epoch [2/10] Batch 2900/7568 Train_loss 1.756004090249148 
Epoch [2/10] Batch 3000/7568 Train_loss 1.7565325741935118 
Epoch [2/10] Batch 3100/7568 Train_loss 1.7582103332534063 
Epoch [2/10] Batch 3200/7568 Train_loss 1.7578238353100615 
Epoch [2/10] Batch 3300/7568 Train_loss 1.7550225536824537 
Epoch [2/10] Batch 3400/7568 Train_loss 1.754962124294111 
Epoch [2/10] Batch 3500/7568 Train_loss 1.7559199275965522 
Epoch [2/10] Batch 3600/7568 Train_loss 1.7572718443774706 
Epoch [2/10] Batch 3700/7568 Train_loss 1.7584163504024835 
Epoch [2/10] Batch 3800/7568 Train_loss 1.756756612038697 
Epoch [2/10] Batch 3900/7568 Train_loss 1.7583653402329256 
Epoch [2/10] Batch 4000/7568 Train_loss 1.7598544164814225 
Epoch [2/10] Batch 4100/7568 Train_loss 1.757098970441041 
Epoch [2/10] Batch 4200/7568 Train_loss 1.7574796126257577 
Epoch [2/10] Batch 4300/7568 Train_loss 1.7567970101773338 
Epoch [2/10] Batch 4400/7568 Train_loss 1.7574891771455023 
Epoch [2/10] Batch 4500/7568 Train_loss 1.756427678751551 
Epoch [2/10] Batch 4600/7568 Train_loss 1.7563780428187092 
Epoch [2/10] Batch 4700/7568 Train_loss 1.7573797906527264 
Epoch [2/10] Batch 4800/7568 Train_loss 1.7577776040860922 
Epoch [2/10] Batch 4900/7568 Train_loss 1.7573274145808495 
Epoch [2/10] Batch 5000/7568 Train_loss 1.7575228469693238 
Epoch [2/10] Batch 5100/7568 Train_loss 1.7577619809957492 
Epoch [2/10] Batch 5200/7568 Train_loss 1.7565382028403613 
Epoch [2/10] Batch 5300/7568 Train_loss 1.7567443189107006 
Epoch [2/10] Batch 5400/7568 Train_loss 1.7558912653044911 
Epoch [2/10] Batch 5500/7568 Train_loss 1.758181897381927 
Epoch [2/10] Batch 5600/7568 Train_loss 1.7590079181943168 
Epoch [2/10] Batch 5700/7568 Train_loss 1.7582109787120273 
Epoch [2/10] Batch 5800/7568 Train_loss 1.758408147326021 
Epoch [2/10] Batch 5900/7568 Train_loss 1.7592122702730084 
Epoch [2/10] Batch 6000/7568 Train_loss 1.7584363137597026 
Epoch [2/10] Batch 6100/7568 Train_loss 1.7600042167290804 
Epoch [2/10] Batch 6200/7568 Train_loss 1.7593877841375314 
Epoch [2/10] Batch 6300/7568 Train_loss 1.758546554530032 
Epoch [2/10] Batch 6400/7568 Train_loss 1.758592423692362 
Epoch [2/10] Batch 6500/7568 Train_loss 1.7590727621093198 
Epoch [2/10] Batch 6600/7568 Train_loss 1.7582978062368924 
Epoch [2/10] Batch 6700/7568 Train_loss 1.7561960691321237 
Epoch [2/10] Batch 6800/7568 Train_loss 1.7566827577061557 
Epoch [2/10] Batch 6900/7568 Train_loss 1.7572143693762126 
Epoch [2/10] Batch 7000/7568 Train_loss 1.758088048001031 
Epoch [2/10] Batch 7100/7568 Train_loss 1.758137245049629 
Epoch [2/10] Batch 7200/7568 Train_loss 1.7590006297370688 
Epoch [2/10] Batch 7300/7568 Train_loss 1.7603338984835069 
Epoch [2/10] Batch 7400/7568 Train_loss 1.7609423827020048 
Epoch [2/10] Batch 7500/7568 Train_loss 1.7603345619882842 
Epoch: 2/10 	Training Loss: 1.761697 	Validation Loss: 1.850203 Duration seconds: 1219.2532682418823 
best_valid_loss_fold [1.7623443915892636] Best_Epoch [2]Epoch [3/10] Batch 0/7568 Train_loss 1.8215899169445038 
Epoch [3/10] Batch 100/7568 Train_loss 1.725909626572439 
Epoch [3/10] Batch 200/7568 Train_loss 1.7205981682263203 
Epoch [3/10] Batch 300/7568 Train_loss 1.7512365986292941 
Epoch [3/10] Batch 400/7568 Train_loss 1.7558894296202279 
Epoch [3/10] Batch 500/7568 Train_loss 1.7339440612884576 
Epoch [3/10] Batch 600/7568 Train_loss 1.7412956045962213 
Epoch [3/10] Batch 700/7568 Train_loss 1.7558335747235 
Epoch [3/10] Batch 800/7568 Train_loss 1.777507997863227 
Epoch [3/10] Batch 900/7568 Train_loss 1.7795311726968772 
Epoch [3/10] Batch 1000/7568 Train_loss 1.774566046313032 
Epoch [3/10] Batch 1100/7568 Train_loss 1.7754744822590813 
Epoch [3/10] Batch 1200/7568 Train_loss 1.7704389765734578 
Epoch [3/10] Batch 1300/7568 Train_loss 1.769232762910191 
Epoch [3/10] Batch 1400/7568 Train_loss 1.7701193653339578 
Epoch [3/10] Batch 1500/7568 Train_loss 1.7681244064586548 
Epoch [3/10] Batch 1600/7568 Train_loss 1.7671150346283686 
Epoch [3/10] Batch 1700/7568 Train_loss 1.76950210500629 
Epoch [3/10] Batch 1800/7568 Train_loss 1.7693895641207562 
Epoch [3/10] Batch 1900/7568 Train_loss 1.7664574566975761 
Epoch [3/10] Batch 2000/7568 Train_loss 1.7680421038747192 
Epoch [3/10] Batch 2100/7568 Train_loss 1.7697136296843246 
Epoch [3/10] Batch 2200/7568 Train_loss 1.7683025127761691 
Epoch [3/10] Batch 2300/7568 Train_loss 1.7650621440237317 
Epoch [3/10] Batch 2400/7568 Train_loss 1.7602303373111083 
Epoch [3/10] Batch 2500/7568 Train_loss 1.7632290019673473 
Epoch [3/10] Batch 2600/7568 Train_loss 1.7606100404219003 
Epoch [3/10] Batch 2700/7568 Train_loss 1.7565778262226373 
Epoch [3/10] Batch 2800/7568 Train_loss 1.754699934384347 
Epoch [3/10] Batch 2900/7568 Train_loss 1.754306346198601 
Epoch [3/10] Batch 3000/7568 Train_loss 1.7550100256173422 
Epoch [3/10] Batch 3100/7568 Train_loss 1.758737280323347 
Epoch [3/10] Batch 3200/7568 Train_loss 1.7563007963877735 
Epoch [3/10] Batch 3300/7568 Train_loss 1.758083746218656 
Epoch [3/10] Batch 3400/7568 Train_loss 1.759914876583726 
Epoch [3/10] Batch 3500/7568 Train_loss 1.757263846001653 
Epoch [3/10] Batch 3600/7568 Train_loss 1.7575909313506566 
Epoch [3/10] Batch 3700/7568 Train_loss 1.7580575768032354 
Epoch [3/10] Batch 3800/7568 Train_loss 1.7573854614267754 
Epoch [3/10] Batch 3900/7568 Train_loss 1.7572355910493942 
Epoch [3/10] Batch 4000/7568 Train_loss 1.7594680583471807 
Epoch [3/10] Batch 4100/7568 Train_loss 1.7601027364314141 
Epoch [3/10] Batch 4200/7568 Train_loss 1.7582561167467352 
Epoch [3/10] Batch 4300/7568 Train_loss 1.7570852439168914 
Epoch [3/10] Batch 4400/7568 Train_loss 1.7572328953550258 
Epoch [3/10] Batch 4500/7568 Train_loss 1.7574433697409508 
Epoch [3/10] Batch 4600/7568 Train_loss 1.7552328416201193 
Epoch [3/10] Batch 4700/7568 Train_loss 1.7545923515298447 
Epoch [3/10] Batch 4800/7568 Train_loss 1.7523476566868732 
Epoch [3/10] Batch 4900/7568 Train_loss 1.7523049047740107 
Epoch [3/10] Batch 5000/7568 Train_loss 1.7506729969127228 
Epoch [3/10] Batch 5100/7568 Train_loss 1.7508020330643308 
Epoch [3/10] Batch 5200/7568 Train_loss 1.7506256363562518 
Epoch [3/10] Batch 5300/7568 Train_loss 1.750442664012653 
Epoch [3/10] Batch 5400/7568 Train_loss 1.74822385074049 
Epoch [3/10] Batch 5500/7568 Train_loss 1.7485769470983301 
Epoch [3/10] Batch 5600/7568 Train_loss 1.7495878186849074 
Epoch [3/10] Batch 5700/7568 Train_loss 1.7504555752726003 
Epoch [3/10] Batch 5800/7568 Train_loss 1.7510536183272225 
Epoch [3/10] Batch 5900/7568 Train_loss 1.7501827458308281 
Epoch [3/10] Batch 6000/7568 Train_loss 1.750751969774968 
Epoch [3/10] Batch 6100/7568 Train_loss 1.750159766296054 
Epoch [3/10] Batch 6200/7568 Train_loss 1.7484364108541592 
Epoch [3/10] Batch 6300/7568 Train_loss 1.7481260224431872 
Epoch [3/10] Batch 6400/7568 Train_loss 1.7473681035604798 
Epoch [3/10] Batch 6500/7568 Train_loss 1.7474856666809155 
Epoch [3/10] Batch 6600/7568 Train_loss 1.7476143988086101 
Epoch [3/10] Batch 6700/7568 Train_loss 1.7495404840196145 
Epoch [3/10] Batch 6800/7568 Train_loss 1.7501790144635094 
Epoch [3/10] Batch 6900/7568 Train_loss 1.7510162032638628 
Epoch [3/10] Batch 7000/7568 Train_loss 1.751005455783445 
Epoch [3/10] Batch 7100/7568 Train_loss 1.7514556156881358 
Epoch [3/10] Batch 7200/7568 Train_loss 1.751003714319468 
Epoch [3/10] Batch 7300/7568 Train_loss 1.7515096665571657 
Epoch [3/10] Batch 7400/7568 Train_loss 1.752980305220433 
Epoch [3/10] Batch 7500/7568 Train_loss 1.753022730302817 
Epoch: 3/10 	Training Loss: 1.753021 	Validation Loss: 1.776332 Duration seconds: 1214.2121970653534 
best_valid_loss_fold [1.7623443915892636] Best_Epoch [3]Epoch [4/10] Batch 0/7568 Train_loss 1.967637687921524 
Epoch [4/10] Batch 100/7568 Train_loss 1.8342028093515057 
Epoch [4/10] Batch 200/7568 Train_loss 1.805079715020621 
Epoch [4/10] Batch 300/7568 Train_loss 1.7860723125974205 
Epoch [4/10] Batch 400/7568 Train_loss 1.7870623744708345 
Epoch [4/10] Batch 500/7568 Train_loss 1.7780569771271266 
Epoch [4/10] Batch 600/7568 Train_loss 1.7724397287590927 
Epoch [4/10] Batch 700/7568 Train_loss 1.7773143257334636 
Epoch [4/10] Batch 800/7568 Train_loss 1.77618340364705 
Epoch [4/10] Batch 900/7568 Train_loss 1.7686973039172467 
Epoch [4/10] Batch 1000/7568 Train_loss 1.7563644728133014 
Epoch [4/10] Batch 1100/7568 Train_loss 1.7609711977349531 
Epoch [4/10] Batch 1200/7568 Train_loss 1.758932042044962 
Epoch [4/10] Batch 1300/7568 Train_loss 1.7526440605303584 
Epoch [4/10] Batch 1400/7568 Train_loss 1.7505740855077945 
Epoch [4/10] Batch 1500/7568 Train_loss 1.7474785619858977 
Epoch [4/10] Batch 1600/7568 Train_loss 1.7499868275317305 
Epoch [4/10] Batch 1700/7568 Train_loss 1.745918104704305 
Epoch [4/10] Batch 1800/7568 Train_loss 1.7426158220552193 
Epoch [4/10] Batch 1900/7568 Train_loss 1.7425727706279082 
Epoch [4/10] Batch 2000/7568 Train_loss 1.7411899480266848 
Epoch [4/10] Batch 2100/7568 Train_loss 1.743011315278301 
Epoch [4/10] Batch 2200/7568 Train_loss 1.7427932888756876 
Epoch [4/10] Batch 2300/7568 Train_loss 1.7376936532424254 
Epoch [4/10] Batch 2400/7568 Train_loss 1.7381548514121872 
Epoch [4/10] Batch 2500/7568 Train_loss 1.738366923722826 
Epoch [4/10] Batch 2600/7568 Train_loss 1.7394386864382227 
Epoch [4/10] Batch 2700/7568 Train_loss 1.7397373753349148 
Epoch [4/10] Batch 2800/7568 Train_loss 1.740973413559804 
Epoch [4/10] Batch 2900/7568 Train_loss 1.7434403405841652 
Epoch [4/10] Batch 3000/7568 Train_loss 1.743765394614562 
Epoch [4/10] Batch 3100/7568 Train_loss 1.7421156948120584 
Epoch [4/10] Batch 3200/7568 Train_loss 1.73944313984547 
Epoch [4/10] Batch 3300/7568 Train_loss 1.73797579848407 
Epoch [4/10] Batch 3400/7568 Train_loss 1.738322333949466 
Epoch [4/10] Batch 3500/7568 Train_loss 1.7406265823113616 
Epoch [4/10] Batch 3600/7568 Train_loss 1.7390450852504376 
Epoch [4/10] Batch 3700/7568 Train_loss 1.738128606325644 
Epoch [4/10] Batch 3800/7568 Train_loss 1.7398658495369914 
Epoch [4/10] Batch 3900/7568 Train_loss 1.740349469044153 
Epoch [4/10] Batch 4000/7568 Train_loss 1.7388884276449605 
Epoch [4/10] Batch 4100/7568 Train_loss 1.7379078261556058 
Epoch [4/10] Batch 4200/7568 Train_loss 1.7393757360686974 
Epoch [4/10] Batch 4300/7568 Train_loss 1.7402054698065086 
Epoch [4/10] Batch 4400/7568 Train_loss 1.7411406604174022 
Epoch [4/10] Batch 4500/7568 Train_loss 1.7399924752335367 
Epoch [4/10] Batch 4600/7568 Train_loss 1.738756777225979 
Epoch [4/10] Batch 4700/7568 Train_loss 1.7409120409907344 
Epoch [4/10] Batch 4800/7568 Train_loss 1.7410768051661816 
Epoch [4/10] Batch 4900/7568 Train_loss 1.741336914061758 
Epoch [4/10] Batch 5000/7568 Train_loss 1.7442783371353001 
Epoch [4/10] Batch 5100/7568 Train_loss 1.7438327085166159 
Epoch [4/10] Batch 5200/7568 Train_loss 1.7446156499710044 
Epoch [4/10] Batch 5300/7568 Train_loss 1.7444672997091497 
Epoch [4/10] Batch 5400/7568 Train_loss 1.7449324402507753 
Epoch [4/10] Batch 5500/7568 Train_loss 1.744963105235007 
Epoch [4/10] Batch 5600/7568 Train_loss 1.7443174406037418 
Epoch [4/10] Batch 5700/7568 Train_loss 1.7449314063985062 
Epoch [4/10] Batch 5800/7568 Train_loss 1.7443885165383088 
Epoch [4/10] Batch 5900/7568 Train_loss 1.745321118471542 
Epoch [4/10] Batch 6000/7568 Train_loss 1.7449904692965614 
Epoch [4/10] Batch 6100/7568 Train_loss 1.7446807793780754 
Epoch [4/10] Batch 6200/7568 Train_loss 1.7448082359721249 
Epoch [4/10] Batch 6300/7568 Train_loss 1.7453179184694438 
Epoch [4/10] Batch 6400/7568 Train_loss 1.745346690577709 
Epoch [4/10] Batch 6500/7568 Train_loss 1.7447641142797992 
Epoch [4/10] Batch 6600/7568 Train_loss 1.7449150465563146 
Epoch [4/10] Batch 6700/7568 Train_loss 1.745155714018641 
Epoch [4/10] Batch 6800/7568 Train_loss 1.746913639209872 
Epoch [4/10] Batch 6900/7568 Train_loss 1.7471487334689322 
Epoch [4/10] Batch 7000/7568 Train_loss 1.7463059547302586 
Epoch [4/10] Batch 7100/7568 Train_loss 1.7465095333267124 
Epoch [4/10] Batch 7200/7568 Train_loss 1.747379582067477 
Epoch [4/10] Batch 7300/7568 Train_loss 1.7473136554342674 
Epoch [4/10] Batch 7400/7568 Train_loss 1.7472487162970285 
Epoch [4/10] Batch 7500/7568 Train_loss 1.7477108622579443 
Epoch: 4/10 	Training Loss: 1.748501 	Validation Loss: 1.742399 Duration seconds: 1301.4995317459106 
Validation loss decreased (1.762344 --> 1.742399).  Saving model ... 
best_valid_loss_fold [1.742399228634372] Best_Epoch [4]Epoch [5/10] Batch 0/7568 Train_loss 1.9200260937213898 
Epoch [5/10] Batch 100/7568 Train_loss 1.7540781081903098 
Epoch [5/10] Batch 200/7568 Train_loss 1.7540165983993024 
Epoch [5/10] Batch 300/7568 Train_loss 1.795188502046555 
Epoch [5/10] Batch 400/7568 Train_loss 1.7965146381770287 
Epoch [5/10] Batch 500/7568 Train_loss 1.7665787128928654 
Epoch [5/10] Batch 600/7568 Train_loss 1.7516817809539706 
Epoch [5/10] Batch 700/7568 Train_loss 1.7495903416872876 
Epoch [5/10] Batch 800/7568 Train_loss 1.7464741530099166 
Epoch [5/10] Batch 900/7568 Train_loss 1.7403254315223995 
Epoch [5/10] Batch 1000/7568 Train_loss 1.742889117803071 
Epoch [5/10] Batch 1100/7568 Train_loss 1.7438275839027007 
Epoch [5/10] Batch 1200/7568 Train_loss 1.7423255012977827 
Epoch [5/10] Batch 1300/7568 Train_loss 1.741156777385608 
Epoch [5/10] Batch 1400/7568 Train_loss 1.738824796311818 
Epoch [5/10] Batch 1500/7568 Train_loss 1.741110629722883 
Epoch [5/10] Batch 1600/7568 Train_loss 1.7422103957691317 
Epoch [5/10] Batch 1700/7568 Train_loss 1.7401422526488088 
Epoch [5/10] Batch 1800/7568 Train_loss 1.739145156941369 
Epoch [5/10] Batch 1900/7568 Train_loss 1.7368406896870867 
Epoch [5/10] Batch 2000/7568 Train_loss 1.737509706224697 
Epoch [5/10] Batch 2100/7568 Train_loss 1.7390630265138423 
Epoch [5/10] Batch 2200/7568 Train_loss 1.7358739411408444 
Epoch [5/10] Batch 2300/7568 Train_loss 1.7364000029755073 
Epoch [5/10] Batch 2400/7568 Train_loss 1.7363257366063842 
Epoch [5/10] Batch 2500/7568 Train_loss 1.7361362387792247 
Epoch [5/10] Batch 2600/7568 Train_loss 1.733106647885588 
Epoch [5/10] Batch 2700/7568 Train_loss 1.7336164805065786 
Epoch [5/10] Batch 2800/7568 Train_loss 1.7319273311207226 
Epoch [5/10] Batch 2900/7568 Train_loss 1.7333283514344293 
Epoch [5/10] Batch 3000/7568 Train_loss 1.7333677550731241 
Epoch [5/10] Batch 3100/7568 Train_loss 1.735434856761928 
Epoch [5/10] Batch 3200/7568 Train_loss 1.736221769123441 
Epoch [5/10] Batch 3300/7568 Train_loss 1.7368491198692637 
Epoch [5/10] Batch 3400/7568 Train_loss 1.7362241341599083 
Epoch [5/10] Batch 3500/7568 Train_loss 1.73908758361581 
Epoch [5/10] Batch 3600/7568 Train_loss 1.737461555330199 
Epoch [5/10] Batch 3700/7568 Train_loss 1.7374337445244987 
Epoch [5/10] Batch 3800/7568 Train_loss 1.7369261078812894 
Epoch [5/10] Batch 3900/7568 Train_loss 1.7366096992365552 
Epoch [5/10] Batch 4000/7568 Train_loss 1.7348363803840672 
Epoch [5/10] Batch 4100/7568 Train_loss 1.736674140244032 
Epoch [5/10] Batch 4200/7568 Train_loss 1.7375088403020493 
Epoch [5/10] Batch 4300/7568 Train_loss 1.7378092857311145 
Epoch [5/10] Batch 4400/7568 Train_loss 1.7378106043318324 
Epoch [5/10] Batch 4500/7568 Train_loss 1.7380876042622508 
Epoch [5/10] Batch 4600/7568 Train_loss 1.7381010697156651 
Epoch [5/10] Batch 4700/7568 Train_loss 1.7387928658534912 
Epoch [5/10] Batch 4800/7568 Train_loss 1.7392098256676825 
Epoch [5/10] Batch 4900/7568 Train_loss 1.7399743917270647 
Epoch [5/10] Batch 5000/7568 Train_loss 1.7394166170040075 
Epoch [5/10] Batch 5100/7568 Train_loss 1.7391954271761465 
Epoch [5/10] Batch 5200/7568 Train_loss 1.7398580052194927 
Epoch [5/10] Batch 5300/7568 Train_loss 1.7393317025163966 
Epoch [5/10] Batch 5400/7568 Train_loss 1.7398356422892303 
Epoch [5/10] Batch 5500/7568 Train_loss 1.7402288285513245 
Epoch [5/10] Batch 5600/7568 Train_loss 1.738968608673769 
Epoch [5/10] Batch 5700/7568 Train_loss 1.7388000891888127 
Epoch [5/10] Batch 5800/7568 Train_loss 1.7378622809079183 
Epoch [5/10] Batch 5900/7568 Train_loss 1.7380420086310993 
Epoch [5/10] Batch 6000/7568 Train_loss 1.7396818017290645 
Epoch [5/10] Batch 6100/7568 Train_loss 1.7392369871844602 
Epoch [5/10] Batch 6200/7568 Train_loss 1.7388611849847389 
Epoch [5/10] Batch 6300/7568 Train_loss 1.7384370946196919 
Epoch [5/10] Batch 6400/7568 Train_loss 1.7377469441251836 
Epoch [5/10] Batch 6500/7568 Train_loss 1.7379718946905856 
Epoch [5/10] Batch 6600/7568 Train_loss 1.7385524773253236 
Epoch [5/10] Batch 6700/7568 Train_loss 1.737655304336731 
Epoch [5/10] Batch 6800/7568 Train_loss 1.7388537045474561 
Epoch [5/10] Batch 6900/7568 Train_loss 1.7397105166791937 
Epoch [5/10] Batch 7000/7568 Train_loss 1.7396583002278283 
Epoch [5/10] Batch 7100/7568 Train_loss 1.7401395058171496 
Epoch [5/10] Batch 7200/7568 Train_loss 1.7416831059232634 
Epoch [5/10] Batch 7300/7568 Train_loss 1.742480816303002 
Epoch [5/10] Batch 7400/7568 Train_loss 1.7432361525349915 
Epoch [5/10] Batch 7500/7568 Train_loss 1.7431079653940857 
Epoch: 5/10 	Training Loss: 1.743551 	Validation Loss: 1.759558 Duration seconds: 1225.3140308856964 
best_valid_loss_fold [1.742399228634372] Best_Epoch [5]Epoch [6/10] Batch 0/7568 Train_loss 0.8702592551708221 
Epoch [6/10] Batch 100/7568 Train_loss 1.7383120238485903 
Epoch [6/10] Batch 200/7568 Train_loss 1.7661141400313496 
Epoch [6/10] Batch 300/7568 Train_loss 1.6948442960597352 
Epoch [6/10] Batch 400/7568 Train_loss 1.7105487288159325 
Epoch [6/10] Batch 500/7568 Train_loss 1.732071711125964 
Epoch [6/10] Batch 600/7568 Train_loss 1.7290937657562548 
Epoch [6/10] Batch 700/7568 Train_loss 1.7192905168773105 
Epoch [6/10] Batch 800/7568 Train_loss 1.7278915743219123 
Epoch [6/10] Batch 900/7568 Train_loss 1.727574388522419 
Epoch [6/10] Batch 1000/7568 Train_loss 1.7397833513570475 
Epoch [6/10] Batch 1100/7568 Train_loss 1.7414227486008627 
Epoch [6/10] Batch 1200/7568 Train_loss 1.7365108672948206 
Epoch [6/10] Batch 1300/7568 Train_loss 1.7370454358261975 
Epoch [6/10] Batch 1400/7568 Train_loss 1.7299627292792257 
Epoch [6/10] Batch 1500/7568 Train_loss 1.7352320156807823 
Epoch [6/10] Batch 1600/7568 Train_loss 1.7413768025439729 
Epoch [6/10] Batch 1700/7568 Train_loss 1.741778118747217 
Epoch [6/10] Batch 1800/7568 Train_loss 1.744286468534884 
Epoch [6/10] Batch 1900/7568 Train_loss 1.7482630253378997 
Epoch [6/10] Batch 2000/7568 Train_loss 1.7465179693000368 
Epoch [6/10] Batch 2100/7568 Train_loss 1.7489352079196125 
Epoch [6/10] Batch 2200/7568 Train_loss 1.7489643046113643 
Epoch [6/10] Batch 2300/7568 Train_loss 1.7466311039049331 
Epoch [6/10] Batch 2400/7568 Train_loss 1.7476497691237942 
Epoch [6/10] Batch 2500/7568 Train_loss 1.748404177244617 
Epoch [6/10] Batch 2600/7568 Train_loss 1.7490794034284474 
Epoch [6/10] Batch 2700/7568 Train_loss 1.7480965957917227 
Epoch [6/10] Batch 2800/7568 Train_loss 1.7487983616126814 
Epoch [6/10] Batch 2900/7568 Train_loss 1.7471835571567547 
Epoch [6/10] Batch 3000/7568 Train_loss 1.746396355291221 
Epoch [6/10] Batch 3100/7568 Train_loss 1.7447733664653908 
Epoch [6/10] Batch 3200/7568 Train_loss 1.7446524277794961 
Epoch [6/10] Batch 3300/7568 Train_loss 1.7406150171636487 
Epoch [6/10] Batch 3400/7568 Train_loss 1.7383202235467712 
Epoch [6/10] Batch 3500/7568 Train_loss 1.73834394098554 
Epoch [6/10] Batch 3600/7568 Train_loss 1.7388560251953271 
Epoch [6/10] Batch 3700/7568 Train_loss 1.7384181760092485 
Epoch [6/10] Batch 3800/7568 Train_loss 1.7389947744426963 
Epoch [6/10] Batch 3900/7568 Train_loss 1.7383552549783616 
Epoch [6/10] Batch 4000/7568 Train_loss 1.7380124564317279 
Epoch [6/10] Batch 4100/7568 Train_loss 1.736970199819339 
Epoch [6/10] Batch 4200/7568 Train_loss 1.7370015718849805 
Epoch [6/10] Batch 4300/7568 Train_loss 1.7395868093828577 
Epoch [6/10] Batch 4400/7568 Train_loss 1.7390514993851793 
Epoch [6/10] Batch 4500/7568 Train_loss 1.738515290216694 
Epoch [6/10] Batch 4600/7568 Train_loss 1.7387676397385066 
Epoch [6/10] Batch 4700/7568 Train_loss 1.7386322953575624 
Epoch [6/10] Batch 4800/7568 Train_loss 1.7384871294447666 
Epoch [6/10] Batch 4900/7568 Train_loss 1.7381110541496658 
Epoch [6/10] Batch 5000/7568 Train_loss 1.7393864821542075 
Epoch [6/10] Batch 5100/7568 Train_loss 1.7374703251489982 
Epoch [6/10] Batch 5200/7568 Train_loss 1.7381241602668784 
Epoch [6/10] Batch 5300/7568 Train_loss 1.737138510426414 
Epoch [6/10] Batch 5400/7568 Train_loss 1.7363437824399577 
Epoch [6/10] Batch 5500/7568 Train_loss 1.7336872628460123 
Epoch [6/10] Batch 5600/7568 Train_loss 1.7335185102241804 
Epoch [6/10] Batch 5700/7568 Train_loss 1.735213968828987 
Epoch [6/10] Batch 5800/7568 Train_loss 1.7353520117516437 
Epoch [6/10] Batch 5900/7568 Train_loss 1.735406856572925 
Epoch [6/10] Batch 6000/7568 Train_loss 1.7352126367418712 
Epoch [6/10] Batch 6100/7568 Train_loss 1.734974033992577 
Epoch [6/10] Batch 6200/7568 Train_loss 1.736348874493292 
Epoch [6/10] Batch 6300/7568 Train_loss 1.7359764732088179 
Epoch [6/10] Batch 6400/7568 Train_loss 1.73593364231122 
Epoch [6/10] Batch 6500/7568 Train_loss 1.7355370074923158 
Epoch [6/10] Batch 6600/7568 Train_loss 1.7356410288838722 
Epoch [6/10] Batch 6700/7568 Train_loss 1.7362214766310464 
Epoch [6/10] Batch 6800/7568 Train_loss 1.7356174341033896 
Epoch [6/10] Batch 6900/7568 Train_loss 1.7348495615386321 
Epoch [6/10] Batch 7000/7568 Train_loss 1.7350944779614281 
Epoch [6/10] Batch 7100/7568 Train_loss 1.7343614693091565 
Epoch [6/10] Batch 7200/7568 Train_loss 1.7349011422759741 
Epoch [6/10] Batch 7300/7568 Train_loss 1.7358185525908567 
Epoch [6/10] Batch 7400/7568 Train_loss 1.735804171062711 
Epoch [6/10] Batch 7500/7568 Train_loss 1.7369877849975566 
Epoch: 6/10 	Training Loss: 1.736783 	Validation Loss: 1.733828 Duration seconds: 1210.5332882404327 
Validation loss decreased (1.742399 --> 1.733828).  Saving model ... 
best_valid_loss_fold [1.7338284895249325] Best_Epoch [6]Epoch [7/10] Batch 0/7568 Train_loss 2.335759848356247 
Epoch [7/10] Batch 100/7568 Train_loss 1.7416258517763403 
Epoch [7/10] Batch 200/7568 Train_loss 1.7484252773855455 
Epoch [7/10] Batch 300/7568 Train_loss 1.7730439991451974 
Epoch [7/10] Batch 400/7568 Train_loss 1.7407172791975691 
Epoch [7/10] Batch 500/7568 Train_loss 1.746850637440196 
Epoch [7/10] Batch 600/7568 Train_loss 1.7285294665919366 
Epoch [7/10] Batch 700/7568 Train_loss 1.7333238148336403 
Epoch [7/10] Batch 800/7568 Train_loss 1.731921604296167 
Epoch [7/10] Batch 900/7568 Train_loss 1.725501381480998 
Epoch [7/10] Batch 1000/7568 Train_loss 1.7284879101055128 
Epoch [7/10] Batch 1100/7568 Train_loss 1.73588070040611 
Epoch [7/10] Batch 1200/7568 Train_loss 1.7364826670023523 
Epoch [7/10] Batch 1300/7568 Train_loss 1.7305913923750649 
Epoch [7/10] Batch 1400/7568 Train_loss 1.7289839189853267 
Epoch [7/10] Batch 1500/7568 Train_loss 1.724563246266513 
Epoch [7/10] Batch 1600/7568 Train_loss 1.7311071042322799 
Epoch [7/10] Batch 1700/7568 Train_loss 1.7256846790503642 
Epoch [7/10] Batch 1800/7568 Train_loss 1.726938478528692 
Epoch [7/10] Batch 1900/7568 Train_loss 1.7270716705772515 
Epoch [7/10] Batch 2000/7568 Train_loss 1.727353397483143 
Epoch [7/10] Batch 2100/7568 Train_loss 1.728836547092192 
Epoch [7/10] Batch 2200/7568 Train_loss 1.7296599836233852 
Epoch [7/10] Batch 2300/7568 Train_loss 1.7298022629300547 
Epoch [7/10] Batch 2400/7568 Train_loss 1.73039225742736 
Epoch [7/10] Batch 2500/7568 Train_loss 1.7273389499207012 
Epoch [7/10] Batch 2600/7568 Train_loss 1.7262131335505244 
Epoch [7/10] Batch 2700/7568 Train_loss 1.7220420536427576 
Epoch [7/10] Batch 2800/7568 Train_loss 1.7232844945986285 
Epoch [7/10] Batch 2900/7568 Train_loss 1.7229222939168327 
Epoch [7/10] Batch 3000/7568 Train_loss 1.7232819437181224 
Epoch [7/10] Batch 3100/7568 Train_loss 1.7229586523849132 
Epoch [7/10] Batch 3200/7568 Train_loss 1.725084576226238 
Epoch [7/10] Batch 3300/7568 Train_loss 1.7235994002121897 
Epoch [7/10] Batch 3400/7568 Train_loss 1.7249693346703554 
Epoch [7/10] Batch 3500/7568 Train_loss 1.7237707014017125 
Epoch [7/10] Batch 3600/7568 Train_loss 1.7249078555997297 
Epoch [7/10] Batch 3700/7568 Train_loss 1.7236182607873425 
Epoch [7/10] Batch 3800/7568 Train_loss 1.7239187739081867 
Epoch [7/10] Batch 3900/7568 Train_loss 1.7246964833322198 
Epoch [7/10] Batch 4000/7568 Train_loss 1.7253145981199203 
Epoch [7/10] Batch 4100/7568 Train_loss 1.725206447584008 
Epoch [7/10] Batch 4200/7568 Train_loss 1.7240636093531783 
Epoch [7/10] Batch 4300/7568 Train_loss 1.7246357740527307 
Epoch [7/10] Batch 4400/7568 Train_loss 1.7248836609940588 
Epoch [7/10] Batch 4500/7568 Train_loss 1.7261274350226468 
Epoch [7/10] Batch 4600/7568 Train_loss 1.7249675998105949 
Epoch [7/10] Batch 4700/7568 Train_loss 1.7261665994751152 
Epoch [7/10] Batch 4800/7568 Train_loss 1.7259251343936404 
Epoch [7/10] Batch 4900/7568 Train_loss 1.7248343341348975 
Epoch [7/10] Batch 5000/7568 Train_loss 1.725450715065372 
Epoch [7/10] Batch 5100/7568 Train_loss 1.7248421894060328 
Epoch [7/10] Batch 5200/7568 Train_loss 1.7243116401484027 
Epoch [7/10] Batch 5300/7568 Train_loss 1.7247923417693263 
Epoch [7/10] Batch 5400/7568 Train_loss 1.7251279527261796 
Epoch [7/10] Batch 5500/7568 Train_loss 1.7248619927766864 
Epoch [7/10] Batch 5600/7568 Train_loss 1.7257786187007584 
Epoch [7/10] Batch 5700/7568 Train_loss 1.7264247967436364 
Epoch [7/10] Batch 5800/7568 Train_loss 1.7261617492394414 
Epoch [7/10] Batch 5900/7568 Train_loss 1.7267480320373727 
Epoch [7/10] Batch 6000/7568 Train_loss 1.7262260208222653 
Epoch [7/10] Batch 6100/7568 Train_loss 1.7271463692657205 
Epoch [7/10] Batch 6200/7568 Train_loss 1.7275853157932577 
Epoch [7/10] Batch 6300/7568 Train_loss 1.727951115662097 
Epoch [7/10] Batch 6400/7568 Train_loss 1.7273198953574942 
Epoch [7/10] Batch 6500/7568 Train_loss 1.72798004035188 
Epoch [7/10] Batch 6600/7568 Train_loss 1.7285256448514064 
Epoch [7/10] Batch 6700/7568 Train_loss 1.729802666566938 
Epoch [7/10] Batch 6800/7568 Train_loss 1.7309145354651436 
Epoch [7/10] Batch 6900/7568 Train_loss 1.7318547026635753 
Epoch [7/10] Batch 7000/7568 Train_loss 1.7335943832308152 
Epoch [7/10] Batch 7100/7568 Train_loss 1.7332985672725705 
Epoch [7/10] Batch 7200/7568 Train_loss 1.7334812803359732 
Epoch [7/10] Batch 7300/7568 Train_loss 1.733919835870494 
Epoch [7/10] Batch 7400/7568 Train_loss 1.7345362806302875 
Epoch [7/10] Batch 7500/7568 Train_loss 1.7347422667536048 
Epoch: 7/10 	Training Loss: 1.734287 	Validation Loss: 1.727639 Duration seconds: 1239.4214754104614 
Validation loss decreased (1.733828 --> 1.727639).  Saving model ... 
best_valid_loss_fold [1.7276394384702467] Best_Epoch [7]Epoch [8/10] Batch 0/7568 Train_loss 1.5708038210868835 
Epoch [8/10] Batch 100/7568 Train_loss 1.7293299299360503 
Epoch [8/10] Batch 200/7568 Train_loss 1.7213000142159154 
Epoch [8/10] Batch 300/7568 Train_loss 1.692854634724384 
Epoch [8/10] Batch 400/7568 Train_loss 1.7024889910719043 
Epoch [8/10] Batch 500/7568 Train_loss 1.7123800918400407 
Epoch [8/10] Batch 600/7568 Train_loss 1.71338679916698 
Epoch [8/10] Batch 700/7568 Train_loss 1.7074952182092953 
Epoch [8/10] Batch 800/7568 Train_loss 1.7172751627797342 
Epoch [8/10] Batch 900/7568 Train_loss 1.7219690754430805 
Epoch [8/10] Batch 1000/7568 Train_loss 1.7156272322743327 
Epoch [8/10] Batch 1100/7568 Train_loss 1.7194425892237097 
Epoch [8/10] Batch 1200/7568 Train_loss 1.721004912479285 
Epoch [8/10] Batch 1300/7568 Train_loss 1.7136874031289828 
Epoch [8/10] Batch 1400/7568 Train_loss 1.7148783516609438 
Epoch [8/10] Batch 1500/7568 Train_loss 1.7181420755348629 
Epoch [8/10] Batch 1600/7568 Train_loss 1.723440753960334 
Epoch [8/10] Batch 1700/7568 Train_loss 1.7234476583230713 
Epoch [8/10] Batch 1800/7568 Train_loss 1.7199482963208952 
Epoch [8/10] Batch 1900/7568 Train_loss 1.7194805604291552 
Epoch [8/10] Batch 2000/7568 Train_loss 1.7162792227868138 
Epoch [8/10] Batch 2100/7568 Train_loss 1.7132348450754495 
Epoch [8/10] Batch 2200/7568 Train_loss 1.7149864997831543 
Epoch [8/10] Batch 2300/7568 Train_loss 1.7194935731890926 
Epoch [8/10] Batch 2400/7568 Train_loss 1.7177659015278675 
Epoch [8/10] Batch 2500/7568 Train_loss 1.7208387770327698 
Epoch [8/10] Batch 2600/7568 Train_loss 1.7207604283463354 
Epoch [8/10] Batch 2700/7568 Train_loss 1.72248847832264 
Epoch [8/10] Batch 2800/7568 Train_loss 1.7254349282653108 
Epoch [8/10] Batch 2900/7568 Train_loss 1.727304298639996 
Epoch [8/10] Batch 3000/7568 Train_loss 1.731118157143515 
Epoch [8/10] Batch 3100/7568 Train_loss 1.7319352801328165 
Epoch [8/10] Batch 3200/7568 Train_loss 1.7288265120318367 
Epoch [8/10] Batch 3300/7568 Train_loss 1.7280403629816579 
Epoch [8/10] Batch 3400/7568 Train_loss 1.728222162228092 
Epoch [8/10] Batch 3500/7568 Train_loss 1.7298818928697932 
Epoch [8/10] Batch 3600/7568 Train_loss 1.7301901994426692 
Epoch [8/10] Batch 3700/7568 Train_loss 1.7306250550297748 
Epoch [8/10] Batch 3800/7568 Train_loss 1.7303665136877753 
Epoch [8/10] Batch 3900/7568 Train_loss 1.7278035423964055 
Epoch [8/10] Batch 4000/7568 Train_loss 1.728655825951224 
Epoch [8/10] Batch 4100/7568 Train_loss 1.726252471220127 
Epoch [8/10] Batch 4200/7568 Train_loss 1.7276336765504259 
Epoch [8/10] Batch 4300/7568 Train_loss 1.728229948579988 
Epoch [8/10] Batch 4400/7568 Train_loss 1.7272058676634559 
Epoch [8/10] Batch 4500/7568 Train_loss 1.7294057782915693 
Epoch [8/10] Batch 4600/7568 Train_loss 1.729057415594383 
Epoch [8/10] Batch 4700/7568 Train_loss 1.7312863460089951 
Epoch [8/10] Batch 4800/7568 Train_loss 1.7302044328411552 
Epoch [8/10] Batch 4900/7568 Train_loss 1.7299914246258188 
Epoch [8/10] Batch 5000/7568 Train_loss 1.7311503273863074 
Epoch [8/10] Batch 5100/7568 Train_loss 1.7301954207563957 
Epoch [8/10] Batch 5200/7568 Train_loss 1.7298001438519675 
Epoch [8/10] Batch 5300/7568 Train_loss 1.7311388631332845 
Epoch [8/10] Batch 5400/7568 Train_loss 1.7306299324491756 
Epoch [8/10] Batch 5500/7568 Train_loss 1.7305913754383166 
Epoch [8/10] Batch 5600/7568 Train_loss 1.7294049561702125 
Epoch [8/10] Batch 5700/7568 Train_loss 1.730087459370967 
Epoch [8/10] Batch 5800/7568 Train_loss 1.729194382004924 
Epoch [8/10] Batch 5900/7568 Train_loss 1.729404436595772 
Epoch [8/10] Batch 6000/7568 Train_loss 1.7288139498165758 
Epoch [8/10] Batch 6100/7568 Train_loss 1.7290167893418775 
Epoch [8/10] Batch 6200/7568 Train_loss 1.7296581933419948 
Epoch [8/10] Batch 6300/7568 Train_loss 1.7281810331890421 
Epoch [8/10] Batch 6400/7568 Train_loss 1.7280986907775095 
Epoch [8/10] Batch 6500/7568 Train_loss 1.7283998835139083 
Epoch [8/10] Batch 6600/7568 Train_loss 1.7286729093467788 
Epoch [8/10] Batch 6700/7568 Train_loss 1.7295730609984348 
Epoch [8/10] Batch 6800/7568 Train_loss 1.73069173239808 
Epoch [8/10] Batch 6900/7568 Train_loss 1.7306904330441544 
Epoch [8/10] Batch 7000/7568 Train_loss 1.7311536499435196 
Epoch [8/10] Batch 7100/7568 Train_loss 1.7306725394178522 
Epoch [8/10] Batch 7200/7568 Train_loss 1.7311849334189138 
Epoch [8/10] Batch 7300/7568 Train_loss 1.731369511607395 
Epoch [8/10] Batch 7400/7568 Train_loss 1.7311852792278688 
Epoch [8/10] Batch 7500/7568 Train_loss 1.7312778620410165 
Epoch: 8/10 	Training Loss: 1.730223 	Validation Loss: 1.750434 Duration seconds: 1259.173488855362 
best_valid_loss_fold [1.7276394384702467] Best_Epoch [8]Epoch [9/10] Batch 0/7568 Train_loss 0.9704814106225967 
Epoch [9/10] Batch 100/7568 Train_loss 1.7327893112260517 
Epoch [9/10] Batch 200/7568 Train_loss 1.7524047664816107 
Epoch [9/10] Batch 300/7568 Train_loss 1.7161962750098634 
Epoch [9/10] Batch 400/7568 Train_loss 1.7082132261590173 
Epoch [9/10] Batch 500/7568 Train_loss 1.7126015549945737 
Epoch [9/10] Batch 600/7568 Train_loss 1.708950148260038 
Epoch [9/10] Batch 700/7568 Train_loss 1.705606016363635 
Epoch [9/10] Batch 800/7568 Train_loss 1.7136683771747179 
Epoch [9/10] Batch 900/7568 Train_loss 1.712052980634269 
Epoch [9/10] Batch 1000/7568 Train_loss 1.7122028795230044 
Epoch [9/10] Batch 1100/7568 Train_loss 1.7132471030498504 
Epoch [9/10] Batch 1200/7568 Train_loss 1.7073835775280972 
Epoch [9/10] Batch 1300/7568 Train_loss 1.7121792169177559 
Epoch [9/10] Batch 1400/7568 Train_loss 1.7133859706926908 
Epoch [9/10] Batch 1500/7568 Train_loss 1.7187923419572846 
Epoch [9/10] Batch 1600/7568 Train_loss 1.724636587177605 
Epoch [9/10] Batch 1700/7568 Train_loss 1.719464033443496 
Epoch [9/10] Batch 1800/7568 Train_loss 1.7213935060617924 
Epoch [9/10] Batch 1900/7568 Train_loss 1.7211939227708009 
Epoch [9/10] Batch 2000/7568 Train_loss 1.7216556784378059 
Epoch [9/10] Batch 2100/7568 Train_loss 1.7213304792094208 
Epoch [9/10] Batch 2200/7568 Train_loss 1.722779927664402 
Epoch [9/10] Batch 2300/7568 Train_loss 1.720660069713718 
Epoch [9/10] Batch 2400/7568 Train_loss 1.720263702761551 
Epoch [9/10] Batch 2500/7568 Train_loss 1.7272084968828145 
Epoch [9/10] Batch 2600/7568 Train_loss 1.7284073674767029 
Epoch [9/10] Batch 2700/7568 Train_loss 1.7293629597945595 
Epoch [9/10] Batch 2800/7568 Train_loss 1.7275297763447854 
Epoch [9/10] Batch 2900/7568 Train_loss 1.7285418539330164 
Epoch [9/10] Batch 3000/7568 Train_loss 1.7233948211313803 
Epoch [9/10] Batch 3100/7568 Train_loss 1.7215454565425412 
Epoch [9/10] Batch 3200/7568 Train_loss 1.719032379949011 
Epoch [9/10] Batch 3300/7568 Train_loss 1.718687041961977 
Epoch [9/10] Batch 3400/7568 Train_loss 1.7200462633162799 
Epoch [9/10] Batch 3500/7568 Train_loss 1.7198700537190577 
Epoch [9/10] Batch 3600/7568 Train_loss 1.7194492345475647 
Epoch [9/10] Batch 3700/7568 Train_loss 1.7200968417363436 
Epoch [9/10] Batch 3800/7568 Train_loss 1.7205053496536409 
Epoch [9/10] Batch 3900/7568 Train_loss 1.723718137748423 
Epoch [9/10] Batch 4000/7568 Train_loss 1.7221079708557223 
Epoch [9/10] Batch 4100/7568 Train_loss 1.7221590892657714 
Epoch [9/10] Batch 4200/7568 Train_loss 1.723005708384176 
Epoch [9/10] Batch 4300/7568 Train_loss 1.7228337786909842 
Epoch [9/10] Batch 4400/7568 Train_loss 1.722678249150386 
Epoch [9/10] Batch 4500/7568 Train_loss 1.723641041965054 
Epoch [9/10] Batch 4600/7568 Train_loss 1.724563022321254 
Epoch [9/10] Batch 4700/7568 Train_loss 1.724027980050047 
Epoch [9/10] Batch 4800/7568 Train_loss 1.7236652142618303 
Epoch [9/10] Batch 4900/7568 Train_loss 1.722808050877633 
Epoch [9/10] Batch 5000/7568 Train_loss 1.722042584988063 
Epoch [9/10] Batch 5100/7568 Train_loss 1.7223650448490209 
Epoch [9/10] Batch 5200/7568 Train_loss 1.72322645286337 
Epoch [9/10] Batch 5300/7568 Train_loss 1.722563920316663 
Epoch [9/10] Batch 5400/7568 Train_loss 1.7228480906378691 
Epoch [9/10] Batch 5500/7568 Train_loss 1.7234905754784695 
Epoch [9/10] Batch 5600/7568 Train_loss 1.7242147042347031 
Epoch [9/10] Batch 5700/7568 Train_loss 1.7250621254975556 
Epoch [9/10] Batch 5800/7568 Train_loss 1.7244783765993597 
Epoch [9/10] Batch 5900/7568 Train_loss 1.7251316653752384 
Epoch [9/10] Batch 6000/7568 Train_loss 1.7244627046693843 
Epoch [9/10] Batch 6100/7568 Train_loss 1.724019714690216 
Epoch [9/10] Batch 6200/7568 Train_loss 1.7237400881966514 
Epoch [9/10] Batch 6300/7568 Train_loss 1.723650734876947 
Epoch [9/10] Batch 6400/7568 Train_loss 1.7243426134950983 
Epoch [9/10] Batch 6500/7568 Train_loss 1.7254083268166762 
Epoch [9/10] Batch 6600/7568 Train_loss 1.7256310786964442 
Epoch [9/10] Batch 6700/7568 Train_loss 1.7260835952350781 
Epoch [9/10] Batch 6800/7568 Train_loss 1.726049023786243 
Epoch [9/10] Batch 6900/7568 Train_loss 1.7259752154814525 
Epoch [9/10] Batch 7000/7568 Train_loss 1.7248314280164547 
Epoch [9/10] Batch 7100/7568 Train_loss 1.7243256321426865 
Epoch [9/10] Batch 7200/7568 Train_loss 1.7233900191077935 
Epoch [9/10] Batch 7300/7568 Train_loss 1.723782342728883 
Epoch [9/10] Batch 7400/7568 Train_loss 1.723303817793447 
Epoch [9/10] Batch 7500/7568 Train_loss 1.7237088961086977 
Epoch: 9/10 	Training Loss: 1.724170 	Validation Loss: 1.763270 Duration seconds: 1216.8171756267548 
best_valid_loss_fold [1.7276394384702467] Best_Epoch [9]
{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "PermissionError",
     "evalue": "[Errno 13] Permission denied: '/extra_disk_1'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPermissionError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-4a729f7e229e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0mmodel_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodels_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodels_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m     \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmakedirs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodels_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"create models_dir: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodels_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/os.py\u001b[0m in \u001b[0;36mmakedirs\u001b[0;34m(name, mode, exist_ok)\u001b[0m\n\u001b[1;32m    208\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhead\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtail\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 210\u001b[0;31m             \u001b[0mmakedirs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexist_ok\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    211\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mFileExistsError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m             \u001b[0;31m# Defeats race condition when another thread created the path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/os.py\u001b[0m in \u001b[0;36mmakedirs\u001b[0;34m(name, mode, exist_ok)\u001b[0m\n\u001b[1;32m    208\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhead\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtail\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 210\u001b[0;31m             \u001b[0mmakedirs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexist_ok\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    211\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mFileExistsError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m             \u001b[0;31m# Defeats race condition when another thread created the path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/os.py\u001b[0m in \u001b[0;36mmakedirs\u001b[0;34m(name, mode, exist_ok)\u001b[0m\n\u001b[1;32m    218\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 220\u001b[0;31m         \u001b[0mmkdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    221\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m         \u001b[0;31m# Cannot rely on checking for EEXIST, since the operating system\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPermissionError\u001b[0m: [Errno 13] Permission denied: '/extra_disk_1'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torch import optim, nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "import torchvision\n",
    "from torch.utils.data.dataset import Dataset\n",
    "import matplotlib.pyplot as plt\n",
    "import os.path\n",
    "from os import path\n",
    "from collections import OrderedDict\n",
    "import time\n",
    "import glob\n",
    "from torch.utils.tensorboard import SummaryWriter  # to print to tensorboard\n",
    "\n",
    "\n",
    "models_dir = os.path.expanduser('/extra_disk_1/trained_model/resnet18_dcgan')\n",
    "model_name = 'resnet18_dcgan_kfold.pt'\n",
    "model_path = os.path.join(models_dir, model_name)\n",
    "if not os.path.exists(models_dir):\n",
    "    os.makedirs(models_dir)\n",
    "    print(\"create models_dir: \", models_dir)\n",
    "\n",
    "print('Model save/load location: {}'.format(model_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Dataset Loading Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_list = ['Cardiomegaly','Emphysema','Effusion','Hernia','Nodule','Pneumothorax','Atelectasis','Pleural_Thickening','Mass','Edema','Consolidation',\n",
    "              'Infiltration','Fibrosis','Pneumonia','No Finding']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resolve_full_path(img_name):\n",
    "    original_is_found = False\n",
    "    dcgan_is_found = False\n",
    "    \n",
    "    # Read 1 image file\n",
    "    folder_idx_range = 13\n",
    "    img_path = ''\n",
    "    for folder_idx in range(folder_idx_range):\n",
    "        path_prefix = \"/extra_disk_1/data/images_\"\n",
    "        path_suffix = \"images/\"\n",
    "        cur_img_dir = path_prefix +str(folder_idx).zfill(3) +'/'\n",
    "        img_folder_path = path.join(cur_img_dir, path_suffix)\n",
    "        img_path = os.path.join(img_folder_path, img_name) \n",
    "        if(path.exists(img_path)):\n",
    "            original_is_found = True\n",
    "            break\n",
    "            \n",
    "    if(not original_is_found):\n",
    "        # search in dcgan_image folder\n",
    "        path_prefix = glob.glob(\"/extra_disk_1/code/medical_ip/NIH_code/DCGAN_NIH/dcgan_image/*\")\n",
    "        for path_folder in path_prefix:\n",
    "            img_path = os.path.join(path_folder, img_name)\n",
    "            if(os.path.exists(img_path)):\n",
    "                dcgan_is_found = True\n",
    "                break\n",
    "        if not dcgan_is_found:\n",
    "            raise Exception('Couldn\\'t find: {} last:{}'.format(img_name, img_path))\n",
    "    return img_path\n",
    "        \n",
    "    \n",
    "class DatasetFromCSV(Dataset):\n",
    "    def __init__(self, csv_path=None, data_frame=None, transform=None):\n",
    "        if(csv_path is not None):\n",
    "            self.data = pd.read_csv(csv_path).head(20)\n",
    "        elif data_frame is not None:\n",
    "            self.data = data_frame\n",
    "        else:\n",
    "            raise Exception('No csv path or data frame provided')\n",
    "            \n",
    "        self.data_len = len(self.data.index)            # csv data length\n",
    "        self.image_names = np.array(self.data.loc[:,'Image Index'])  # image names\n",
    "    \n",
    "        self.labels = torch.zeros(self.data_len, 15)\n",
    "        labels = self.data.loc[:,'Finding Labels'] #.map(lambda x: x.split('|'))\n",
    "        self.multi_hot_encoding_label(labels)\n",
    "    \n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.data_len\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        # Read 1 image name\n",
    "        img_name = self.image_names[index]\n",
    "        img_path = resolve_full_path(img_name)\n",
    "        img_as_img = Image.open(img_path)\n",
    "\n",
    "        img_as_img = img_as_img.convert(\"RGB\")\n",
    "        # Transform image to tensor\n",
    "        img_as_tensor = self.transform(img_as_img)\n",
    "\n",
    "        # Read 1 label:\n",
    "        image_label = self.labels[index]\n",
    "\n",
    "        return img_as_tensor, image_label\n",
    "    \n",
    "    def multi_hot_encoding_label(self, labels):\n",
    "            for i,label in enumerate(labels):\n",
    "                for idx in range(len(label_list)):\n",
    "                    if label_list[idx] in label:\n",
    "                        self.labels[i][idx] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resample imbalanced dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate through all_labels to calculate each case's weight\n",
    "def calculate_weight(data):\n",
    "    D_single_weight = calculate_single_label_weight(data)\n",
    "    weight = torch.zeros(data.shape[0])\n",
    "    \n",
    "    all_labels = data.loc[:,'Finding Labels'].map(lambda x: x.split('|'))\n",
    "    for i, labels in enumerate(all_labels):\n",
    "        for ii, label in enumerate(labels):\n",
    "            weight[i] += D_single_weight[label]\n",
    "    \n",
    "    return weight\n",
    "\n",
    "def calculate_single_label_weight(data):\n",
    "    # Calculate single label weight\n",
    "    D_sorted = count_label(data)\n",
    "    D_single_weight = D_sorted.copy()\n",
    "    for i, label in enumerate(D_single_weight.keys()):\n",
    "        D_single_weight[label] = 1.0/D_single_weight[label]*1e5\n",
    "        \n",
    "    return D_single_weight\n",
    "\n",
    "def count_label(data):\n",
    "    D_label_count = dict()\n",
    "    all_labels = data.loc[:,'Finding Labels'].map(lambda x: x.split('|'))\n",
    "    for i,labels in enumerate(all_labels):\n",
    "        for ii, label in enumerate(labels):\n",
    "            D_label_count[label] = D_label_count.get(label, 0) + 1\n",
    "    D = D_label_count\n",
    "    D_sorted = OrderedDict(sorted(D.items(), key=lambda x: x[1], reverse=True))\n",
    "    \n",
    "    return D_sorted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define transforms\n",
    "transform = transforms.Compose([transforms.Resize(256),\n",
    "                                transforms.RandomResizedCrop(224),\n",
    "                                transforms.RandomHorizontalFlip(), # randomly flip and rotate\n",
    "                                transforms.RandomRotation(10),\n",
    "                                transforms.ToTensor(),\n",
    "                                transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the model with Transfer learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device:  cuda\n",
      "ResNet(\n",
      "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu): ReLU(inplace=True)\n",
      "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "  (layer1): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  (fc): Sequential(\n",
      "    (0): Linear(in_features=512, out_features=256, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Dropout(p=0.2, inplace=False)\n",
      "    (3): Linear(in_features=256, out_features=15, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from torchvision import models\n",
    "\n",
    "# Use GPU if it's available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device: \",device)\n",
    "\n",
    "def create_model():\n",
    "    \n",
    "    # Import pre-trained densenet-121\n",
    "    model = models.resnet18(pretrained=True)\n",
    "\n",
    "    # Freeze parameters so we don't backprop through them\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "    # Change output to classfiy 14 conditioins + nothing.\n",
    "    # Change a new classifier\n",
    "    model.fc = nn.Sequential(\n",
    "        nn.Linear(512, 256),\n",
    "        nn.ReLU(),\n",
    "        nn.Dropout(0.2),\n",
    "        nn.Linear(256, 15)\n",
    "    )\n",
    "    return model\n",
    "model = create_model()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specify Loss function and Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "learning_rate = 0.001\n",
    "# Only train the classifier parameters, feature parameters are frozen\n",
    "optimizer = optim.Adam(model.fc.parameters(), lr=learning_rate)\n",
    "\n",
    "model = model.to(device);\n",
    "writer = SummaryWriter(f'run/k_fold_resnet18/training')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kfold prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sklearn'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-1fd1561e0a8a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mKFold\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Define sampler to resample the imbalanced dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtrain_dataset_entry\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/extra_disk_1/code/medical_ip/Multi_Label_Dataloader_and_Classifier/traindata_paul.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mvalid_dataset_entry\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/extra_disk_1/code/medical_ip/Multi_Label_Dataloader_and_Classifier/valdata_paul.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'sklearn'"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# Define sampler to resample the imbalanced dataset\n",
    "train_dataset_entry = pd.read_csv(\"/extra_disk_1/code/medical_ip/Multi_Label_Dataloader_and_Classifier/traindata_paul.csv\")\n",
    "valid_dataset_entry = pd.read_csv(\"/extra_disk_1/code/medical_ip/Multi_Label_Dataloader_and_Classifier/valdata_paul.csv\")\n",
    "\n",
    "dcgan_Cardiomegaly_entry = pd.read_csv(\"/extra_disk_1/code/medical_ip/NIH_code/DCGAN_NIH/dcgan_image_csv/dcgan_Cardiomegaly.csv\")\n",
    "dcgan_Consolidation_entry = pd.read_csv(\"/extra_disk_1/code/medical_ip/NIH_code/DCGAN_NIH/dcgan_image_csv/dcgan_Consolidation.csv\")\n",
    "dcgan_Emphysema_entry = pd.read_csv(\"/extra_disk_1/code/medical_ip/NIH_code/DCGAN_NIH/dcgan_image_csv/dcgan_Emphysema.csv\")\n",
    "dcgan_Pleural_Thickening_entry = pd.read_csv(\"/extra_disk_1/code/medical_ip/NIH_code/DCGAN_NIH/dcgan_image_csv/dcgan_Pleural_Thickening.csv\")\n",
    "dcgan_Pneumothorax_entry = pd.read_csv(\"/extra_disk_1/code/medical_ip/NIH_code/DCGAN_NIH/dcgan_image_csv/dcgan_Pneumothorax.csv\")\n",
    "\n",
    "n_splits = 5\n",
    "kf = KFold(n_splits = n_splits, shuffle = True, random_state = 2)\n",
    "\n",
    "# non_test_set = pd.concat([train_dataset_entry, valid_dataset_entry], axis=0)\n",
    "non_test_set = pd.concat([train_dataset_entry, valid_dataset_entry, dcgan_Cardiomegaly_entry,\n",
    "                          dcgan_Consolidation_entry, dcgan_Emphysema_entry, dcgan_Pleural_Thickening_entry,\n",
    "                          dcgan_Pneumothorax_entry], axis=0, ignore_index=True, join='inner')\n",
    "\n",
    "print(non_test_set.head)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Variable\n",
    "\n",
    "# number of epochs to train the model\n",
    "n_epochs = 10\n",
    "k =1\n",
    "valid_losses = []\n",
    "train_losses = []\n",
    "for  train_index, valid_index in kf.split(non_test_set):\n",
    "    train = non_test_set.iloc[train_index]\n",
    "    valid =  non_test_set.iloc[valid_index]\n",
    "    # Define custom data loader\n",
    "    train_dataset = DatasetFromCSV(data_frame=train,transform=transform)\n",
    "    valid_dataset = DatasetFromCSV(data_frame=valid,transform=transform)\n",
    "    batch_size_ = 10\n",
    "    # Define two data loaders\n",
    "    train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                                    batch_size=batch_size_,\n",
    "                                                    num_workers=6,\n",
    "                                                    shuffle=True)\n",
    "\n",
    "    valid_loader = torch.utils.data.DataLoader(dataset=valid_dataset,\n",
    "                                                    batch_size=batch_size_,\n",
    "                                                    num_workers=6,\n",
    "                                                    shuffle=True)\n",
    "    log_file = open('log.txt', 'a')\n",
    "    log_file.write('Fold: {}/{} \\n'.format(k, n_splits,))\n",
    "    k = k+1\n",
    "    log_file.close()\n",
    "\n",
    "    # k fold setup before\n",
    "    valid_loss_min = np.Inf # track change in validation loss\n",
    "    writer.add_scalar('learning rate', learning_rate)\n",
    "    for epoch in range(0, n_epochs):\n",
    "        t0 = time.time()\n",
    "        # keep track of training and validation loss\n",
    "        train_loss = 0.0\n",
    "        valid_loss = 0.0\n",
    "\n",
    "        ###################\n",
    "        # train the model #\n",
    "        ###################\n",
    "        model.train()\n",
    "        for batch_idx, (data, target) in enumerate(train_loader):\n",
    "            # move tensors to GPU if CUDA is available\n",
    "            data = data.to(device)\n",
    "            target = target.to(device)\n",
    "            model = model.to(device)\n",
    "            # clear the gradients of all optimized variables\n",
    "            optimizer.zero_grad()\n",
    "            # forward pass: compute predicted outputs by passing inputs to the model\n",
    "            output = model(data)\n",
    "            # calculate the batch loss\n",
    "            loss = criterion(output, target)\n",
    "            # backward pass: compute gradient of the loss with respect to model parameters\n",
    "            loss.backward()\n",
    "            # perform a single optimization step (parameter update)\n",
    "            optimizer.step()\n",
    "            # update training loss\n",
    "            train_loss += loss.item()*data.size(0)\n",
    "\n",
    "            # Print losses ocassionally and print to tensorboard\n",
    "            if batch_idx % 100 == 0:\n",
    "                train_loss_divided = train_loss/(batch_idx+1)\n",
    "                log_file = open('log.txt', 'a')\n",
    "                log_file.write(f'Epoch [{epoch}/{n_epochs}] Batch {batch_idx}/{len(train_loader)} Train_loss {train_loss_divided} \\n')\n",
    "                log_file.close()\n",
    "                \n",
    "                writer.add_scalar('loss', train_loss_divided, epoch*len(train_loader)+batch_idx)\n",
    "                with torch.no_grad():\n",
    "                    img_grid_real = torchvision.utils.make_grid(data, normalize=True)\n",
    "                    writer.add_image(\"Train Lung Xray Images\", img_grid_real)\n",
    "\n",
    "        ######################    \n",
    "        # validate the model #\n",
    "        ######################\n",
    "        model.eval()\n",
    "        for batch_idx, (data, target) in enumerate(valid_loader):\n",
    "\n",
    "            data = data.to(device)\n",
    "            target = target.to(device)\n",
    "            # forward pass: compute predicted outputs by passing inputs to the model\n",
    "            output = model(data)\n",
    "            # calculate the batch loss\n",
    "            loss = criterion(output, target)\n",
    "            # update average validation loss \n",
    "            valid_loss += loss.item()*data.size(0)\n",
    "\n",
    "        # calculate average losses\n",
    "        train_loss = train_loss/len(train_loader)\n",
    "        valid_loss = valid_loss/len(valid_loader)\n",
    "\n",
    "        t1 = time.time()\n",
    "        total = t1-t0\n",
    "        \n",
    "        # print training/validation statistics \n",
    "        log_file = open('log.txt', 'a')\n",
    "        log_file.write('Epoch: {}/{} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f} Duration seconds: {} \\n'.format(\n",
    "                        epoch, n_epochs, train_loss, valid_loss, total))\n",
    "        log_file.close()\n",
    "        \n",
    "        writer.add_scalar('train_loss', train_loss, epoch)\n",
    "        writer.add_scalar('valid_loss', valid_loss, epoch)\n",
    "\n",
    "\n",
    "        # save model if validation loss has decreased\n",
    "        if valid_loss <= valid_loss_min:\n",
    "            log_file = open('log.txt', 'a')\n",
    "            log_file.write('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ... \\n'.format(valid_loss_min, valid_loss))\n",
    "            log_file.close()\n",
    "            \n",
    "            torch.save(model.state_dict(), model_path)\n",
    "            valid_loss_min = valid_loss\n",
    "        writer.add_scalar('best_valid_loss_fold', valid_loss_min, epoch)\n",
    "        \n",
    "        log_file = open('log.txt', 'a')\n",
    "        log_file.write(f'best_valid_loss_fold [{valid_loss_min}] Best_Epoch [{epoch}]')\n",
    "        log_file.close()\n",
    "        \n",
    "    valid_losses.append(valid_loss_min)\n",
    "    train_losses.append(train_loss)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test classficaton on a single image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "# Doing inference on cpu as it doesn't take much effort feel free to change.\n",
    "# Had some trouble loading it on GPU. \n",
    "image, label = next(iter(valid_loader))\n",
    "model = create_model()\n",
    "model.load_state_dict(torch.load(model_path, map_location='cpu'))\n",
    "model.eval()\n",
    "\n",
    "data = image.to('cpu')\n",
    "# forward pass: compute predicted outputs by passing inputs to the model\n",
    "output = model(data)\n",
    "output = output.to('cpu').detach().numpy()\n",
    "target = label.to('cpu').detach().numpy()\n",
    "cur = 7\n",
    "print(np.shape(target))\n",
    "print(np.shape(output))\n",
    "\n",
    "print(\"Predicted: {}\".format(output[cur]))\n",
    "print(\"Predicted sigmoid: {}\".format(sigmoid(output[cur])))\n",
    "\n",
    "print(\"Actual: {}\".format(target[cur]))\n",
    "\n",
    "print(\"Predicted Max : {}\".format(output[cur].max()))\n",
    "print(\"Actual Max : {}\".format(target[cur].max()))\n",
    "\n",
    "print(\"Predicted Sigmoid Arg Max : {}\".format(sigmoid(output[cur].argmax())))\n",
    "print(\"Actual Arg Max : {}\".format(target[cur].argmax()))\n",
    "\n",
    "\n",
    "print(output[cur].max())\n",
    "print(\"\\nimage batch shape: \", image.shape)\n",
    "print(\"single image shape: \", image[cur].shape)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 1 channel image\n",
    "img_1_channel = image.numpy()[cur][1]\n",
    "print(\"img_1channel shape: \", img_1_channel.shape)\n",
    "plt.figure()\n",
    "plt.imshow(img_1_channel)\n",
    "\n",
    "# 3 channel image\n",
    "plt.figure()\n",
    "img_3_channel = image[cur].permute(1, 2, 0)\n",
    "plt.imshow(img_3_channel, cmap='cool')\n",
    "print(\"img_3channel shape:\", img_3_channel.shape)\n",
    "\n",
    "# print label\n",
    "print(\"labels:\",label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/extra_disk_1/pytorch/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/extra_disk_1/pytorch/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/extra_disk_1/pytorch/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/extra_disk_1/pytorch/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/extra_disk_1/pytorch/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/extra_disk_1/pytorch/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model save/load location: /extra_disk_1/trained_model/resnet50_dcgan_SGD/resnet50_dcgan_SGD.pt\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torch import optim, nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "import torchvision\n",
    "from torch.utils.data.dataset import Dataset\n",
    "import matplotlib.pyplot as plt\n",
    "import os.path\n",
    "from os import path\n",
    "from collections import OrderedDict\n",
    "import time\n",
    "import glob\n",
    "from torch.utils.tensorboard import SummaryWriter  # to print to tensorboard\n",
    "\n",
    "\n",
    "models_dir = os.path.expanduser('/extra_disk_1/trained_model/resnet50_dcgan_SGD')\n",
    "model_name = 'resnet50_dcgan_SGD.pt'\n",
    "model_path = os.path.join(models_dir, model_name)\n",
    "if not os.path.exists(models_dir):\n",
    "    os.makedirs(models_dir)\n",
    "    print(\"create models_dir: \", models_dir)\n",
    "\n",
    "print('Model save/load location: {}'.format(model_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Dataset Loading Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_list = ['Cardiomegaly','Emphysema','Effusion','Hernia','Nodule','Pneumothorax','Atelectasis','Pleural_Thickening','Mass','Edema','Consolidation',\n",
    "              'Infiltration','Fibrosis','Pneumonia','No Finding']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resolve_full_path(img_name):\n",
    "    original_is_found = False\n",
    "    dcgan_is_found = False\n",
    "    \n",
    "    # Read 1 image file\n",
    "    folder_idx_range = 13\n",
    "    img_path = ''\n",
    "    for folder_idx in range(folder_idx_range):\n",
    "        path_prefix = \"/extra_disk_1/data/images_\"\n",
    "        path_suffix = \"images/\"\n",
    "        cur_img_dir = path_prefix +str(folder_idx).zfill(3) +'/'\n",
    "        img_folder_path = path.join(cur_img_dir, path_suffix)\n",
    "        img_path = os.path.join(img_folder_path, img_name) \n",
    "        if(path.exists(img_path)):\n",
    "            original_is_found = True\n",
    "            break\n",
    "            \n",
    "    if(not original_is_found):\n",
    "        # search in dcgan_image folder\n",
    "        path_prefix = glob.glob(\"/extra_disk_1/code/medical_ip/NIH_code/DCGAN_NIH/dcgan_image/*\")\n",
    "        for path_folder in path_prefix:\n",
    "            img_path = os.path.join(path_folder, img_name)\n",
    "            if(os.path.exists(img_path)):\n",
    "                dcgan_is_found = True\n",
    "                break\n",
    "        if not dcgan_is_found:\n",
    "            raise Exception('Couldn\\'t find: {} last:{}'.format(img_name, img_path))\n",
    "    return img_path\n",
    "        \n",
    "    \n",
    "class DatasetFromCSV(Dataset):\n",
    "    def __init__(self, csv_path=None, data_frame=None, transform=None):\n",
    "        if(csv_path is not None):\n",
    "            self.data = pd.read_csv(csv_path).head(20)\n",
    "        elif data_frame is not None:\n",
    "            self.data = data_frame\n",
    "        else:\n",
    "            raise Exception('No csv path or data frame provided')\n",
    "            \n",
    "        self.data_len = len(self.data.index)            # csv data length\n",
    "        self.image_names = np.array(self.data.loc[:,'Image Index'])  # image names\n",
    "    \n",
    "        self.labels = torch.zeros(self.data_len, 15)\n",
    "        labels = self.data.loc[:,'Finding Labels'] #.map(lambda x: x.split('|'))\n",
    "        self.multi_hot_encoding_label(labels)\n",
    "    \n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.data_len\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        # Read 1 image name\n",
    "        img_name = self.image_names[index]\n",
    "        img_path = resolve_full_path(img_name)\n",
    "        img_as_img = Image.open(img_path)\n",
    "\n",
    "        img_as_img = img_as_img.convert(\"RGB\")\n",
    "        # Transform image to tensor\n",
    "        img_as_tensor = self.transform(img_as_img)\n",
    "\n",
    "        # Read 1 label:\n",
    "        image_label = self.labels[index]\n",
    "\n",
    "        return img_as_tensor, image_label\n",
    "    \n",
    "    def multi_hot_encoding_label(self, labels):\n",
    "            for i,label in enumerate(labels):\n",
    "                for idx in range(len(label_list)):\n",
    "                    if label_list[idx] in label:\n",
    "                        self.labels[i][idx] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resample imbalanced dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate through all_labels to calculate each case's weight\n",
    "def calculate_weight(data):\n",
    "    D_single_weight = calculate_single_label_weight(data)\n",
    "    weight = torch.zeros(data.shape[0])\n",
    "    \n",
    "    all_labels = data.loc[:,'Finding Labels'].map(lambda x: x.split('|'))\n",
    "    for i, labels in enumerate(all_labels):\n",
    "        for ii, label in enumerate(labels):\n",
    "            weight[i] += D_single_weight[label]\n",
    "    \n",
    "    return weight\n",
    "\n",
    "def calculate_single_label_weight(data):\n",
    "    # Calculate single label weight\n",
    "    D_sorted = count_label(data)\n",
    "    D_single_weight = D_sorted.copy()\n",
    "    for i, label in enumerate(D_single_weight.keys()):\n",
    "        D_single_weight[label] = 1.0/D_single_weight[label]*1e5\n",
    "        \n",
    "    return D_single_weight\n",
    "\n",
    "def count_label(data):\n",
    "    D_label_count = dict()\n",
    "    all_labels = data.loc[:,'Finding Labels'].map(lambda x: x.split('|'))\n",
    "    for i,labels in enumerate(all_labels):\n",
    "        for ii, label in enumerate(labels):\n",
    "            D_label_count[label] = D_label_count.get(label, 0) + 1\n",
    "    D = D_label_count\n",
    "    D_sorted = OrderedDict(sorted(D.items(), key=lambda x: x[1], reverse=True))\n",
    "    \n",
    "    return D_sorted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define transforms\n",
    "transform = transforms.Compose([transforms.Resize(256),\n",
    "                                transforms.RandomResizedCrop(224),\n",
    "                                transforms.RandomHorizontalFlip(), # randomly flip and rotate\n",
    "                                transforms.RandomRotation(10),\n",
    "                                transforms.ToTensor(),\n",
    "                                transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the model with Transfer learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training different layers at 4 stages, each stage has epochs of 2, 3, 3, 2\n",
    "def switch_stage(epoch, model):\n",
    "    if epoch <= 2:\n",
    "        stage=1\n",
    "        print('----- STAGE 1 -----') # only training 'layer2', 'layer3', 'layer4' and 'fc'\n",
    "        for name, param in model.named_parameters(): # all requires_grad by default, are True initially\n",
    "            if ('layer2' in name) or ('layer3' in name) or ('layer4' in name) or ('fc' in name):\n",
    "                param.requires_grad = True \n",
    "            else:\n",
    "                param.requires_grad = False\n",
    "    if 2 < epoch <= 5:\n",
    "        stage=2\n",
    "        print('\\n----- STAGE 2 -----') # only training 'layer3', 'layer4' and 'fc'\n",
    "        for name, param in model.named_parameters(): \n",
    "            if ('layer3' in name) or ('layer4' in name) or ('fc' in name):\n",
    "                param.requires_grad = True \n",
    "            else:\n",
    "                param.requires_grad = False\n",
    "    if 5 < epoch <= 8:\n",
    "        stage=3\n",
    "        print('\\n----- STAGE 3 -----') # only training  'layer4' and 'fc'\n",
    "        for name, param in model.named_parameters(): \n",
    "            if ('layer4' in name) or ('fc' in name):\n",
    "                param.requires_grad = True \n",
    "            else:\n",
    "                param.requires_grad = False\n",
    "    if 8 < epoch <= 10:\n",
    "        stage=4\n",
    "        print('\\n----- STAGE 4 -----') # only training  'layer4' and 'fc'\n",
    "        for name, param in model.named_parameters(): \n",
    "            if 'fc' in name:\n",
    "                param.requires_grad = True \n",
    "            else:\n",
    "                param.requires_grad = False\n",
    "    log_stage = open('log_stage.txt','a')\n",
    "    log_stage.write(f\"---------------------------stage: [{stage}]--------------------------\\n\")\n",
    "    for name, param in model.named_parameters(): # all requires_grad by default, are True initially\n",
    "        log_stage.write('{}: {}\\n'.format(name, param.requires_grad))\n",
    "    log_stage.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device:  cuda\n"
     ]
    }
   ],
   "source": [
    "from torchvision import models\n",
    "\n",
    "# Use GPU if it's available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device: \",device)\n",
    "\n",
    "def create_model():\n",
    "    \n",
    "    # Import pre-trained resnet\n",
    "    model = models.resnet50(pretrained=False)\n",
    "\n",
    "    # Freeze parameters so we don't backprop through them\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = True\n",
    "#     print('----- STAGE 1 -----') # only training 'layer2', 'layer3', 'layer4' and 'fc'\n",
    "#     for name, param in model.named_parameters(): # all requires_grad by default, are True initially\n",
    "#         if ('layer2' in name) or ('layer3' in name) or ('layer4' in name) or ('fc' in name):\n",
    "#             param.requires_grad = True \n",
    "#         else:\n",
    "#             param.requires_grad = False\n",
    "\n",
    " \n",
    "    # Change output to classfiy 14 conditioins + nothing.\n",
    "    # Change a new classifier\n",
    "    model.fc = nn.Sequential(\n",
    "        nn.Linear(model.fc.in_features, 256),\n",
    "        nn.ReLU(),\n",
    "        nn.Dropout(0.2),\n",
    "        nn.Linear(256, 15)\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "model = create_model()\n",
    "# print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specify Loss function and Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "learning_rate = 0.001\n",
    "# Only train the classifier parameters, feature parameters are frozen\n",
    "# optimizer = optim.Adam(model.fc.parameters(), lr=learning_rate)\n",
    "# optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=learning_rate)\n",
    "optimizer = optim.SGD(filter(lambda p: p.requires_grad, model.parameters()), lr=learning_rate, momentum=0.9)\n",
    "\n",
    "model = model.to(device);\n",
    "writer = SummaryWriter(f'run/k_fold_resnet50_SGD/training')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kfold prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method NDFrame.head of                     Image Index       Finding Labels\n",
      "0              00000002_000.png           No Finding\n",
      "1              00000003_000.png               Hernia\n",
      "2              00000003_001.png               Hernia\n",
      "3              00000003_002.png               Hernia\n",
      "4              00000003_003.png  Hernia|Infiltration\n",
      "...                         ...                  ...\n",
      "94585  Pneumothorax_019_045.png         Pneumothorax\n",
      "94586  Pneumothorax_019_046.png         Pneumothorax\n",
      "94587  Pneumothorax_019_047.png         Pneumothorax\n",
      "94588  Pneumothorax_019_048.png         Pneumothorax\n",
      "94589  Pneumothorax_019_049.png         Pneumothorax\n",
      "\n",
      "[94590 rows x 2 columns]>\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# Define sampler to resample the imbalanced dataset\n",
    "train_dataset_entry = pd.read_csv(\"/extra_disk_1/code/medical_ip/Multi_Label_Dataloader_and_Classifier/traindata_paul.csv\")\n",
    "valid_dataset_entry = pd.read_csv(\"/extra_disk_1/code/medical_ip/Multi_Label_Dataloader_and_Classifier/valdata_paul.csv\")\n",
    "\n",
    "dcgan_Cardiomegaly_entry = pd.read_csv(\"/extra_disk_1/code/medical_ip/NIH_code/DCGAN_NIH/dcgan_image_csv/dcgan_Cardiomegaly.csv\")\n",
    "dcgan_Consolidation_entry = pd.read_csv(\"/extra_disk_1/code/medical_ip/NIH_code/DCGAN_NIH/dcgan_image_csv/dcgan_Consolidation.csv\")\n",
    "dcgan_Emphysema_entry = pd.read_csv(\"/extra_disk_1/code/medical_ip/NIH_code/DCGAN_NIH/dcgan_image_csv/dcgan_Emphysema.csv\")\n",
    "dcgan_Pleural_Thickening_entry = pd.read_csv(\"/extra_disk_1/code/medical_ip/NIH_code/DCGAN_NIH/dcgan_image_csv/dcgan_Pleural_Thickening.csv\")\n",
    "dcgan_Pneumothorax_entry = pd.read_csv(\"/extra_disk_1/code/medical_ip/NIH_code/DCGAN_NIH/dcgan_image_csv/dcgan_Pneumothorax.csv\")\n",
    "\n",
    "n_splits = 5\n",
    "kf = KFold(n_splits = n_splits, shuffle = True, random_state = 2)\n",
    "\n",
    "# non_test_set = pd.concat([train_dataset_entry, valid_dataset_entry], axis=0)\n",
    "non_test_set = pd.concat([train_dataset_entry, valid_dataset_entry, dcgan_Cardiomegaly_entry,\n",
    "                          dcgan_Consolidation_entry, dcgan_Emphysema_entry, dcgan_Pleural_Thickening_entry,\n",
    "                          dcgan_Pneumothorax_entry], axis=0, ignore_index=True, join='inner')\n",
    "\n",
    "print(non_test_set.head)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Variable\n",
    "\n",
    "# number of epochs to train the model\n",
    "n_epochs = 10\n",
    "k =1\n",
    "valid_losses = []\n",
    "train_losses = []\n",
    "for  train_index, valid_index in kf.split(non_test_set):\n",
    "    train = non_test_set.iloc[train_index]\n",
    "    valid =  non_test_set.iloc[valid_index]\n",
    "    # Define custom data loader\n",
    "    train_dataset = DatasetFromCSV(data_frame=train,transform=transform)\n",
    "    valid_dataset = DatasetFromCSV(data_frame=valid,transform=transform)\n",
    "    batch_size_ = 10\n",
    "    # Define two data loaders\n",
    "    train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                                    batch_size=batch_size_,\n",
    "                                                    num_workers=4,\n",
    "                                                    shuffle=True)\n",
    "\n",
    "    valid_loader = torch.utils.data.DataLoader(dataset=valid_dataset,\n",
    "                                                    batch_size=batch_size_,\n",
    "                                                    num_workers=4,\n",
    "                                                    shuffle=True)\n",
    "    log_file = open('run/SGD/log.txt', 'a')\n",
    "    log_file.write('Fold: {}/{} \\n'.format(k, n_splits,))\n",
    "    k = k+1\n",
    "    log_file.close()\n",
    "\n",
    "    # k fold setup before\n",
    "    valid_loss_min = np.Inf # track change in validation loss\n",
    "    writer.add_scalar('learning rate', learning_rate)\n",
    "    for epoch in range(0, n_epochs):\n",
    "        t0 = time.time()\n",
    "        # keep track of training and validation loss\n",
    "        train_loss = 0.0\n",
    "        valid_loss = 0.0\n",
    "        \n",
    "        # switch stage\n",
    "#         switch_stage(epoch, model)\n",
    "        \n",
    "        log_stage_epoch = open('run/SGD/log_stage_epoch.txt','a')\n",
    "        log_stage_epoch.write(f\"---------------------------epoch: [{epoch}]--------------------------\\n\")\n",
    "        for name, param in model.named_parameters(): # all requires_grad by default, are True initially\n",
    "            log_stage_epoch.write('{}: {}\\n'.format(name, param.requires_grad))\n",
    "        log_stage_epoch.close()\n",
    "        \n",
    "        ###################\n",
    "        # train the model #\n",
    "        ###################\n",
    "        model.train()\n",
    "        for batch_idx, (data, target) in enumerate(train_loader):\n",
    "            # move tensors to GPU if CUDA is available\n",
    "            data = data.to(device)\n",
    "            target = target.to(device)\n",
    "            model = model.to(device)\n",
    "            # clear the gradients of all optimized variables\n",
    "            optimizer.zero_grad()\n",
    "            # forward pass: compute predicted outputs by passing inputs to the model\n",
    "            output = model(data)\n",
    "            # calculate the batch loss\n",
    "            loss = criterion(output, target)\n",
    "            # backward pass: compute gradient of the loss with respect to model parameters\n",
    "            loss.backward()\n",
    "            # perform a single optimization step (parameter update)\n",
    "            optimizer.step()\n",
    "            # update training loss\n",
    "            train_loss += loss.item()*data.size(0)\n",
    "\n",
    "            # Print losses ocassionally and print to tensorboard\n",
    "            if batch_idx % 100 == 0:\n",
    "                train_loss_divided = train_loss/(batch_idx+1)\n",
    "                log_file = open('run/SGD/log.txt', 'a')\n",
    "                log_file.write(f'Epoch [{epoch}/{n_epochs}] Batch {batch_idx}/{len(train_loader)} Train_loss {train_loss_divided} \\n')\n",
    "                log_file.close()\n",
    "                \n",
    "                writer.add_scalar('loss', train_loss_divided, epoch*len(train_loader)+batch_idx)\n",
    "#                 with torch.no_grad():\n",
    "#                     img_grid_real = torchvision.utils.make_grid(data, normalize=True)\n",
    "#                     writer.add_image(\"Train Lung Xray Images\", img_grid_real)\n",
    "\n",
    "        ######################    \n",
    "        # validate the model #\n",
    "        ######################\n",
    "        model.eval()\n",
    "        for batch_idx, (data, target) in enumerate(valid_loader):\n",
    "\n",
    "            data = data.to(device)\n",
    "            target = target.to(device)\n",
    "            # forward pass: compute predicted outputs by passing inputs to the model\n",
    "            output = model(data)\n",
    "            # calculate the batch loss\n",
    "            loss = criterion(output, target)\n",
    "            # update average validation loss \n",
    "            valid_loss += loss.item()*data.size(0)\n",
    "\n",
    "        # calculate average losses\n",
    "        train_loss = train_loss/len(train_loader)\n",
    "        valid_loss = valid_loss/len(valid_loader)\n",
    "\n",
    "        t1 = time.time()\n",
    "        total = t1-t0\n",
    "        \n",
    "        # print training/validation statistics \n",
    "        log_file = open('run/SGD/log.txt', 'a')\n",
    "        log_file.write('Epoch: {}/{} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f} Duration seconds: {} \\n'.format(\n",
    "                        epoch, n_epochs, train_loss, valid_loss, total))\n",
    "        log_file.close()\n",
    "        \n",
    "        writer.add_scalar('train_loss', train_loss, epoch)\n",
    "        writer.add_scalar('valid_loss', valid_loss, epoch)\n",
    "\n",
    "\n",
    "        # save model if validation loss has decreased\n",
    "        if valid_loss <= valid_loss_min:\n",
    "            log_file = open('run/SGD/log.txt', 'a')\n",
    "            log_file.write('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ... \\n'.format(valid_loss_min, valid_loss))\n",
    "            log_file.close()\n",
    "            \n",
    "            torch.save(model.state_dict(), model_path)\n",
    "            valid_loss_min = valid_loss\n",
    "        writer.add_scalar('best_valid_loss_fold', valid_loss_min, epoch)\n",
    "        \n",
    "        log_file = open('run/SGD/log.txt', 'a')\n",
    "        log_file.write(f'best_valid_loss_fold [{valid_loss_min}] Best_Epoch [{epoch}]')\n",
    "        log_file.close()\n",
    "        \n",
    "    valid_losses.append(valid_loss_min)\n",
    "    train_losses.append(train_loss)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test classficaton on a single image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "# Doing inference on cpu as it doesn't take much effort feel free to change.\n",
    "# Had some trouble loading it on GPU. \n",
    "image, label = next(iter(valid_loader))\n",
    "model = create_model()\n",
    "model.load_state_dict(torch.load(model_path, map_location='cpu'))\n",
    "model.eval()\n",
    "\n",
    "data = image.to('cpu')\n",
    "# forward pass: compute predicted outputs by passing inputs to the model\n",
    "output = model(data)\n",
    "output = output.to('cpu').detach().numpy()\n",
    "target = label.to('cpu').detach().numpy()\n",
    "cur = 7\n",
    "print(np.shape(target))\n",
    "print(np.shape(output))\n",
    "\n",
    "print(\"Predicted: {}\".format(output[cur]))\n",
    "print(\"Predicted sigmoid: {}\".format(sigmoid(output[cur])))\n",
    "\n",
    "print(\"Actual: {}\".format(target[cur]))\n",
    "\n",
    "print(\"Predicted Max : {}\".format(output[cur].max()))\n",
    "print(\"Actual Max : {}\".format(target[cur].max()))\n",
    "\n",
    "print(\"Predicted Sigmoid Arg Max : {}\".format(sigmoid(output[cur].argmax())))\n",
    "print(\"Actual Arg Max : {}\".format(target[cur].argmax()))\n",
    "\n",
    "\n",
    "print(output[cur].max())\n",
    "print(\"\\nimage batch shape: \", image.shape)\n",
    "print(\"single image shape: \", image[cur].shape)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 1 channel image\n",
    "img_1_channel = image.numpy()[cur][1]\n",
    "print(\"img_1channel shape: \", img_1_channel.shape)\n",
    "plt.figure()\n",
    "plt.imshow(img_1_channel)\n",
    "\n",
    "# 3 channel image\n",
    "plt.figure()\n",
    "img_3_channel = image[cur].permute(1, 2, 0)\n",
    "plt.imshow(img_3_channel, cmap='cool')\n",
    "print(\"img_3channel shape:\", img_3_channel.shape)\n",
    "\n",
    "# print label\n",
    "print(\"labels:\",label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
